import type { Chapter } from '../routes/api/generate/types'

export const chapters: Chapter[] = [
	{ timestamp: "0:00",
		title: "Intro & Manus"
	},
	{ timestamp: "3:44",
		title: "KV Cache Deep"
	},
	{ timestamp: "14:03",
		title: "Cache Control"
	},
	{ timestamp: "23:29",
		title: "Eval & Optim"
	},
	{ timestamp: "32:40",
		title: "Cache Control Mechanics"
	},
	{ timestamp: "36:15",
		title: "Prompt Injection"
	},
	{ timestamp: "50:28",
		title: "Context Compression"
	},
	{ timestamp: "54:56",
		title: "Tool Call Nuances"
	},
]

import type { VideoChaptersResponse } from '$lib/baml_client'

export const baml: VideoChaptersResponse = {
	"summary": "Dex and Viob unpack the Manus paper’s context‑engineering breakthroughs, focusing on KV caching, dynamic content placement, provider cache mechanics, evaluation strategies, and advanced tool‑calling and compression techniques. They explain how to keep cache hits, the trade‑offs of dynamic system messages and tool lists, and discuss practical steps for implementing and testing caching in real‑world pipelines.",
	"chapters": [
			{
					"summay": "Hosts introduce the episode, explain why deep understanding of LLM internals matters, and preview the Manus paper.",
					"chapter_title": "Intro & Manus Overview",
					"bookmark": 0
			},
			{
					"summay": "Detailed walk‑through of KV caching: transformer reuse, alignment limits, and internal architecture.",
					"chapter_title": "KV Cache Foundations",
					"bookmark": 154
			},
			{
					"summay": "Dynamic elements like system messages or tool lists break KV continuity; best practices for positioning them to preserve cache hits.",
					"chapter_title": "Dynamic Context Effects",
					"bookmark": 250
			},
			{
					"summay": "Provider‑side caching, TTLs, cross‑user isolation, and the trade‑off between manual control and provider handling.",
					"chapter_title": "Provider Cache Rules",
					"bookmark": 380
			},
			{
					"summay": "Practical evaluation: defining quality, measuring speed and cost, and how to test cache effectiveness with headers or latency proxies.",
					"chapter_title": "Evaluation & Testing",
					"bookmark": 470
			},
			{
					"summay": "Placement of the cache‑control block in prompts and its impact on cache hits for self‑hosted vs API deployments.",
					"chapter_title": "Cache‑Control Placement",
					"bookmark": 609
			},
			{
					"summay": "Tool‑calling strategies, constrained decoding, function‑calling token manipulation, and avoiding token‑boundary pitfalls.",
					"chapter_title": "Tool‑Calling Tricks",
					"bookmark": 1053
			},
			{
					"summay": "Compressing long contexts with restorable data blobs, injecting URLs instead of full docs, and balancing tool calls with token limits.",
					"chapter_title": "Compression & Blobs",
					"bookmark": 935
			}
	]
}


import Groq from 'groq-sdk'

export const whisper: Groq.Audio.Transcription[] = [
	{
		"task": "transcribe",
		"language": "English",
		"duration": 1540.012789,
		"text": " All right. Welcome back, everyone. I think this week is going to be a really, really fun conversation between Desher and I about a topic that I thought was going a little viral for a bit. Many of you might have seen the Manus paper that came out about all their findings about context engineering and other such related things, about what benefits that they saw with some of the techniques that they did. For those of you that didn't, no worries. We're going to go describe all of them and we'll talk about the trade-offs. But I think in general, the most interesting thing about that whole thing that I saw was just how deep this stuff goes. Like a lot of us take for granted, it's like we learned the first level, it's good. But a lot of the stuff goes really deep and it's all based out of the foundational stuff behind how these LLMs work. It's not always even obvious how you would predict that if you don't actually understand how an LLM produces tokens. I think that's kind of what we should talk about today. and how findings cannot go ahead that's there sorry no and like i i really love the the way you frame that which is like um there were things that are just completely beyond your understanding and they're not hard to understand but if you don't know to learn about them and you don't know to go research them and figure out how this stuff works under the hood uh because none of us is going to go and get a machine learning phd in the next two weeks but um if you can find certain things and find understanding of how things work under the hood, there are certain slices you can take of that knowledge that can make you a much better AI engineer. Yeah. And I think part of the conversation today is not just going to be about what those learnings are, but how you could derive them for first principles so that you can go on and perhaps find new things on your own. Because I think that's the most valuable insight. It's not about being able to copy what other people do. It's being able to figure out what is the thing that you can invent and help the rest of us along the way. Because if all of us with all our different backgrounds can go and investigate these areas, then we can converge on what makes good AI pipelines way faster as a general community building this stuff out. So with that- Do you want to do quick intros? And then I have a quick announcement and then let's get into it. Cool. So I'm Dex. Well, the announcement is basically if you're new to the show, we post every episode on GitHub. And so you can come here and see all of them. if you're looking for the recaps of the context engineering for AI agent stuff you can find that here we've done a lot of episodes on context engineering in the past and so just another reminder encouragement we put all of the links all the show notes will be here in the get repo including links to sign up for the next episode and all the whiteboards that we publish so without further ado I'm Dex I'm the founder of Human Layer we've been doing this show since March and this is AI that works where we teach you to be a better AI engineer. I don't know. You're going to have to pitch the show for me. I'm Viob. I write some code. That's about it. Let's talk about fun stuff. Let's get to the meat. Let's do it. One second. Context engineering. ESP. Cool. And now I'm going to go ahead and screen share. So I want to go straight into the Manus paper and just go talk about this from the very beginning. Dexter, I don't have a second screen today, so you're going to have to keep on the lookout for questions people ask and just let me know. Yep. You want to make me a host? Yes. I'm surprised I did not do that already. All right. So the first thing is, like, what is Manus? For those of you that haven't seen, it's a pretty, pretty cool project that got a lot of popularity. it's based I've not signed in they still have the videos on the website where you can see the example use cases yeah I think it's probably worth looking at because I think it tells you roughly what it is it basically kind of just does stuff it's an agent that is very generalized and does stuff and it's really impressive what it was able to do from make me sign in I'll show you guys and I think part of what made this app so viral and impressive is the fact that these folks figured out some ways to stop asking just let me show you what to do but you'll notice okay i don't have enough credits okay i already used all my credits when i was doing this earlier but um part of what made the app really impressive was just how broad of a task i was able to do and how like how high quality was able to handle for the generality of what it did and there's small things like i don't know if you guys noticed just like when i was even on the home page there's small things here that get really fast i do this and it immediately fills in these things how is it so freaking fast we're all using the same models it's not like they have anything special under the hood and these small things around how they design the ux how they design the interfaces how they make the models slightly faster are what i think make madness go viral because one of the first apps to go do that i think when they announced on their techniques one of the most talked about techniques was around this idea of a kv cache but i'll talk about that in a little bit later um it talked about another concept called masking not removing and there's this concept when you use mcps which goes around this idea of like how do you deal with the fact that i don't want to give my model every single tool all the time and why you don't want to do that it makes sense because i don't want my model to be dumb and like have choices between tools that you don't have if it's going to choose between writing a document and i know the user wants a notion document don't give it the google docs tools that is literally just going to confuse it that makes a lot of sense but there's trade-offs in this and what happens so we'll talk about that in a second it talked about how you actually engineer for super large context and context compression i didn't use that word specifically but i think that's what they were hinting at which how to intelligently compress context when you have it's using right yeah this is using basically the same tools that agents already know how to use a file system basically. And so it becomes a really nice way for the agent to organize his own memories, right? Kind of, at least from what I understood. It might be slightly different. And I think the other thing that I thought was when he talked about this concept of how it actually summarizes steps, this was very similar if you were here in the last episode when Dexter was talking about how he does summarization for his coding workflow where he does like a research step a planning step and then an execution step And the whole idea is you're able to compress context in a more concise way, simply by some of the techniques that they discuss here. They talked about keeping the wrong stuff in and like how that hurts your system. And then thing that I often say, which is few-shot prompting sucks. they talk about why they also found few shot prompting sucks more often than not but we'll go into some of these now for everyone else just to give us an idea how many of you have actually read through this or have ideas around what this thing is so we know what kind of conversation we're going to be having today is this the one that mentions i think i saw it as you scrolled up the number of tool uses that an agent should make that's something i remember i can't remember I don't know if this one talks about here or not. No, that's not this one. This one does not talk about the number of tool uses. Okay, so that was like five or something that I remember. Perfect. In that case, let's just start with the first one, designing around the KB cache. I think that is the most interesting one out of all of them because it really inspires for everything else around this. And there's no reason to actually do any of the pre-reading. That's kind of why we have this conversation over here. Dexter and I do this stuff for fun, and we just get to share about it with you, everyone. So one second. I'm going to go screen share, but I think the most important thing I'm going to want is a whiteboard. So let me pull that up, Dexter. Oh, yeah. Let me ship you a whiteboard. I'm just going to buy you $7 a month for a whiteboard account. I don't know if you just like it because the best part is when you do it, I can ask you to take the screenshots and not me. Fair enough. I sent it to you in Slack. Perfect. And I'm going to go back to screen sharing. So let's talk about what it means to design around the KV cache. So firstly, what does a KV cache mean? What is actually happening? So let's just start off with the basics of what an LLM is. An LLM is a thing that takes in a bunch of tokens, spits out a bunch of token probabilities, and then you somehow will apply some algorithm, typically like a softmax or something, to then pick out what is the token that I should actually be selecting. And these are just probabilities of final output tokens. And once you select your next token, then the LLM will take this thing back. I don't know how to draw this arrow, but I suspect Dextra will fix it for me. I'll append this selected token to the very next item on the array, and then I'll just append it and it'll go back in the loop over and over again. Most of us probably know this is how alums work. The thing that I think a lot of people don't realize is this, if you've done software, probably looks like a dynamic programming problem. This looks like a thing where you have almost the same array going in over and over again, but then you just have one new element every single time. It sounds like we should be able to repeat some of the math and not have to redo everything all the time. If you actually go look into here, what you'll find is the LLM has some architecture stuff in here that allows it to not have to repeat all the math all the time, assuming that you have some stability guaranteed. So you can pre-compute some of this stuff as a part of the encoder decoder layer that the LLM has inside of itself. The problem is that pre-computed section is purely dependent on continuity. so it's very hard for you to pre-compute like random segments of it what you can do is much easier to pre-compute chunks of it along the way and I should be using different colors so you can pre-compute this part you can pre-compute this part and then you can pre-compute this part I'm sorry I'm not quite clear what like the vertical access means sorry what I mean by here is like the tokens going in these are all the tokens that are in from like index zero So I should, what I mean by here is like, this is index zero, this is index one. And these are just tokens. Okay, and zero, index zero, index one might be like. Like the word, exactly. It would be like. Think of like a string that I drew vertically because of how the model is laid out. And that's easier for me to think about. Yeah. And then you have like index like. I see. Okay, so you're caching the first three words in the sentence. and then once you come back with this next like once you come back with the the fourth word in the sentence here that is like four uh oh and then you ask okay what's the next token all of all of this part of it basically is is cached all the math for this part of it is cached and so you're just kind of like taking that pre-fill um whatever they're like matrix of the meaning of what's being accumulated in the transformer and so you're just kind of it's all cost and timing right it's like it's much faster to just say okay cool now here's the next token now give me the next inference bit right exactly and just to be very clear like i know some of you are like ah but technically doesn't the walking word impact what all the other words mean so like how can you actually cache this very question i would not have asked that question that's a great question well it turns out the transformer has like two different parts of encoders and decoders that are all laid out So as it turns out, you don't actually run all the computation all at once. You actually break down the input strings, the smaller subsequences of strings that then can each be individually computed. And then you can go on stacking that along the way. So if you go on to like Anthropic's caching docs, so we just go here, for example, you'll find that Anthropic has a minimum cacheable prompt. they do that because less than 1024 tokens probably doesn't fit in whatever cache alignment block that they have for actually doing the computation behind some architecture decisions that they have made and it wouldn't make sense for them to cache that because it's basically throwaway work because every token outside of 124 is going to cross compete need to do some cross attention stuff and actually layer the information between the words but the 124 boundaries they probably have some layer of their network that can be done in parallel and have deterministic results So that is why it's not just important to go read the docs, but just to understand that there's some architecture decision that everyone implementing these models is going to make that dramatically changes how fast this works. And some of you might be wondering, why is it so much smaller in Opus versus haiku. One intuition I could have, I don't know, can't guarantee it, is that it's possible that these haiku models use a different token vocabulary that is much smaller than the original models. Therefore, they're able to compress it better. Or these might be using float 16 instead of float 32 or float 64, so they can double compress it along the way in terms of how much data they can stuff into the same amount of computational unit. But I think the point stands is that there's some computation in every LLM that can be cached to some degree of it. And that the amount of degree that can be done is based on how much similarity you have and how big of an input space you have that is changing. But what that is to say is that if you are constantly running a chat thread and you're dynamically changing the system message all the time, somewhere in here, you're basically breaking the entire KB cache every single time. That means you're having to recompute all the work, always and you can't take any benefit of the caching system that means just just to make this a little more concrete um can i like just draw a little bit more so like let's say you have your your system prompt here yeah uh system prompt and then you have your tools are technically like end up part of the system prompt but i'll just put them separately the idea would be like if at some point you wanted to tell the agent like okay based on some decision that's happening way over way over on the right here uh we want to make sure that the tool set is like only browser-based tools or whatever it is you basically you're going to get a slower iteration speed on the max token that gets generated yeah so when you change this you completely if you just like even if you're just removing stuff when you change things you completely kill the cache and so this has to be recomputed from scratch which is going to be way slower and also like four times the cost exactly there's another subtle thing that probably isn't even obvious here almost everyone i know that's doing some sort of chatbot will say like date in here in their system prompt and yet that that's killing the kv cache for no reason you're literally just hurting the cache every single time i mean if you include the time right if you just put the date it might be okay right yes if you're putting the date sorry i should be you are correct on that there's some time resolution that you're definitely killing the KV cache. But even if you don't include the date, if you have a long running thread, like last time we talked about DRM that was coming through from that last time, the problem with DRM is it's a long process. Decaying resolution memory. That was like the context engineering for a model that can remember stuff that happened months ago. Exactly. Not a model, but an agent. But in that case, today's date is going to break the KV cache all the time. You're just going to be slower for the entire workload that's happening. It's generally always going to be good to put the dynamic stuff in your system as late as possible. That means if you have a giant chat thread that you're running on, it might actually be, if you want the best performance, why can't I not draw a rectangle? It's probably going to be better for you to have system prompt or your chat messages and then your dynamic variables at the very end. Because then as your chat message grows, you're actually going to have the ability to KV cache your chat history as well along with everything else so it's better to say today's date at the very end of this rather than putting it anywhere beforehand even at the end of the system message i don't know these kinds of things i see align with what this is so i imagine you're going to go to the point of like oh if you really want to dynamically change the tools and you don't want to think about what we're about to go to which is like using the like log probabilities and like zeroing things out to remove them from the tool set that you could change your tools and put the tools at the end of the context window instead of at the beginning. Exactly. So if you're not, same thing with a tool set. Like if you want to dynamically change your tools, you got to put them at the bottom. If you put them at the top, you were just shooting yourself in the foot in this system. You will just get slower API calls, more expensive API calls with no real benefit that you're really having. There's a secondary benefit that you get for free, which is at the very, very end, if you put things down here, the model is just likely to pay more attention to it. So anything that is actually relevant and highly dynamic, it's way easier to guarantee the model won't accidentally forget it because it has a recency bias in general. So question like application of that. And I think this is maybe also in the paper. And I actually was doing some more reverse engineering of cloud code with a proxy the other day, trying to figure out what the to-do tool does. And what I would think would be, hey, once you write to the to-dos, the system will occasionally re-inject the list of to-dos near the end of the context window and say, hey, by the way, here's what you said you were going to be working on. And so you force the attention to be on that stuff rather than like hoping that Claude will remember it did that 15 turns ago. Exactly. And that's kind of what one of the steps down here talks about. Oh, bomb cashing doesn't really matter. In the Manus paper, where they release some of the same stuff, which is. Where is this? where's the repetition one oh your recitation so the whole point of this is saying the same thing oh yeah this is simply just stacking stuff all the way down eventually your information will get lost it's not a just like with a normal human if you gave them a giant to-do list of everything to do and you have one single paper at the very front that they have to somehow remember they will forget steps we build processes in place to make sure that stuff gets duplicated and done more than once in like highly yield scenarios, like if I have a rocket ship that's going to launch, I will make sure 15 people make check every single thing because it's just way less likely that someone will slip up and make a mistake on all different things. The model kind of behaves similarly as well. The more steps you put on it, and as the models get better, the distance between how long it can remember and what it can remember does go up That undeniably true But there always a distance at which it will never work as well So having some repetition in there is going to make a huge impact on your output quality for that same reason. Because what they're really saying here is they found that you can do about 50 tool calls to actually produce their tasks. But most models will basically go off track somewhere in the middle really, really fast. What we found in general, I'm like, why does this happen? Well, if you think about how attention works, you'll often see the needle in the haystack problem as a giant benchmark that people go and evaluate for well terrible benchmark i don't know about terrible benchmark but like in general in english language it makes sense that words near each other have way more impact than words far away from each other it's just the any sort of data set that you're going to go train on is always going to have that the sentence i said at the very beginning of this uh talk matters way less than the sentence i just said two minutes ago and that's the whole point. Well, it matters way less to what you're about to say next. Yes. Sorry. Yes, exactly. And that's the same thing that the model is going through. The thing that I want to produce over here is way, way more likely to be related to something that I'm linking over here. I don't know how to connect those than it is to be something I'm linking over here. Now that it's true that system messages are special and I do want system messages treated differently. And that's why the model trainers are working on training the model to understand that better. That system as it is do have impact throughout the entire span of the conversation. But that's a training task. And we can wait for the models to get trained to do that better, but we don't have that guarantee. So the easiest way to provide the guarantee and just to get the best results is to, again, just put things that are most relevant to what the user needs to do as close to what it needs to happen next. So if I need to complete a new task and I know that the model just checked off. Let's say I'm building cloud code and the model just finished task one. I should probably repeat the task list again, assuming I'm using a very dumb model to say, this is the task, which task should you do next? It's just going to work better than just having the task at the top, saying I completed task one. Now, what do I do next? Because the distance is just further. And in that case, I happen to know exactly what needs to happen, which is I need to make progress on a task list. So it's easy for me to manually inject that. But the more I can do that sort of behavior, the more consistency I'll get out of my model outputs. And this just feels like another recitation of kind of that same theme of like, yeah, you could just go back and forth with the model forever and use the default like tool call, tool, tool call, tool call, context window. But if it lets you push your accuracy by 5% or your speed or your cache efficiency by 5%, then why not engineer the thing to be as good as possible? Exactly. Exactly. There's two questions really fast, and I want to make sure we talk about them before we go too deep. Oh, sorry. I know I'm supposed to be on question duty. If you have users using the same models, how will this cache be preserved between the same user calls? Well, the way that the model provides it is, the caching is not something you own, unless you're actually running it on inference. If you do, then you can control how long the cache runs. the model base the model providers basically compute some of this work ahead of time and they just save it into like some data structure probably a redis cache or something i have no idea what they use under the hood and they just say hey if we get the sequence of tokens again just pull this data out it's almost independent of your user message because uh dexter says it's all the time the only thing that impacts the model is actually like the tokens in tokens out so like the fact that it's a user doesn't really matter the model doesn't know that OpenAI, Anthropic don't need to know that as well. And that's how they kind of manage this. I think in general, most of them will not preserve the same cache between, like, technically, if it's content addressable, then like the hash of the content will always be the hash of the content. So you're not leaking, you know, pre-computed attention between users. But like, I think technically people still do segment it out where it's like, hey, you're never going to use the cache from another user. Yeah, I assume that they did it. I assume that they did it because of, oh, Eugene. Yes, that is also true. The model also has its own cache that computes stuff because of the architecture. I'm assuming the caching the model providers are doing are slightly different. I think that also helps quite a lot. The actual inference time is also faster if you have repeated tokens. Maybe that is what the paper was talking about, and I misinterpreted that. But really quickly, I suspect the model providers don't do the caching storage on a cross-user basis because they need to have TTLs. And those TTLs are really based on your personal usage, not on anyone else's. Raycast is a fantastic way to test this out quickly and locally. Can you introduce... You can introduce dynamic variables. Is there something else you would use for a similar purpose? I don't know about something myself, Jens. I was just thinking about like a quick snippet copy-paste tool. I don't know if you store prompts in anything else or if you have a local thing that you guys use. I have my own other setup myself, including that. So just curious. I don't have anything, Dex. Yeah, no, I mean, I think part of it is like, this is a lot more about like building, like this is more applicable to like building software that interacts with models than for like how you prompt them. Yeah, yeah, no, I get that. But I guess as I iterate, I still test things locally before I would put them into what I'm actually going to use. I don't know if that makes sense in the way I'm thinking of it, but if I'm iterating on a prompt essentially or a structure or workflow that I'm going to be implementing through a series of orchestrating agents, I will iterate on each part of that separately through using some sort of way to iterate on the snippet basically. I mean, Vibeoff's got a lot of opinions on how to iterate on prompt snippets. Yeah, so I guess that's where my mind's at right now. So sidetracked. I would say like none of this, none of the stuff that we're talking about today really matters for prompt iteration speed. That really doesn't matter. This only matters for like you have a prompt, it's working, but you want to make that thing work faster and better.",
		"segments": [
			{
				"id": 0,
				"seek": 0,
				"start": 0,
				"end": 3.96,
				"text": " All right. Welcome back, everyone.",
				"tokens": [
					50365,
					1057,
					558,
					13,
					4027,
					646,
					11,
					1518,
					13,
					50563
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 1,
				"seek": 0,
				"start": 4.82,
				"end": 12.56,
				"text": " I think this week is going to be a really, really fun conversation between Desher and I about a topic that I thought was going a little viral for a bit.",
				"tokens": [
					50606,
					286,
					519,
					341,
					1243,
					307,
					516,
					281,
					312,
					257,
					534,
					11,
					534,
					1019,
					3761,
					1296,
					3885,
					511,
					293,
					286,
					466,
					257,
					4829,
					300,
					286,
					1194,
					390,
					516,
					257,
					707,
					16132,
					337,
					257,
					857,
					13,
					50993
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 2,
				"seek": 0,
				"start": 14.22,
				"end": 20.34,
				"text": " Many of you might have seen the Manus paper that came out about all their findings about context engineering and other such related things,",
				"tokens": [
					51076,
					5126,
					295,
					291,
					1062,
					362,
					1612,
					264,
					2458,
					301,
					3035,
					300,
					1361,
					484,
					466,
					439,
					641,
					16483,
					466,
					4319,
					7043,
					293,
					661,
					1270,
					4077,
					721,
					11,
					51382
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 3,
				"seek": 0,
				"start": 20.34,
				"end": 25.22,
				"text": " about what benefits that they saw with some of the techniques that they did.",
				"tokens": [
					51382,
					466,
					437,
					5311,
					300,
					436,
					1866,
					365,
					512,
					295,
					264,
					7512,
					300,
					436,
					630,
					13,
					51626
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 4,
				"seek": 0,
				"start": 25.32,
				"end": 26.64,
				"text": " For those of you that didn't, no worries.",
				"tokens": [
					51631,
					1171,
					729,
					295,
					291,
					300,
					994,
					380,
					11,
					572,
					16340,
					13,
					51697
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 5,
				"seek": 0,
				"start": 26.82,
				"end": 29.04,
				"text": " We're going to go describe all of them and we'll talk about the trade-offs.",
				"tokens": [
					51706,
					492,
					434,
					516,
					281,
					352,
					6786,
					439,
					295,
					552,
					293,
					321,
					603,
					751,
					466,
					264,
					4923,
					12,
					19231,
					13,
					51817
				],
				"temperature": 0,
				"avg_logprob": -0.1142349,
				"compression_ratio": 1.7003257,
				"no_speech_prob": 1.9128382e-12
			},
			{
				"id": 6,
				"seek": 2904,
				"start": 29.68,
				"end": 35.34,
				"text": " But I think in general, the most interesting thing about that whole thing that I saw was just how deep this stuff goes.",
				"tokens": [
					50397,
					583,
					286,
					519,
					294,
					2674,
					11,
					264,
					881,
					1880,
					551,
					466,
					300,
					1379,
					551,
					300,
					286,
					1866,
					390,
					445,
					577,
					2452,
					341,
					1507,
					1709,
					13,
					50680
				],
				"temperature": 0,
				"avg_logprob": -0.1601471,
				"compression_ratio": 1.7544484,
				"no_speech_prob": 1.2448967e-12
			},
			{
				"id": 7,
				"seek": 2904,
				"start": 35.92,
				"end": 40.34,
				"text": " Like a lot of us take for granted, it's like we learned the first level, it's good.",
				"tokens": [
					50709,
					1743,
					257,
					688,
					295,
					505,
					747,
					337,
					12344,
					11,
					309,
					311,
					411,
					321,
					3264,
					264,
					700,
					1496,
					11,
					309,
					311,
					665,
					13,
					50930
				],
				"temperature": 0,
				"avg_logprob": -0.1601471,
				"compression_ratio": 1.7544484,
				"no_speech_prob": 1.2448967e-12
			},
			{
				"id": 8,
				"seek": 2904,
				"start": 40.84,
				"end": 46.58,
				"text": " But a lot of the stuff goes really deep and it's all based out of the foundational stuff behind how these LLMs work.",
				"tokens": [
					50955,
					583,
					257,
					688,
					295,
					264,
					1507,
					1709,
					534,
					2452,
					293,
					309,
					311,
					439,
					2361,
					484,
					295,
					264,
					32195,
					1507,
					2261,
					577,
					613,
					441,
					43,
					26386,
					589,
					13,
					51242
				],
				"temperature": 0,
				"avg_logprob": -0.1601471,
				"compression_ratio": 1.7544484,
				"no_speech_prob": 1.2448967e-12
			},
			{
				"id": 9,
				"seek": 2904,
				"start": 47.06,
				"end": 54.16,
				"text": " It's not always even obvious how you would predict that if you don't actually understand how an LLM produces tokens.",
				"tokens": [
					51266,
					467,
					311,
					406,
					1009,
					754,
					6322,
					577,
					291,
					576,
					6069,
					300,
					498,
					291,
					500,
					380,
					767,
					1223,
					577,
					364,
					441,
					43,
					44,
					14725,
					22667,
					13,
					51621
				],
				"temperature": 0,
				"avg_logprob": -0.1601471,
				"compression_ratio": 1.7544484,
				"no_speech_prob": 1.2448967e-12
			},
			{
				"id": 10,
				"seek": 2904,
				"start": 54.72,
				"end": 56.68,
				"text": " I think that's kind of what we should talk about today.",
				"tokens": [
					51649,
					286,
					519,
					300,
					311,
					733,
					295,
					437,
					321,
					820,
					751,
					466,
					965,
					13,
					51747
				],
				"temperature": 0,
				"avg_logprob": -0.1601471,
				"compression_ratio": 1.7544484,
				"no_speech_prob": 1.2448967e-12
			},
			{
				"id": 11,
				"seek": 5668,
				"start": 56.68,
				"end": 62.84,
				"text": " and how findings cannot go ahead that's there sorry no and like i i really love the the way",
				"tokens": [
					50365,
					293,
					577,
					16483,
					2644,
					352,
					2286,
					300,
					311,
					456,
					2597,
					572,
					293,
					411,
					741,
					741,
					534,
					959,
					264,
					264,
					636,
					50673
				],
				"temperature": 0,
				"avg_logprob": -0.06461773,
				"compression_ratio": 1.8113208,
				"no_speech_prob": 2.3256228e-12
			},
			{
				"id": 12,
				"seek": 5668,
				"start": 62.84,
				"end": 68.34,
				"text": " you frame that which is like um there were things that are just completely beyond your understanding",
				"tokens": [
					50673,
					291,
					3920,
					300,
					597,
					307,
					411,
					1105,
					456,
					645,
					721,
					300,
					366,
					445,
					2584,
					4399,
					428,
					3701,
					50948
				],
				"temperature": 0,
				"avg_logprob": -0.06461773,
				"compression_ratio": 1.8113208,
				"no_speech_prob": 2.3256228e-12
			},
			{
				"id": 13,
				"seek": 5668,
				"start": 68.34,
				"end": 72.18,
				"text": " and they're not hard to understand but if you don't know to learn about them and you don't know",
				"tokens": [
					50948,
					293,
					436,
					434,
					406,
					1152,
					281,
					1223,
					457,
					498,
					291,
					500,
					380,
					458,
					281,
					1466,
					466,
					552,
					293,
					291,
					500,
					380,
					458,
					51140
				],
				"temperature": 0,
				"avg_logprob": -0.06461773,
				"compression_ratio": 1.8113208,
				"no_speech_prob": 2.3256228e-12
			},
			{
				"id": 14,
				"seek": 5668,
				"start": 72.18,
				"end": 76.36,
				"text": " to go research them and figure out how this stuff works under the hood uh because none of us is",
				"tokens": [
					51140,
					281,
					352,
					2132,
					552,
					293,
					2573,
					484,
					577,
					341,
					1507,
					1985,
					833,
					264,
					13376,
					2232,
					570,
					6022,
					295,
					505,
					307,
					51349
				],
				"temperature": 0,
				"avg_logprob": -0.06461773,
				"compression_ratio": 1.8113208,
				"no_speech_prob": 2.3256228e-12
			},
			{
				"id": 15,
				"seek": 5668,
				"start": 76.36,
				"end": 83.38,
				"text": " going to go and get a machine learning phd in the next two weeks but um if you can find certain",
				"tokens": [
					51349,
					516,
					281,
					352,
					293,
					483,
					257,
					3479,
					2539,
					903,
					67,
					294,
					264,
					958,
					732,
					3259,
					457,
					1105,
					498,
					291,
					393,
					915,
					1629,
					51700
				],
				"temperature": 0,
				"avg_logprob": -0.06461773,
				"compression_ratio": 1.8113208,
				"no_speech_prob": 2.3256228e-12
			},
			{
				"id": 16,
				"seek": 8338,
				"start": 83.38,
				"end": 88.52,
				"text": " things and find understanding of how things work under the hood, there are certain slices you can",
				"tokens": [
					50365,
					721,
					293,
					915,
					3701,
					295,
					577,
					721,
					589,
					833,
					264,
					13376,
					11,
					456,
					366,
					1629,
					19793,
					291,
					393,
					50622
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 17,
				"seek": 8338,
				"start": 88.52,
				"end": 94.88,
				"text": " take of that knowledge that can make you a much better AI engineer. Yeah. And I think part of the",
				"tokens": [
					50622,
					747,
					295,
					300,
					3601,
					300,
					393,
					652,
					291,
					257,
					709,
					1101,
					7318,
					11403,
					13,
					865,
					13,
					400,
					286,
					519,
					644,
					295,
					264,
					50940
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 18,
				"seek": 8338,
				"start": 94.88,
				"end": 100.06,
				"text": " conversation today is not just going to be about what those learnings are, but how you could derive",
				"tokens": [
					50940,
					3761,
					965,
					307,
					406,
					445,
					516,
					281,
					312,
					466,
					437,
					729,
					2539,
					82,
					366,
					11,
					457,
					577,
					291,
					727,
					28446,
					51199
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 19,
				"seek": 8338,
				"start": 100.06,
				"end": 104.44,
				"text": " them for first principles so that you can go on and perhaps find new things on your own.",
				"tokens": [
					51199,
					552,
					337,
					700,
					9156,
					370,
					300,
					291,
					393,
					352,
					322,
					293,
					4317,
					915,
					777,
					721,
					322,
					428,
					1065,
					13,
					51418
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 20,
				"seek": 8338,
				"start": 105.06,
				"end": 108.38,
				"text": " Because I think that's the most valuable insight. It's not about being able to copy what other people",
				"tokens": [
					51449,
					1436,
					286,
					519,
					300,
					311,
					264,
					881,
					8263,
					11269,
					13,
					467,
					311,
					406,
					466,
					885,
					1075,
					281,
					5055,
					437,
					661,
					561,
					51615
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 21,
				"seek": 8338,
				"start": 108.38,
				"end": 112.14,
				"text": " do. It's being able to figure out what is the thing that you can invent and help the rest of us",
				"tokens": [
					51615,
					360,
					13,
					467,
					311,
					885,
					1075,
					281,
					2573,
					484,
					437,
					307,
					264,
					551,
					300,
					291,
					393,
					7962,
					293,
					854,
					264,
					1472,
					295,
					505,
					51803
				],
				"temperature": 0,
				"avg_logprob": -0.064849,
				"compression_ratio": 1.8074534,
				"no_speech_prob": 1.7901164e-12
			},
			{
				"id": 22,
				"seek": 11214,
				"start": 112.14,
				"end": 113.12,
				"text": " along the way.",
				"tokens": [
					50365,
					2051,
					264,
					636,
					13,
					50414
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 23,
				"seek": 11214,
				"start": 113.18,
				"end": 113.98,
				"text": " Because if all of us",
				"tokens": [
					50417,
					1436,
					498,
					439,
					295,
					505,
					50457
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 24,
				"seek": 11214,
				"start": 113.98,
				"end": 115.06,
				"text": " with all our different backgrounds",
				"tokens": [
					50457,
					365,
					439,
					527,
					819,
					17336,
					50511
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 25,
				"seek": 11214,
				"start": 115.06,
				"end": 116.64,
				"text": " can go and investigate these areas,",
				"tokens": [
					50511,
					393,
					352,
					293,
					15013,
					613,
					3179,
					11,
					50590
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 26,
				"seek": 11214,
				"start": 117.4,
				"end": 118.26,
				"text": " then we can converge",
				"tokens": [
					50628,
					550,
					321,
					393,
					41881,
					50671
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 27,
				"seek": 11214,
				"start": 118.26,
				"end": 120.28,
				"text": " on what makes good AI pipelines way faster",
				"tokens": [
					50671,
					322,
					437,
					1669,
					665,
					7318,
					40168,
					636,
					4663,
					50772
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 28,
				"seek": 11214,
				"start": 120.28,
				"end": 123.36,
				"text": " as a general community building this stuff out.",
				"tokens": [
					50772,
					382,
					257,
					2674,
					1768,
					2390,
					341,
					1507,
					484,
					13,
					50926
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 29,
				"seek": 11214,
				"start": 124.4,
				"end": 125.98,
				"text": " So with that-",
				"tokens": [
					50978,
					407,
					365,
					300,
					12,
					51057
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 30,
				"seek": 11214,
				"start": 125.98,
				"end": 126.78,
				"text": " Do you want to do quick intros?",
				"tokens": [
					51057,
					1144,
					291,
					528,
					281,
					360,
					1702,
					560,
					2635,
					30,
					51097
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 31,
				"seek": 11214,
				"start": 126.86,
				"end": 127.9,
				"text": " And then I have a quick announcement",
				"tokens": [
					51101,
					400,
					550,
					286,
					362,
					257,
					1702,
					12847,
					51153
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 32,
				"seek": 11214,
				"start": 127.9,
				"end": 129.1,
				"text": " and then let's get into it.",
				"tokens": [
					51153,
					293,
					550,
					718,
					311,
					483,
					666,
					309,
					13,
					51213
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 33,
				"seek": 11214,
				"start": 130.22,
				"end": 131.12,
				"text": " Cool. So I'm Dex.",
				"tokens": [
					51269,
					8561,
					13,
					407,
					286,
					478,
					1346,
					87,
					13,
					51314
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 34,
				"seek": 11214,
				"start": 132.52,
				"end": 134.16,
				"text": " Well, the announcement is basically",
				"tokens": [
					51384,
					1042,
					11,
					264,
					12847,
					307,
					1936,
					51466
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 35,
				"seek": 11214,
				"start": 134.16,
				"end": 135.46,
				"text": " if you're new to the show,",
				"tokens": [
					51466,
					498,
					291,
					434,
					777,
					281,
					264,
					855,
					11,
					51531
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 36,
				"seek": 11214,
				"start": 135.56,
				"end": 136.92,
				"text": " we post every episode on GitHub.",
				"tokens": [
					51536,
					321,
					2183,
					633,
					3500,
					322,
					23331,
					13,
					51604
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 37,
				"seek": 11214,
				"start": 137.44,
				"end": 139.26,
				"text": " And so you can come here and see all of them.",
				"tokens": [
					51630,
					400,
					370,
					291,
					393,
					808,
					510,
					293,
					536,
					439,
					295,
					552,
					13,
					51721
				],
				"temperature": 0,
				"avg_logprob": -0.20792426,
				"compression_ratio": 1.6245847,
				"no_speech_prob": 3.559972e-12
			},
			{
				"id": 38,
				"seek": 13926,
				"start": 139.26,
				"end": 145.02,
				"text": " if you're looking for the recaps of the context engineering for AI agent stuff you can find that",
				"tokens": [
					50365,
					498,
					291,
					434,
					1237,
					337,
					264,
					43086,
					1878,
					295,
					264,
					4319,
					7043,
					337,
					7318,
					9461,
					1507,
					291,
					393,
					915,
					300,
					50653
				],
				"temperature": 0,
				"avg_logprob": -0.11012377,
				"compression_ratio": 1.7765151,
				"no_speech_prob": 1.6045086e-12
			},
			{
				"id": 39,
				"seek": 13926,
				"start": 145.02,
				"end": 150.54,
				"text": " here we've done a lot of episodes on context engineering in the past and so just another",
				"tokens": [
					50653,
					510,
					321,
					600,
					1096,
					257,
					688,
					295,
					9313,
					322,
					4319,
					7043,
					294,
					264,
					1791,
					293,
					370,
					445,
					1071,
					50929
				],
				"temperature": 0,
				"avg_logprob": -0.11012377,
				"compression_ratio": 1.7765151,
				"no_speech_prob": 1.6045086e-12
			},
			{
				"id": 40,
				"seek": 13926,
				"start": 150.54,
				"end": 154.9,
				"text": " reminder encouragement we put all of the links all the show notes will be here in the get repo",
				"tokens": [
					50929,
					13548,
					25683,
					321,
					829,
					439,
					295,
					264,
					6123,
					439,
					264,
					855,
					5570,
					486,
					312,
					510,
					294,
					264,
					483,
					49040,
					51147
				],
				"temperature": 0,
				"avg_logprob": -0.11012377,
				"compression_ratio": 1.7765151,
				"no_speech_prob": 1.6045086e-12
			},
			{
				"id": 41,
				"seek": 13926,
				"start": 154.9,
				"end": 159.36,
				"text": " including links to sign up for the next episode and all the whiteboards that we publish",
				"tokens": [
					51147,
					3009,
					6123,
					281,
					1465,
					493,
					337,
					264,
					958,
					3500,
					293,
					439,
					264,
					2418,
					17228,
					300,
					321,
					11374,
					51370
				],
				"temperature": 0,
				"avg_logprob": -0.11012377,
				"compression_ratio": 1.7765151,
				"no_speech_prob": 1.6045086e-12
			},
			{
				"id": 42,
				"seek": 13926,
				"start": 159.36,
				"end": 166.18,
				"text": " so without further ado I'm Dex I'm the founder of Human Layer we've been doing this show since March",
				"tokens": [
					51370,
					370,
					1553,
					3052,
					22450,
					286,
					478,
					1346,
					87,
					286,
					478,
					264,
					14917,
					295,
					10294,
					35166,
					321,
					600,
					668,
					884,
					341,
					855,
					1670,
					6129,
					51711
				],
				"temperature": 0,
				"avg_logprob": -0.11012377,
				"compression_ratio": 1.7765151,
				"no_speech_prob": 1.6045086e-12
			},
			{
				"id": 43,
				"seek": 16618,
				"start": 166.18,
				"end": 168.74,
				"text": " and this is AI that works",
				"tokens": [
					50365,
					293,
					341,
					307,
					7318,
					300,
					1985,
					50493
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 44,
				"seek": 16618,
				"start": 168.74,
				"end": 170.9,
				"text": " where we teach you to be a better AI engineer.",
				"tokens": [
					50493,
					689,
					321,
					2924,
					291,
					281,
					312,
					257,
					1101,
					7318,
					11403,
					13,
					50601
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 45,
				"seek": 16618,
				"start": 171.6,
				"end": 171.86,
				"text": " I don't know.",
				"tokens": [
					50636,
					286,
					500,
					380,
					458,
					13,
					50649
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 46,
				"seek": 16618,
				"start": 172.04,
				"end": 173.96,
				"text": " You're going to have to pitch the show for me.",
				"tokens": [
					50658,
					509,
					434,
					516,
					281,
					362,
					281,
					7293,
					264,
					855,
					337,
					385,
					13,
					50754
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 47,
				"seek": 16618,
				"start": 175.16,
				"end": 175.78,
				"text": " I'm Viob.",
				"tokens": [
					50814,
					286,
					478,
					6626,
					996,
					13,
					50845
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 48,
				"seek": 16618,
				"start": 175.96,
				"end": 176.6,
				"text": " I write some code.",
				"tokens": [
					50854,
					286,
					2464,
					512,
					3089,
					13,
					50886
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 49,
				"seek": 16618,
				"start": 177.6,
				"end": 178.44,
				"text": " That's about it.",
				"tokens": [
					50936,
					663,
					311,
					466,
					309,
					13,
					50978
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 50,
				"seek": 16618,
				"start": 178.56,
				"end": 179.58,
				"text": " Let's talk about fun stuff.",
				"tokens": [
					50984,
					961,
					311,
					751,
					466,
					1019,
					1507,
					13,
					51035
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 51,
				"seek": 16618,
				"start": 179.68,
				"end": 180.4,
				"text": " Let's get to the meat.",
				"tokens": [
					51040,
					961,
					311,
					483,
					281,
					264,
					4615,
					13,
					51076
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 52,
				"seek": 16618,
				"start": 181.46,
				"end": 182.14,
				"text": " Let's do it.",
				"tokens": [
					51129,
					961,
					311,
					360,
					309,
					13,
					51163
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 53,
				"seek": 16618,
				"start": 183.14,
				"end": 183.78,
				"text": " One second.",
				"tokens": [
					51213,
					1485,
					1150,
					13,
					51245
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 54,
				"seek": 16618,
				"start": 183.96,
				"end": 184.86,
				"text": " Context engineering.",
				"tokens": [
					51254,
					4839,
					3828,
					7043,
					13,
					51299
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 55,
				"seek": 16618,
				"start": 190.98,
				"end": 191.42,
				"text": " ESP.",
				"tokens": [
					51605,
					12564,
					47,
					13,
					51627
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 56,
				"seek": 16618,
				"start": 192.02,
				"end": 192.38,
				"text": " Cool.",
				"tokens": [
					51657,
					8561,
					13,
					51675
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 57,
				"seek": 16618,
				"start": 193.9,
				"end": 195.52,
				"text": " And now I'm going to go ahead and screen share.",
				"tokens": [
					51751,
					400,
					586,
					286,
					478,
					516,
					281,
					352,
					2286,
					293,
					2568,
					2073,
					13,
					51832
				],
				"temperature": 0,
				"avg_logprob": -0.35499284,
				"compression_ratio": 1.550926,
				"no_speech_prob": 9.143539e-13
			},
			{
				"id": 58,
				"seek": 19618,
				"start": 196.18,
				"end": 203.34,
				"text": " So I want to go straight into the Manus paper and just go talk about this from the very",
				"tokens": [
					50365,
					407,
					286,
					528,
					281,
					352,
					2997,
					666,
					264,
					2458,
					301,
					3035,
					293,
					445,
					352,
					751,
					466,
					341,
					490,
					264,
					588,
					50723
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 59,
				"seek": 19618,
				"start": 203.34,
				"end": 203.7,
				"text": " beginning.",
				"tokens": [
					50723,
					2863,
					13,
					50741
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 60,
				"seek": 19618,
				"start": 204.7,
				"end": 207.5,
				"text": " Dexter, I don't have a second screen today, so you're going to have to keep on the lookout",
				"tokens": [
					50791,
					1346,
					36671,
					11,
					286,
					500,
					380,
					362,
					257,
					1150,
					2568,
					965,
					11,
					370,
					291,
					434,
					516,
					281,
					362,
					281,
					1066,
					322,
					264,
					41025,
					50931
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 61,
				"seek": 19618,
				"start": 207.5,
				"end": 209.8,
				"text": " for questions people ask and just let me know.",
				"tokens": [
					50931,
					337,
					1651,
					561,
					1029,
					293,
					445,
					718,
					385,
					458,
					13,
					51046
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 62,
				"seek": 19618,
				"start": 210.28,
				"end": 210.4,
				"text": " Yep.",
				"tokens": [
					51070,
					7010,
					13,
					51076
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 63,
				"seek": 19618,
				"start": 211.24,
				"end": 212.5,
				"text": " You want to make me a host?",
				"tokens": [
					51118,
					509,
					528,
					281,
					652,
					385,
					257,
					3975,
					30,
					51181
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 64,
				"seek": 19618,
				"start": 213.26,
				"end": 213.52,
				"text": " Yes.",
				"tokens": [
					51219,
					1079,
					13,
					51232
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 65,
				"seek": 19618,
				"start": 213.68,
				"end": 215.06,
				"text": " I'm surprised I did not do that already.",
				"tokens": [
					51240,
					286,
					478,
					6100,
					286,
					630,
					406,
					360,
					300,
					1217,
					13,
					51309
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 66,
				"seek": 19618,
				"start": 217.12,
				"end": 217.6,
				"text": " All right.",
				"tokens": [
					51412,
					1057,
					558,
					13,
					51436
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 67,
				"seek": 19618,
				"start": 217.96,
				"end": 220.18,
				"text": " So the first thing is, like, what is Manus?",
				"tokens": [
					51454,
					407,
					264,
					700,
					551,
					307,
					11,
					411,
					11,
					437,
					307,
					2458,
					301,
					30,
					51565
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 68,
				"seek": 19618,
				"start": 220.3,
				"end": 223.62,
				"text": " For those of you that haven't seen, it's a pretty, pretty cool project that got a lot",
				"tokens": [
					51571,
					1171,
					729,
					295,
					291,
					300,
					2378,
					380,
					1612,
					11,
					309,
					311,
					257,
					1238,
					11,
					1238,
					1627,
					1716,
					300,
					658,
					257,
					688,
					51737
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 69,
				"seek": 19618,
				"start": 223.62,
				"end": 224.18,
				"text": " of popularity.",
				"tokens": [
					51737,
					295,
					19301,
					13,
					51765
				],
				"temperature": 0,
				"avg_logprob": -0.26189148,
				"compression_ratio": 1.5858586,
				"no_speech_prob": 1.8833485e-12
			},
			{
				"id": 70,
				"seek": 22418,
				"start": 224.18,
				"end": 226.18,
				"text": " it's based",
				"tokens": [
					50365,
					309,
					311,
					2361,
					50465
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 71,
				"seek": 22418,
				"start": 226.18,
				"end": 227.26,
				"text": " I've not signed in",
				"tokens": [
					50465,
					286,
					600,
					406,
					8175,
					294,
					50519
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 72,
				"seek": 22418,
				"start": 227.26,
				"end": 230.5,
				"text": " they still have the videos",
				"tokens": [
					50519,
					436,
					920,
					362,
					264,
					2145,
					50681
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 73,
				"seek": 22418,
				"start": 230.5,
				"end": 232.36,
				"text": " on the website where you can see",
				"tokens": [
					50681,
					322,
					264,
					3144,
					689,
					291,
					393,
					536,
					50774
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 74,
				"seek": 22418,
				"start": 232.36,
				"end": 234.98,
				"text": " the example use cases",
				"tokens": [
					50774,
					264,
					1365,
					764,
					3331,
					50905
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 75,
				"seek": 22418,
				"start": 234.98,
				"end": 236.7,
				"text": " yeah I think it's probably worth",
				"tokens": [
					50905,
					1338,
					286,
					519,
					309,
					311,
					1391,
					3163,
					50991
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 76,
				"seek": 22418,
				"start": 236.7,
				"end": 238.58,
				"text": " looking at because I think it tells you",
				"tokens": [
					50991,
					1237,
					412,
					570,
					286,
					519,
					309,
					5112,
					291,
					51085
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 77,
				"seek": 22418,
				"start": 238.58,
				"end": 240.5,
				"text": " roughly what it is it basically kind of just does stuff",
				"tokens": [
					51085,
					9810,
					437,
					309,
					307,
					309,
					1936,
					733,
					295,
					445,
					775,
					1507,
					51181
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 78,
				"seek": 22418,
				"start": 240.5,
				"end": 242.54,
				"text": " it's an agent that is very generalized and does stuff",
				"tokens": [
					51181,
					309,
					311,
					364,
					9461,
					300,
					307,
					588,
					44498,
					293,
					775,
					1507,
					51283
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 79,
				"seek": 22418,
				"start": 242.54,
				"end": 244.86,
				"text": " and it's really impressive",
				"tokens": [
					51283,
					293,
					309,
					311,
					534,
					8992,
					51399
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 80,
				"seek": 22418,
				"start": 244.86,
				"end": 246.1,
				"text": " what it was able to do",
				"tokens": [
					51399,
					437,
					309,
					390,
					1075,
					281,
					360,
					51461
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 81,
				"seek": 22418,
				"start": 246.1,
				"end": 248.56,
				"text": " from",
				"tokens": [
					51461,
					490,
					51584
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 82,
				"seek": 22418,
				"start": 248.56,
				"end": 250.48,
				"text": " make me sign in I'll show you guys",
				"tokens": [
					51584,
					652,
					385,
					1465,
					294,
					286,
					603,
					855,
					291,
					1074,
					51680
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 83,
				"seek": 22418,
				"start": 250.48,
				"end": 252.78,
				"text": " and I think part of what",
				"tokens": [
					51680,
					293,
					286,
					519,
					644,
					295,
					437,
					51795
				],
				"temperature": 0,
				"avg_logprob": -0.20023341,
				"compression_ratio": 1.7184874,
				"no_speech_prob": 2.21054e-12
			},
			{
				"id": 84,
				"seek": 25278,
				"start": 252.78,
				"end": 258.22,
				"text": " made this app so viral and impressive is the fact that these folks figured out some ways to",
				"tokens": [
					50365,
					1027,
					341,
					724,
					370,
					16132,
					293,
					8992,
					307,
					264,
					1186,
					300,
					613,
					4024,
					8932,
					484,
					512,
					2098,
					281,
					50637
				],
				"temperature": 0,
				"avg_logprob": -0.048953045,
				"compression_ratio": 1.8358779,
				"no_speech_prob": 1.7971271e-12
			},
			{
				"id": 85,
				"seek": 25278,
				"start": 258.22,
				"end": 265.96,
				"text": " stop asking just let me show you what to do but you'll notice okay i don't have enough credits",
				"tokens": [
					50637,
					1590,
					3365,
					445,
					718,
					385,
					855,
					291,
					437,
					281,
					360,
					457,
					291,
					603,
					3449,
					1392,
					741,
					500,
					380,
					362,
					1547,
					16816,
					51024
				],
				"temperature": 0,
				"avg_logprob": -0.048953045,
				"compression_ratio": 1.8358779,
				"no_speech_prob": 1.7971271e-12
			},
			{
				"id": 86,
				"seek": 25278,
				"start": 265.96,
				"end": 270.26,
				"text": " okay i already used all my credits when i was doing this earlier but um part of what made the",
				"tokens": [
					51024,
					1392,
					741,
					1217,
					1143,
					439,
					452,
					16816,
					562,
					741,
					390,
					884,
					341,
					3071,
					457,
					1105,
					644,
					295,
					437,
					1027,
					264,
					51239
				],
				"temperature": 0,
				"avg_logprob": -0.048953045,
				"compression_ratio": 1.8358779,
				"no_speech_prob": 1.7971271e-12
			},
			{
				"id": 87,
				"seek": 25278,
				"start": 270.26,
				"end": 276.9,
				"text": " app really impressive was just how broad of a task i was able to do and how like how high quality was",
				"tokens": [
					51239,
					724,
					534,
					8992,
					390,
					445,
					577,
					4152,
					295,
					257,
					5633,
					741,
					390,
					1075,
					281,
					360,
					293,
					577,
					411,
					577,
					1090,
					3125,
					390,
					51571
				],
				"temperature": 0,
				"avg_logprob": -0.048953045,
				"compression_ratio": 1.8358779,
				"no_speech_prob": 1.7971271e-12
			},
			{
				"id": 88,
				"seek": 25278,
				"start": 276.9,
				"end": 281.58,
				"text": " able to handle for the generality of what it did and there's small things like i don't know if you",
				"tokens": [
					51571,
					1075,
					281,
					4813,
					337,
					264,
					1337,
					1860,
					295,
					437,
					309,
					630,
					293,
					456,
					311,
					1359,
					721,
					411,
					741,
					500,
					380,
					458,
					498,
					291,
					51805
				],
				"temperature": 0,
				"avg_logprob": -0.048953045,
				"compression_ratio": 1.8358779,
				"no_speech_prob": 1.7971271e-12
			},
			{
				"id": 89,
				"seek": 28158,
				"start": 281.58,
				"end": 286.32,
				"text": " guys noticed just like when i was even on the home page there's small things here that get really",
				"tokens": [
					50365,
					1074,
					5694,
					445,
					411,
					562,
					741,
					390,
					754,
					322,
					264,
					1280,
					3028,
					456,
					311,
					1359,
					721,
					510,
					300,
					483,
					534,
					50602
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 90,
				"seek": 28158,
				"start": 286.32,
				"end": 291.4,
				"text": " fast i do this and it immediately fills in these things how is it so freaking fast we're all using",
				"tokens": [
					50602,
					2370,
					741,
					360,
					341,
					293,
					309,
					4258,
					22498,
					294,
					613,
					721,
					577,
					307,
					309,
					370,
					14612,
					2370,
					321,
					434,
					439,
					1228,
					50856
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 91,
				"seek": 28158,
				"start": 291.4,
				"end": 295.32,
				"text": " the same models it's not like they have anything special under the hood and these small things",
				"tokens": [
					50856,
					264,
					912,
					5245,
					309,
					311,
					406,
					411,
					436,
					362,
					1340,
					2121,
					833,
					264,
					13376,
					293,
					613,
					1359,
					721,
					51052
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 92,
				"seek": 28158,
				"start": 295.32,
				"end": 298.92,
				"text": " around how they design the ux how they design the interfaces how they make the models slightly",
				"tokens": [
					51052,
					926,
					577,
					436,
					1715,
					264,
					344,
					87,
					577,
					436,
					1715,
					264,
					28416,
					577,
					436,
					652,
					264,
					5245,
					4748,
					51232
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 93,
				"seek": 28158,
				"start": 298.92,
				"end": 303.7,
				"text": " faster are what i think make madness go viral because one of the first apps to go do that",
				"tokens": [
					51232,
					4663,
					366,
					437,
					741,
					519,
					652,
					28736,
					352,
					16132,
					570,
					472,
					295,
					264,
					700,
					7733,
					281,
					352,
					360,
					300,
					51471
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 94,
				"seek": 28158,
				"start": 303.7,
				"end": 309.02,
				"text": " i think when they announced on their techniques one of the most talked about techniques was around",
				"tokens": [
					51471,
					741,
					519,
					562,
					436,
					7548,
					322,
					641,
					7512,
					472,
					295,
					264,
					881,
					2825,
					466,
					7512,
					390,
					926,
					51737
				],
				"temperature": 0,
				"avg_logprob": -0.042746145,
				"compression_ratio": 1.9624573,
				"no_speech_prob": 2.0125032e-12
			},
			{
				"id": 95,
				"seek": 30902,
				"start": 309.02,
				"end": 315.26,
				"text": " this idea of a kv cache but i'll talk about that in a little bit later um it talked about another",
				"tokens": [
					50365,
					341,
					1558,
					295,
					257,
					350,
					85,
					19459,
					457,
					741,
					603,
					751,
					466,
					300,
					294,
					257,
					707,
					857,
					1780,
					1105,
					309,
					2825,
					466,
					1071,
					50677
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 96,
				"seek": 30902,
				"start": 315.26,
				"end": 320.92,
				"text": " concept called masking not removing and there's this concept when you use mcps which goes around",
				"tokens": [
					50677,
					3410,
					1219,
					31226,
					406,
					12720,
					293,
					456,
					311,
					341,
					3410,
					562,
					291,
					764,
					275,
					66,
					1878,
					597,
					1709,
					926,
					50960
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 97,
				"seek": 30902,
				"start": 320.92,
				"end": 325.4,
				"text": " this idea of like how do you deal with the fact that i don't want to give my model every single",
				"tokens": [
					50960,
					341,
					1558,
					295,
					411,
					577,
					360,
					291,
					2028,
					365,
					264,
					1186,
					300,
					741,
					500,
					380,
					528,
					281,
					976,
					452,
					2316,
					633,
					2167,
					51184
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 98,
				"seek": 30902,
				"start": 325.4,
				"end": 330.74,
				"text": " tool all the time and why you don't want to do that it makes sense because i don't want my model",
				"tokens": [
					51184,
					2290,
					439,
					264,
					565,
					293,
					983,
					291,
					500,
					380,
					528,
					281,
					360,
					300,
					309,
					1669,
					2020,
					570,
					741,
					500,
					380,
					528,
					452,
					2316,
					51451
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 99,
				"seek": 30902,
				"start": 330.74,
				"end": 334.82,
				"text": " to be dumb and like have choices between tools that you don't have if it's going to choose between",
				"tokens": [
					51451,
					281,
					312,
					10316,
					293,
					411,
					362,
					7994,
					1296,
					3873,
					300,
					291,
					500,
					380,
					362,
					498,
					309,
					311,
					516,
					281,
					2826,
					1296,
					51655
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 100,
				"seek": 30902,
				"start": 334.82,
				"end": 338.66,
				"text": " writing a document and i know the user wants a notion document don't give it the google docs",
				"tokens": [
					51655,
					3579,
					257,
					4166,
					293,
					741,
					458,
					264,
					4195,
					2738,
					257,
					10710,
					4166,
					500,
					380,
					976,
					309,
					264,
					20742,
					45623,
					51847
				],
				"temperature": 0,
				"avg_logprob": -0.05185871,
				"compression_ratio": 1.93,
				"no_speech_prob": 1.351274e-12
			},
			{
				"id": 101,
				"seek": 33866,
				"start": 338.66,
				"end": 344.2,
				"text": " tools that is literally just going to confuse it that makes a lot of sense but there's trade-offs",
				"tokens": [
					50365,
					3873,
					300,
					307,
					3736,
					445,
					516,
					281,
					28584,
					309,
					300,
					1669,
					257,
					688,
					295,
					2020,
					457,
					456,
					311,
					4923,
					12,
					19231,
					50642
				],
				"temperature": 0,
				"avg_logprob": -0.07309665,
				"compression_ratio": 1.7886792,
				"no_speech_prob": 2.6148309e-12
			},
			{
				"id": 102,
				"seek": 33866,
				"start": 344.2,
				"end": 348.78,
				"text": " in this and what happens so we'll talk about that in a second it talked about how you actually",
				"tokens": [
					50642,
					294,
					341,
					293,
					437,
					2314,
					370,
					321,
					603,
					751,
					466,
					300,
					294,
					257,
					1150,
					309,
					2825,
					466,
					577,
					291,
					767,
					50871
				],
				"temperature": 0,
				"avg_logprob": -0.07309665,
				"compression_ratio": 1.7886792,
				"no_speech_prob": 2.6148309e-12
			},
			{
				"id": 103,
				"seek": 33866,
				"start": 348.78,
				"end": 353.6,
				"text": " engineer for super large context and context compression i didn't use that word specifically",
				"tokens": [
					50871,
					11403,
					337,
					1687,
					2416,
					4319,
					293,
					4319,
					19355,
					741,
					994,
					380,
					764,
					300,
					1349,
					4682,
					51112
				],
				"temperature": 0,
				"avg_logprob": -0.07309665,
				"compression_ratio": 1.7886792,
				"no_speech_prob": 2.6148309e-12
			},
			{
				"id": 104,
				"seek": 33866,
				"start": 353.6,
				"end": 358.2,
				"text": " but i think that's what they were hinting at which how to intelligently compress context",
				"tokens": [
					51112,
					457,
					741,
					519,
					300,
					311,
					437,
					436,
					645,
					12075,
					278,
					412,
					597,
					577,
					281,
					5613,
					2276,
					14778,
					4319,
					51342
				],
				"temperature": 0,
				"avg_logprob": -0.07309665,
				"compression_ratio": 1.7886792,
				"no_speech_prob": 2.6148309e-12
			},
			{
				"id": 105,
				"seek": 33866,
				"start": 358.2,
				"end": 364.56,
				"text": " when you have it's using right yeah this is using basically the same tools that agents already know",
				"tokens": [
					51342,
					562,
					291,
					362,
					309,
					311,
					1228,
					558,
					1338,
					341,
					307,
					1228,
					1936,
					264,
					912,
					3873,
					300,
					12554,
					1217,
					458,
					51660
				],
				"temperature": 0,
				"avg_logprob": -0.07309665,
				"compression_ratio": 1.7886792,
				"no_speech_prob": 2.6148309e-12
			},
			{
				"id": 106,
				"seek": 36456,
				"start": 364.56,
				"end": 365.96,
				"text": " how to use a file system basically.",
				"tokens": [
					50365,
					577,
					281,
					764,
					257,
					3991,
					1185,
					1936,
					13,
					50435
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 107,
				"seek": 36456,
				"start": 366.2,
				"end": 367.46,
				"text": " And so it becomes a really nice way",
				"tokens": [
					50447,
					400,
					370,
					309,
					3643,
					257,
					534,
					1481,
					636,
					50510
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 108,
				"seek": 36456,
				"start": 367.46,
				"end": 369.58,
				"text": " for the agent to organize his own memories, right?",
				"tokens": [
					50510,
					337,
					264,
					9461,
					281,
					13859,
					702,
					1065,
					8495,
					11,
					558,
					30,
					50616
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 109,
				"seek": 36456,
				"start": 370.1,
				"end": 371.86,
				"text": " Kind of, at least from what I understood.",
				"tokens": [
					50642,
					9242,
					295,
					11,
					412,
					1935,
					490,
					437,
					286,
					7320,
					13,
					50730
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 110,
				"seek": 36456,
				"start": 371.98,
				"end": 372.86,
				"text": " It might be slightly different.",
				"tokens": [
					50736,
					467,
					1062,
					312,
					4748,
					819,
					13,
					50780
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 111,
				"seek": 36456,
				"start": 373.94,
				"end": 376.02,
				"text": " And I think the other thing that I thought was",
				"tokens": [
					50834,
					400,
					286,
					519,
					264,
					661,
					551,
					300,
					286,
					1194,
					390,
					50938
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 112,
				"seek": 36456,
				"start": 376.02,
				"end": 378.42,
				"text": " when he talked about this concept",
				"tokens": [
					50938,
					562,
					415,
					2825,
					466,
					341,
					3410,
					51058
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 113,
				"seek": 36456,
				"start": 378.42,
				"end": 379.88,
				"text": " of how it actually summarizes steps,",
				"tokens": [
					51058,
					295,
					577,
					309,
					767,
					14611,
					5660,
					4439,
					11,
					51131
				],
				"temperature": 0,
				"avg_logprob": -0.13905385,
				"compression_ratio": 1.6910828,
				"no_speech_prob": 1.6948866e-12
			},
			{
				"id": 114,
				"seek": 380,
				"start": 379.94,
				"end": 389.38428,
				"text": " this was very similar if you were here in the last episode when Dexter was talking about how he does summarization for his coding workflow where he does like a research step a planning step and then an execution step",
				"tokens": [
					51134,
					341,
					390,
					588,
					2531,
					498,
					291,
					645,
					510,
					294,
					264,
					1036,
					3500,
					51224,
					51224,
					562,
					1346,
					36671,
					390,
					1417,
					466,
					51332,
					51332,
					577,
					415,
					775,
					50637,
					50637,
					337,
					702,
					17720,
					20993,
					11,
					50689,
					50689,
					689,
					415,
					775,
					411,
					257,
					2132,
					1823,
					11,
					257,
					5038,
					1823,
					11,
					50772,
					50774,
					293,
					550,
					364,
					15058,
					1823,
					13,
					50834
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 115,
				"seek": 380,
				"start": 390.12427,
				"end": 392.10428,
				"text": " And the whole idea is you're able to compress context",
				"tokens": [
					50871,
					400,
					264,
					1379,
					1558,
					307,
					291,
					434,
					1075,
					281,
					14778,
					4319,
					50970
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 116,
				"seek": 380,
				"start": 392.10428,
				"end": 393.56427,
				"text": " in a more concise way,",
				"tokens": [
					50970,
					294,
					257,
					544,
					44882,
					636,
					11,
					51043
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 117,
				"seek": 380,
				"start": 394.20428,
				"end": 397.48428,
				"text": " simply by some of the techniques that they discuss here.",
				"tokens": [
					51075,
					2935,
					538,
					512,
					295,
					264,
					7512,
					300,
					436,
					2248,
					510,
					13,
					51239
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 118,
				"seek": 380,
				"start": 398.66428,
				"end": 400.88428,
				"text": " They talked about keeping the wrong stuff in",
				"tokens": [
					51298,
					814,
					2825,
					466,
					5145,
					264,
					2085,
					1507,
					294,
					51409
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 119,
				"seek": 380,
				"start": 400.88428,
				"end": 402.28427,
				"text": " and like how that hurts your system.",
				"tokens": [
					51409,
					293,
					411,
					577,
					300,
					11051,
					428,
					1185,
					13,
					51479
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 120,
				"seek": 380,
				"start": 402.78427,
				"end": 405.12427,
				"text": " And then thing that I often say,",
				"tokens": [
					51504,
					400,
					550,
					551,
					300,
					286,
					2049,
					584,
					11,
					51621
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 121,
				"seek": 380,
				"start": 405.52426,
				"end": 406.9243,
				"text": " which is few-shot prompting sucks.",
				"tokens": [
					51641,
					597,
					307,
					1326,
					12,
					18402,
					12391,
					278,
					15846,
					13,
					51711
				],
				"temperature": 0,
				"avg_logprob": -0.2036407,
				"compression_ratio": 1.6755853,
				"no_speech_prob": 1.6620777e-12
			},
			{
				"id": 122,
				"seek": 3072,
				"start": 406.9243,
				"end": 413.66428,
				"text": " they talk about why they also found few shot prompting sucks more often than not but we'll",
				"tokens": [
					50365,
					436,
					751,
					466,
					983,
					436,
					611,
					1352,
					1326,
					3347,
					12391,
					278,
					15846,
					544,
					2049,
					813,
					406,
					457,
					321,
					603,
					50702
				],
				"temperature": 0,
				"avg_logprob": -0.082117654,
				"compression_ratio": 1.7592592,
				"no_speech_prob": 1.2995538e-12
			},
			{
				"id": 123,
				"seek": 3072,
				"start": 413.66428,
				"end": 418.88428,
				"text": " go into some of these now for everyone else just to give us an idea how many of you have actually",
				"tokens": [
					50702,
					352,
					666,
					512,
					295,
					613,
					586,
					337,
					1518,
					1646,
					445,
					281,
					976,
					505,
					364,
					1558,
					577,
					867,
					295,
					291,
					362,
					767,
					50963
				],
				"temperature": 0,
				"avg_logprob": -0.082117654,
				"compression_ratio": 1.7592592,
				"no_speech_prob": 1.2995538e-12
			},
			{
				"id": 124,
				"seek": 3072,
				"start": 418.88428,
				"end": 422.84427,
				"text": " read through this or have ideas around what this thing is so we know what kind of conversation",
				"tokens": [
					50963,
					1401,
					807,
					341,
					420,
					362,
					3487,
					926,
					437,
					341,
					551,
					307,
					370,
					321,
					458,
					437,
					733,
					295,
					3761,
					51161
				],
				"temperature": 0,
				"avg_logprob": -0.082117654,
				"compression_ratio": 1.7592592,
				"no_speech_prob": 1.2995538e-12
			},
			{
				"id": 125,
				"seek": 3072,
				"start": 422.84427,
				"end": 427.62427,
				"text": " we're going to be having today is this the one that mentions i think i saw it as you scrolled",
				"tokens": [
					51161,
					321,
					434,
					516,
					281,
					312,
					1419,
					965,
					307,
					341,
					264,
					472,
					300,
					23844,
					741,
					519,
					741,
					1866,
					309,
					382,
					291,
					11369,
					292,
					51400
				],
				"temperature": 0,
				"avg_logprob": -0.082117654,
				"compression_ratio": 1.7592592,
				"no_speech_prob": 1.2995538e-12
			},
			{
				"id": 126,
				"seek": 3072,
				"start": 427.62427,
				"end": 432.74426,
				"text": " up the number of tool uses that an agent should make that's something i remember i can't remember",
				"tokens": [
					51400,
					493,
					264,
					1230,
					295,
					2290,
					4960,
					300,
					364,
					9461,
					820,
					652,
					300,
					311,
					746,
					741,
					1604,
					741,
					393,
					380,
					1604,
					51656
				],
				"temperature": 0,
				"avg_logprob": -0.082117654,
				"compression_ratio": 1.7592592,
				"no_speech_prob": 1.2995538e-12
			},
			{
				"id": 127,
				"seek": 5654,
				"start": 432.74426,
				"end": 434.30426,
				"text": " I don't know if this one talks about here or not.",
				"tokens": [
					50365,
					286,
					500,
					380,
					458,
					498,
					341,
					472,
					6686,
					466,
					510,
					420,
					406,
					13,
					50443
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 128,
				"seek": 5654,
				"start": 434.56427,
				"end": 435.42426,
				"text": " No, that's not this one.",
				"tokens": [
					50456,
					883,
					11,
					300,
					311,
					406,
					341,
					472,
					13,
					50499
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 129,
				"seek": 5654,
				"start": 435.46426,
				"end": 436.98428,
				"text": " This one does not talk about the number of tool uses.",
				"tokens": [
					50501,
					639,
					472,
					775,
					406,
					751,
					466,
					264,
					1230,
					295,
					2290,
					4960,
					13,
					50577
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 130,
				"seek": 5654,
				"start": 437.78427,
				"end": 439.90427,
				"text": " Okay, so that was like five or something that I remember.",
				"tokens": [
					50617,
					1033,
					11,
					370,
					300,
					390,
					411,
					1732,
					420,
					746,
					300,
					286,
					1604,
					13,
					50723
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 131,
				"seek": 5654,
				"start": 440.6443,
				"end": 440.96426,
				"text": " Perfect.",
				"tokens": [
					50760,
					10246,
					13,
					50776
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 132,
				"seek": 5654,
				"start": 441.40427,
				"end": 443.00427,
				"text": " In that case, let's just start with the first one,",
				"tokens": [
					50798,
					682,
					300,
					1389,
					11,
					718,
					311,
					445,
					722,
					365,
					264,
					700,
					472,
					11,
					50878
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 133,
				"seek": 5654,
				"start": 443.24426,
				"end": 444.52426,
				"text": " designing around the KB cache.",
				"tokens": [
					50890,
					14685,
					926,
					264,
					591,
					33,
					19459,
					13,
					50954
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 134,
				"seek": 5654,
				"start": 444.5843,
				"end": 446.92426,
				"text": " I think that is the most interesting one out of all of them",
				"tokens": [
					50957,
					286,
					519,
					300,
					307,
					264,
					881,
					1880,
					472,
					484,
					295,
					439,
					295,
					552,
					51074
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 135,
				"seek": 5654,
				"start": 446.92426,
				"end": 451.32428,
				"text": " because it really inspires for everything else around this.",
				"tokens": [
					51074,
					570,
					309,
					534,
					32566,
					337,
					1203,
					1646,
					926,
					341,
					13,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 136,
				"seek": 5654,
				"start": 451.38428,
				"end": 453.02426,
				"text": " And there's no reason to actually do any of the pre-reading.",
				"tokens": [
					51297,
					400,
					456,
					311,
					572,
					1778,
					281,
					767,
					360,
					604,
					295,
					264,
					659,
					12,
					35908,
					13,
					51379
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 137,
				"seek": 5654,
				"start": 453.02426,
				"end": 455.20428,
				"text": " That's kind of why we have this conversation over here.",
				"tokens": [
					51379,
					663,
					311,
					733,
					295,
					983,
					321,
					362,
					341,
					3761,
					670,
					510,
					13,
					51488
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 138,
				"seek": 5654,
				"start": 455.56427,
				"end": 456.86426,
				"text": " Dexter and I do this stuff for fun,",
				"tokens": [
					51506,
					1346,
					36671,
					293,
					286,
					360,
					341,
					1507,
					337,
					1019,
					11,
					51571
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 139,
				"seek": 5654,
				"start": 457.00427,
				"end": 459.10428,
				"text": " and we just get to share about it with you, everyone.",
				"tokens": [
					51578,
					293,
					321,
					445,
					483,
					281,
					2073,
					466,
					309,
					365,
					291,
					11,
					1518,
					13,
					51683
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 140,
				"seek": 5654,
				"start": 460.02426,
				"end": 460.76428,
				"text": " So one second.",
				"tokens": [
					51729,
					407,
					472,
					1150,
					13,
					51766
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 141,
				"seek": 5654,
				"start": 460.90427,
				"end": 461.72427,
				"text": " I'm going to go screen share,",
				"tokens": [
					51773,
					286,
					478,
					516,
					281,
					352,
					2568,
					2073,
					11,
					51814
				],
				"temperature": 0,
				"avg_logprob": -0.17898972,
				"compression_ratio": 1.7588075,
				"no_speech_prob": 2.2104885e-12
			},
			{
				"id": 142,
				"seek": 8552,
				"start": 461.72427,
				"end": 464.0843,
				"text": " but I think the most important thing I'm going to want is a whiteboard.",
				"tokens": [
					50365,
					457,
					286,
					519,
					264,
					881,
					1021,
					551,
					286,
					478,
					516,
					281,
					528,
					307,
					257,
					2418,
					3787,
					13,
					50483
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 143,
				"seek": 8552,
				"start": 464.20428,
				"end": 465.68427,
				"text": " So let me pull that up, Dexter.",
				"tokens": [
					50489,
					407,
					718,
					385,
					2235,
					300,
					493,
					11,
					1346,
					36671,
					13,
					50563
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 144,
				"seek": 8552,
				"start": 466.26428,
				"end": 466.78427,
				"text": " Oh, yeah.",
				"tokens": [
					50592,
					876,
					11,
					1338,
					13,
					50618
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 145,
				"seek": 8552,
				"start": 466.82428,
				"end": 468.1443,
				"text": " Let me ship you a whiteboard.",
				"tokens": [
					50620,
					961,
					385,
					5374,
					291,
					257,
					2418,
					3787,
					13,
					50686
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 146,
				"seek": 8552,
				"start": 470.24426,
				"end": 473.82428,
				"text": " I'm just going to buy you $7 a month for a whiteboard account.",
				"tokens": [
					50791,
					286,
					478,
					445,
					516,
					281,
					2256,
					291,
					1848,
					22,
					257,
					1618,
					337,
					257,
					2418,
					3787,
					2696,
					13,
					50970
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 147,
				"seek": 8552,
				"start": 475.20428,
				"end": 477.98428,
				"text": " I don't know if you just like it because the best part is when you do it,",
				"tokens": [
					51039,
					286,
					500,
					380,
					458,
					498,
					291,
					445,
					411,
					309,
					570,
					264,
					1151,
					644,
					307,
					562,
					291,
					360,
					309,
					11,
					51178
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 148,
				"seek": 8552,
				"start": 478.04428,
				"end": 480.0843,
				"text": " I can ask you to take the screenshots and not me.",
				"tokens": [
					51181,
					286,
					393,
					1029,
					291,
					281,
					747,
					264,
					40661,
					293,
					406,
					385,
					13,
					51283
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 149,
				"seek": 8552,
				"start": 482.04428,
				"end": 482.6443,
				"text": " Fair enough.",
				"tokens": [
					51381,
					12157,
					1547,
					13,
					51411
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 150,
				"seek": 8552,
				"start": 484.86426,
				"end": 486.50427,
				"text": " I sent it to you in Slack.",
				"tokens": [
					51522,
					286,
					2279,
					309,
					281,
					291,
					294,
					37211,
					13,
					51604
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 151,
				"seek": 8552,
				"start": 487.0843,
				"end": 487.36426,
				"text": " Perfect.",
				"tokens": [
					51633,
					10246,
					13,
					51647
				],
				"temperature": 0,
				"avg_logprob": -0.210392,
				"compression_ratio": 1.5857741,
				"no_speech_prob": 1.7623885e-12
			},
			{
				"id": 152,
				"seek": 11552,
				"start": 491.72427,
				"end": 493.32428,
				"text": " And I'm going to go back to screen sharing.",
				"tokens": [
					50365,
					400,
					286,
					478,
					516,
					281,
					352,
					646,
					281,
					2568,
					5414,
					13,
					50445
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 153,
				"seek": 11552,
				"start": 493.78427,
				"end": 496.48428,
				"text": " So let's talk about what it means to design around the KV cache.",
				"tokens": [
					50468,
					407,
					718,
					311,
					751,
					466,
					437,
					309,
					1355,
					281,
					1715,
					926,
					264,
					591,
					53,
					19459,
					13,
					50603
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 154,
				"seek": 11552,
				"start": 497.04428,
				"end": 501.86426,
				"text": " So firstly, what does a KV cache mean?",
				"tokens": [
					50631,
					407,
					27376,
					11,
					437,
					775,
					257,
					591,
					53,
					19459,
					914,
					30,
					50872
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 155,
				"seek": 11552,
				"start": 502.00427,
				"end": 503.20428,
				"text": " What is actually happening?",
				"tokens": [
					50879,
					708,
					307,
					767,
					2737,
					30,
					50939
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 156,
				"seek": 11552,
				"start": 503.36426,
				"end": 506.42426,
				"text": " So let's just start off with the basics of what an LLM is.",
				"tokens": [
					50947,
					407,
					718,
					311,
					445,
					722,
					766,
					365,
					264,
					14688,
					295,
					437,
					364,
					441,
					43,
					44,
					307,
					13,
					51100
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 157,
				"seek": 11552,
				"start": 506.88428,
				"end": 509.38428,
				"text": " An LLM is a thing that takes in a bunch of tokens,",
				"tokens": [
					51123,
					1107,
					441,
					43,
					44,
					307,
					257,
					551,
					300,
					2516,
					294,
					257,
					3840,
					295,
					22667,
					11,
					51248
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 158,
				"seek": 11552,
				"start": 509.90427,
				"end": 511.62427,
				"text": " spits out a bunch of token probabilities,",
				"tokens": [
					51274,
					637,
					1208,
					484,
					257,
					3840,
					295,
					14862,
					33783,
					11,
					51360
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 159,
				"seek": 11552,
				"start": 512.18427,
				"end": 514.60425,
				"text": " and then you somehow will apply some algorithm,",
				"tokens": [
					51388,
					293,
					550,
					291,
					6063,
					486,
					3079,
					512,
					9284,
					11,
					51509
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 160,
				"seek": 11552,
				"start": 514.8843,
				"end": 516.7043,
				"text": " typically like a softmax or something,",
				"tokens": [
					51523,
					5850,
					411,
					257,
					2787,
					41167,
					420,
					746,
					11,
					51614
				],
				"temperature": 0,
				"avg_logprob": -0.19452725,
				"compression_ratio": 1.6299213,
				"no_speech_prob": 1.0524094e-12
			},
			{
				"id": 161,
				"seek": 14050,
				"start": 516.7043,
				"end": 521.7043,
				"text": " to then pick out what is the token",
				"tokens": [
					50365,
					281,
					550,
					1888,
					484,
					437,
					307,
					264,
					14862,
					50615
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 162,
				"seek": 14050,
				"start": 521.7043,
				"end": 523.0643,
				"text": " that I should actually be selecting.",
				"tokens": [
					50615,
					300,
					286,
					820,
					767,
					312,
					18182,
					13,
					50683
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 163,
				"seek": 14050,
				"start": 526.9043,
				"end": 529.4643,
				"text": " And these are just probabilities of final output tokens.",
				"tokens": [
					50875,
					400,
					613,
					366,
					445,
					33783,
					295,
					2572,
					5598,
					22667,
					13,
					51003
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 164,
				"seek": 14050,
				"start": 530.3243,
				"end": 531.7843,
				"text": " And once you select your next token,",
				"tokens": [
					51046,
					400,
					1564,
					291,
					3048,
					428,
					958,
					14862,
					11,
					51119
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 165,
				"seek": 14050,
				"start": 532.34424,
				"end": 534.24426,
				"text": " then the LLM will take this thing back.",
				"tokens": [
					51147,
					550,
					264,
					441,
					43,
					44,
					486,
					747,
					341,
					551,
					646,
					13,
					51242
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 166,
				"seek": 14050,
				"start": 534.7643,
				"end": 535.98425,
				"text": " I don't know how to draw this arrow,",
				"tokens": [
					51268,
					286,
					500,
					380,
					458,
					577,
					281,
					2642,
					341,
					11610,
					11,
					51329
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 167,
				"seek": 14050,
				"start": 536.1443,
				"end": 537.98425,
				"text": " but I suspect Dextra will fix it for me.",
				"tokens": [
					51337,
					457,
					286,
					9091,
					1346,
					734,
					424,
					486,
					3191,
					309,
					337,
					385,
					13,
					51429
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 168,
				"seek": 14050,
				"start": 539.0043,
				"end": 541.6643,
				"text": " I'll append this selected token",
				"tokens": [
					51480,
					286,
					603,
					34116,
					341,
					8209,
					14862,
					51613
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 169,
				"seek": 14050,
				"start": 541.6643,
				"end": 546.3243,
				"text": " to the very next item on the array,",
				"tokens": [
					51613,
					281,
					264,
					588,
					958,
					3174,
					322,
					264,
					10225,
					11,
					51846
				],
				"temperature": 0,
				"avg_logprob": -0.2260055,
				"compression_ratio": 1.581081,
				"no_speech_prob": 7.699671e-13
			},
			{
				"id": 170,
				"seek": 17012,
				"start": 546.3243,
				"end": 547.5643,
				"text": " and then I'll just append it",
				"tokens": [
					50365,
					293,
					550,
					286,
					603,
					445,
					34116,
					309,
					50427
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 171,
				"seek": 17012,
				"start": 547.5643,
				"end": 549.3843,
				"text": " and it'll go back in the loop over and over again.",
				"tokens": [
					50427,
					293,
					309,
					603,
					352,
					646,
					294,
					264,
					6367,
					670,
					293,
					670,
					797,
					13,
					50518
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 172,
				"seek": 17012,
				"start": 549.84424,
				"end": 551.9043,
				"text": " Most of us probably know this is how alums work.",
				"tokens": [
					50541,
					4534,
					295,
					505,
					1391,
					458,
					341,
					307,
					577,
					419,
					8099,
					589,
					13,
					50644
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 173,
				"seek": 17012,
				"start": 552.60425,
				"end": 555.0643,
				"text": " The thing that I think a lot of people don't realize",
				"tokens": [
					50679,
					440,
					551,
					300,
					286,
					519,
					257,
					688,
					295,
					561,
					500,
					380,
					4325,
					50802
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 174,
				"seek": 17012,
				"start": 555.0643,
				"end": 559.10425,
				"text": " is this, if you've done software,",
				"tokens": [
					50802,
					307,
					341,
					11,
					498,
					291,
					600,
					1096,
					4722,
					11,
					51004
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 175,
				"seek": 17012,
				"start": 559.34424,
				"end": 561.4043,
				"text": " probably looks like a dynamic programming problem.",
				"tokens": [
					51016,
					1391,
					1542,
					411,
					257,
					8546,
					9410,
					1154,
					13,
					51119
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 176,
				"seek": 17012,
				"start": 561.98425,
				"end": 563.42426,
				"text": " This looks like a thing where you have",
				"tokens": [
					51148,
					639,
					1542,
					411,
					257,
					551,
					689,
					291,
					362,
					51220
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 177,
				"seek": 17012,
				"start": 563.42426,
				"end": 566.4643,
				"text": " almost the same array going in over and over again,",
				"tokens": [
					51220,
					1920,
					264,
					912,
					10225,
					516,
					294,
					670,
					293,
					670,
					797,
					11,
					51372
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 178,
				"seek": 17012,
				"start": 566.5043,
				"end": 568.4643,
				"text": " but then you just have one new element every single time.",
				"tokens": [
					51374,
					457,
					550,
					291,
					445,
					362,
					472,
					777,
					4478,
					633,
					2167,
					565,
					13,
					51472
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 179,
				"seek": 17012,
				"start": 569.30426,
				"end": 572.10425,
				"text": " It sounds like we should be able to repeat some of the math",
				"tokens": [
					51514,
					467,
					3263,
					411,
					321,
					820,
					312,
					1075,
					281,
					7149,
					512,
					295,
					264,
					5221,
					51654
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 180,
				"seek": 17012,
				"start": 572.10425,
				"end": 574.7843,
				"text": " and not have to redo everything all the time.",
				"tokens": [
					51654,
					293,
					406,
					362,
					281,
					29956,
					1203,
					439,
					264,
					565,
					13,
					51788
				],
				"temperature": 0,
				"avg_logprob": -0.15592377,
				"compression_ratio": 1.7842466,
				"no_speech_prob": 1.146812e-12
			},
			{
				"id": 181,
				"seek": 19858,
				"start": 574.7843,
				"end": 587.84424,
				"text": " If you actually go look into here, what you'll find is the LLM has some architecture stuff in here that allows it to not have to repeat all the math all the time, assuming that you have some stability guaranteed.",
				"tokens": [
					50365,
					759,
					291,
					767,
					352,
					574,
					666,
					510,
					11,
					437,
					291,
					603,
					915,
					307,
					264,
					441,
					43,
					44,
					575,
					512,
					9482,
					1507,
					294,
					510,
					300,
					4045,
					309,
					281,
					406,
					362,
					281,
					7149,
					439,
					264,
					5221,
					439,
					264,
					565,
					11,
					11926,
					300,
					291,
					362,
					512,
					11826,
					18031,
					13,
					51018
				],
				"temperature": 0,
				"avg_logprob": -0.12991759,
				"compression_ratio": 1.6694214,
				"no_speech_prob": 7.3756333e-13
			},
			{
				"id": 182,
				"seek": 19858,
				"start": 588.1643,
				"end": 593.4443,
				"text": " So you can pre-compute some of this stuff as a part of the encoder decoder layer that the LLM has inside of itself.",
				"tokens": [
					51034,
					407,
					291,
					393,
					659,
					12,
					21541,
					1169,
					512,
					295,
					341,
					1507,
					382,
					257,
					644,
					295,
					264,
					2058,
					19866,
					979,
					19866,
					4583,
					300,
					264,
					441,
					43,
					44,
					575,
					1854,
					295,
					2564,
					13,
					51298
				],
				"temperature": 0,
				"avg_logprob": -0.12991759,
				"compression_ratio": 1.6694214,
				"no_speech_prob": 7.3756333e-13
			},
			{
				"id": 183,
				"seek": 19858,
				"start": 594.30426,
				"end": 601.36426,
				"text": " The problem is that pre-computed section is purely dependent on continuity.",
				"tokens": [
					51341,
					440,
					1154,
					307,
					300,
					659,
					12,
					1112,
					2582,
					292,
					3541,
					307,
					17491,
					12334,
					322,
					23807,
					13,
					51694
				],
				"temperature": 0,
				"avg_logprob": -0.12991759,
				"compression_ratio": 1.6694214,
				"no_speech_prob": 7.3756333e-13
			},
			{
				"id": 184,
				"seek": 22516,
				"start": 601.36426,
				"end": 604.5843,
				"text": " so it's very hard for you to pre-compute",
				"tokens": [
					50365,
					370,
					309,
					311,
					588,
					1152,
					337,
					291,
					281,
					659,
					12,
					21541,
					1169,
					50526
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 185,
				"seek": 22516,
				"start": 604.5843,
				"end": 608.2643,
				"text": " like random segments of it",
				"tokens": [
					50526,
					411,
					4974,
					19904,
					295,
					309,
					50710
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 186,
				"seek": 22516,
				"start": 608.2643,
				"end": 611.86426,
				"text": " what you can do is much easier to pre-compute chunks of it along the way",
				"tokens": [
					50710,
					437,
					291,
					393,
					360,
					307,
					709,
					3571,
					281,
					659,
					12,
					21541,
					1169,
					24004,
					295,
					309,
					2051,
					264,
					636,
					50890
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 187,
				"seek": 22516,
				"start": 611.86426,
				"end": 613.12427,
				"text": " and I should be using different colors",
				"tokens": [
					50890,
					293,
					286,
					820,
					312,
					1228,
					819,
					4577,
					50953
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 188,
				"seek": 22516,
				"start": 613.12427,
				"end": 616.74426,
				"text": " so you can pre-compute this part",
				"tokens": [
					50953,
					370,
					291,
					393,
					659,
					12,
					21541,
					1169,
					341,
					644,
					51134
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 189,
				"seek": 22516,
				"start": 616.74426,
				"end": 618.1643,
				"text": " you can pre-compute this part",
				"tokens": [
					51134,
					291,
					393,
					659,
					12,
					21541,
					1169,
					341,
					644,
					51205
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 190,
				"seek": 22516,
				"start": 618.1643,
				"end": 620.3843,
				"text": " and then you can pre-compute this part",
				"tokens": [
					51205,
					293,
					550,
					291,
					393,
					659,
					12,
					21541,
					1169,
					341,
					644,
					51316
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 191,
				"seek": 22516,
				"start": 620.3843,
				"end": 625.8243,
				"text": " I'm sorry I'm not quite clear what like the vertical access means",
				"tokens": [
					51316,
					286,
					478,
					2597,
					286,
					478,
					406,
					1596,
					1850,
					437,
					411,
					264,
					9429,
					2105,
					1355,
					51588
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 192,
				"seek": 22516,
				"start": 625.8243,
				"end": 628.7043,
				"text": " sorry what I mean by here is like the tokens going in",
				"tokens": [
					51588,
					2597,
					437,
					286,
					914,
					538,
					510,
					307,
					411,
					264,
					22667,
					516,
					294,
					51732
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 193,
				"seek": 22516,
				"start": 628.7043,
				"end": 631.1443,
				"text": " these are all the tokens that are in from like index zero",
				"tokens": [
					51732,
					613,
					366,
					439,
					264,
					22667,
					300,
					366,
					294,
					490,
					411,
					8186,
					4018,
					51854
				],
				"temperature": 0,
				"avg_logprob": -0.11971753,
				"compression_ratio": 1.9784483,
				"no_speech_prob": 1.2160073e-12
			},
			{
				"id": 194,
				"seek": 25494,
				"start": 631.1443,
				"end": 635.0843,
				"text": " So I should, what I mean by here is like, this is index zero, this is index one.",
				"tokens": [
					50365,
					407,
					286,
					820,
					11,
					437,
					286,
					914,
					538,
					510,
					307,
					411,
					11,
					341,
					307,
					8186,
					4018,
					11,
					341,
					307,
					8186,
					472,
					13,
					50562
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 195,
				"seek": 25494,
				"start": 636.3243,
				"end": 637.74426,
				"text": " And these are just tokens.",
				"tokens": [
					50624,
					400,
					613,
					366,
					445,
					22667,
					13,
					50695
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 196,
				"seek": 25494,
				"start": 638.5443,
				"end": 641.7843,
				"text": " Okay, and zero, index zero, index one might be like.",
				"tokens": [
					50735,
					1033,
					11,
					293,
					4018,
					11,
					8186,
					4018,
					11,
					8186,
					472,
					1062,
					312,
					411,
					13,
					50897
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 197,
				"seek": 25494,
				"start": 642.5043,
				"end": 644.74426,
				"text": " Like the word, exactly.",
				"tokens": [
					50933,
					1743,
					264,
					1349,
					11,
					2293,
					13,
					51045
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 198,
				"seek": 25494,
				"start": 644.84424,
				"end": 645.4443,
				"text": " It would be like.",
				"tokens": [
					51050,
					467,
					576,
					312,
					411,
					13,
					51080
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 199,
				"seek": 25494,
				"start": 645.5043,
				"end": 648.96423,
				"text": " Think of like a string that I drew vertically because of how the model is laid out.",
				"tokens": [
					51083,
					6557,
					295,
					411,
					257,
					6798,
					300,
					286,
					12804,
					28450,
					570,
					295,
					577,
					264,
					2316,
					307,
					9897,
					484,
					13,
					51256
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 200,
				"seek": 25494,
				"start": 649.0043,
				"end": 650.2043,
				"text": " And that's easier for me to think about.",
				"tokens": [
					51258,
					400,
					300,
					311,
					3571,
					337,
					385,
					281,
					519,
					466,
					13,
					51318
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 201,
				"seek": 25494,
				"start": 650.68427,
				"end": 651.0443,
				"text": " Yeah.",
				"tokens": [
					51342,
					865,
					13,
					51360
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 202,
				"seek": 25494,
				"start": 655.22424,
				"end": 656.5643,
				"text": " And then you have like index like.",
				"tokens": [
					51569,
					400,
					550,
					291,
					362,
					411,
					8186,
					411,
					13,
					51636
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 203,
				"seek": 25494,
				"start": 656.62427,
				"end": 657.0643,
				"text": " I see.",
				"tokens": [
					51639,
					286,
					536,
					13,
					51661
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 204,
				"seek": 25494,
				"start": 657.0643,
				"end": 659.6643,
				"text": " Okay, so you're caching the first three words in the sentence.",
				"tokens": [
					51661,
					1033,
					11,
					370,
					291,
					434,
					269,
					2834,
					264,
					700,
					1045,
					2283,
					294,
					264,
					8174,
					13,
					51791
				],
				"temperature": 0,
				"avg_logprob": -0.19969693,
				"compression_ratio": 1.7176471,
				"no_speech_prob": 1.7280985e-12
			},
			{
				"id": 205,
				"seek": 28346,
				"start": 659.6643,
				"end": 664.0643,
				"text": " and then once you come back with this next like once you come back with the the fourth word in",
				"tokens": [
					50365,
					293,
					550,
					1564,
					291,
					808,
					646,
					365,
					341,
					958,
					411,
					1564,
					291,
					808,
					646,
					365,
					264,
					264,
					6409,
					1349,
					294,
					50585
				],
				"temperature": 0,
				"avg_logprob": -0.08283522,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.0731203e-12
			},
			{
				"id": 206,
				"seek": 28346,
				"start": 664.0643,
				"end": 677.74426,
				"text": " the sentence here that is like four uh oh and then you ask okay what's the next token all of all of",
				"tokens": [
					50585,
					264,
					8174,
					510,
					300,
					307,
					411,
					1451,
					2232,
					1954,
					293,
					550,
					291,
					1029,
					1392,
					437,
					311,
					264,
					958,
					14862,
					439,
					295,
					439,
					295,
					51269
				],
				"temperature": 0,
				"avg_logprob": -0.08283522,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.0731203e-12
			},
			{
				"id": 207,
				"seek": 28346,
				"start": 677.74426,
				"end": 682.0643,
				"text": " this part of it basically is is cached all the math for this part of it is cached and so you're",
				"tokens": [
					51269,
					341,
					644,
					295,
					309,
					1936,
					307,
					307,
					269,
					15095,
					439,
					264,
					5221,
					337,
					341,
					644,
					295,
					309,
					307,
					269,
					15095,
					293,
					370,
					291,
					434,
					51485
				],
				"temperature": 0,
				"avg_logprob": -0.08283522,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.0731203e-12
			},
			{
				"id": 208,
				"seek": 28346,
				"start": 682.0643,
				"end": 688.46423,
				"text": " just kind of like taking that pre-fill um whatever they're like matrix of the meaning of what's being",
				"tokens": [
					51485,
					445,
					733,
					295,
					411,
					1940,
					300,
					659,
					12,
					31072,
					1105,
					2035,
					436,
					434,
					411,
					8141,
					295,
					264,
					3620,
					295,
					437,
					311,
					885,
					51805
				],
				"temperature": 0,
				"avg_logprob": -0.08283522,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.0731203e-12
			},
			{
				"id": 209,
				"seek": 31226,
				"start": 688.46423,
				"end": 693.5842,
				"text": " accumulated in the transformer and so you're just kind of it's all cost and timing right it's like",
				"tokens": [
					50365,
					31346,
					294,
					264,
					31782,
					293,
					370,
					291,
					434,
					445,
					733,
					295,
					309,
					311,
					439,
					2063,
					293,
					10822,
					558,
					309,
					311,
					411,
					50621
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 210,
				"seek": 31226,
				"start": 693.5842,
				"end": 698.0643,
				"text": " it's much faster to just say okay cool now here's the next token now give me the next inference bit",
				"tokens": [
					50621,
					309,
					311,
					709,
					4663,
					281,
					445,
					584,
					1392,
					1627,
					586,
					510,
					311,
					264,
					958,
					14862,
					586,
					976,
					385,
					264,
					958,
					38253,
					857,
					50845
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 211,
				"seek": 31226,
				"start": 698.0643,
				"end": 703.6643,
				"text": " right exactly and just to be very clear like i know some of you are like ah but technically doesn't",
				"tokens": [
					50845,
					558,
					2293,
					293,
					445,
					281,
					312,
					588,
					1850,
					411,
					741,
					458,
					512,
					295,
					291,
					366,
					411,
					3716,
					457,
					12120,
					1177,
					380,
					51125
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 212,
				"seek": 31226,
				"start": 703.6643,
				"end": 707.2643,
				"text": " the walking word impact what all the other words mean so like how can you actually cache this",
				"tokens": [
					51125,
					264,
					4494,
					1349,
					2712,
					437,
					439,
					264,
					661,
					2283,
					914,
					370,
					411,
					577,
					393,
					291,
					767,
					19459,
					341,
					51305
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 213,
				"seek": 31226,
				"start": 707.2643,
				"end": 714.2043,
				"text": " very question i would not have asked that question that's a great question well it turns out the",
				"tokens": [
					51305,
					588,
					1168,
					741,
					576,
					406,
					362,
					2351,
					300,
					1168,
					300,
					311,
					257,
					869,
					1168,
					731,
					309,
					4523,
					484,
					264,
					51652
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 214,
				"seek": 31226,
				"start": 714.2043,
				"end": 717.4043,
				"text": " transformer has like two different parts of encoders and decoders that are all laid out",
				"tokens": [
					51652,
					31782,
					575,
					411,
					732,
					819,
					3166,
					295,
					2058,
					378,
					433,
					293,
					979,
					378,
					433,
					300,
					366,
					439,
					9897,
					484,
					51812
				],
				"temperature": 0,
				"avg_logprob": -0.083494395,
				"compression_ratio": 1.8553054,
				"no_speech_prob": 2.6456276e-12
			},
			{
				"id": 215,
				"seek": 34120,
				"start": 717.4043,
				"end": 721.74426,
				"text": " So as it turns out, you don't actually run all the computation all at once.",
				"tokens": [
					50365,
					407,
					382,
					309,
					4523,
					484,
					11,
					291,
					500,
					380,
					767,
					1190,
					439,
					264,
					24903,
					439,
					412,
					1564,
					13,
					50582
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 216,
				"seek": 34120,
				"start": 721.74426,
				"end": 725.3843,
				"text": " You actually break down the input strings, the smaller subsequences of strings that then",
				"tokens": [
					50582,
					509,
					767,
					1821,
					760,
					264,
					4846,
					13985,
					11,
					264,
					4356,
					13924,
					2667,
					295,
					13985,
					300,
					550,
					50764
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 217,
				"seek": 34120,
				"start": 725.3843,
				"end": 727.4443,
				"text": " can each be individually computed.",
				"tokens": [
					50764,
					393,
					1184,
					312,
					16652,
					40610,
					13,
					50867
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 218,
				"seek": 34120,
				"start": 727.6643,
				"end": 729.98425,
				"text": " And then you can go on stacking that along the way.",
				"tokens": [
					50878,
					400,
					550,
					291,
					393,
					352,
					322,
					41376,
					300,
					2051,
					264,
					636,
					13,
					50994
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 219,
				"seek": 34120,
				"start": 730.6643,
				"end": 740.8243,
				"text": " So if you go on to like Anthropic's caching docs, so we just go here, for example, you'll",
				"tokens": [
					51028,
					407,
					498,
					291,
					352,
					322,
					281,
					411,
					12727,
					39173,
					311,
					269,
					2834,
					45623,
					11,
					370,
					321,
					445,
					352,
					510,
					11,
					337,
					1365,
					11,
					291,
					603,
					51536
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 220,
				"seek": 34120,
				"start": 740.8243,
				"end": 743.24426,
				"text": " find that Anthropic has a minimum cacheable prompt.",
				"tokens": [
					51536,
					915,
					300,
					12727,
					39173,
					575,
					257,
					7285,
					19459,
					712,
					12391,
					13,
					51657
				],
				"temperature": 0,
				"avg_logprob": -0.16546547,
				"compression_ratio": 1.6582278,
				"no_speech_prob": 1.0279971e-12
			},
			{
				"id": 221,
				"seek": 36704,
				"start": 743.24426,
				"end": 745.4443,
				"text": " they do that because",
				"tokens": [
					50365,
					436,
					360,
					300,
					570,
					50475
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 222,
				"seek": 36704,
				"start": 745.4443,
				"end": 748.18427,
				"text": " less than 1024 tokens",
				"tokens": [
					50475,
					1570,
					813,
					1266,
					7911,
					22667,
					50612
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 223,
				"seek": 36704,
				"start": 748.18427,
				"end": 750.2643,
				"text": " probably doesn't fit in whatever cache alignment",
				"tokens": [
					50612,
					1391,
					1177,
					380,
					3318,
					294,
					2035,
					19459,
					18515,
					50716
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 224,
				"seek": 36704,
				"start": 750.2643,
				"end": 752.12427,
				"text": " block that they have for actually doing the computation",
				"tokens": [
					50716,
					3461,
					300,
					436,
					362,
					337,
					767,
					884,
					264,
					24903,
					50809
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 225,
				"seek": 36704,
				"start": 752.12427,
				"end": 754.24426,
				"text": " behind some architecture decisions that they have made",
				"tokens": [
					50809,
					2261,
					512,
					9482,
					5327,
					300,
					436,
					362,
					1027,
					50915
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 226,
				"seek": 36704,
				"start": 754.24426,
				"end": 756.2843,
				"text": " and it wouldn't make sense for them to cache",
				"tokens": [
					50915,
					293,
					309,
					2759,
					380,
					652,
					2020,
					337,
					552,
					281,
					19459,
					51017
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 227,
				"seek": 36704,
				"start": 756.2843,
				"end": 758.0043,
				"text": " that because it's basically throwaway work because",
				"tokens": [
					51017,
					300,
					570,
					309,
					311,
					1936,
					3507,
					10318,
					589,
					570,
					51103
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 228,
				"seek": 36704,
				"start": 758.0043,
				"end": 760.48425,
				"text": " every token outside of 124",
				"tokens": [
					51103,
					633,
					14862,
					2380,
					295,
					2272,
					19,
					51227
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 229,
				"seek": 36704,
				"start": 760.48425,
				"end": 761.68427,
				"text": " is going to cross compete",
				"tokens": [
					51227,
					307,
					516,
					281,
					3278,
					11831,
					51287
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 230,
				"seek": 36704,
				"start": 761.68427,
				"end": 764.30426,
				"text": " need to do some",
				"tokens": [
					51287,
					643,
					281,
					360,
					512,
					51418
				],
				"temperature": 0,
				"avg_logprob": -0.11711839,
				"compression_ratio": 1.8450705,
				"no_speech_prob": 1.1694355e-12
			},
			{
				"id": 231,
				"seek": 765,
				"start": 764.30426,
				"end": 774.4285,
				"text": " cross attention stuff and actually layer the information between the words but the 124 boundaries they probably have some layer of their network that can be done in parallel and have deterministic results",
				"tokens": [
					51418,
					3278,
					3202,
					1507,
					293,
					767,
					4583,
					264,
					51513,
					51513,
					1589,
					1296,
					264,
					2283,
					51584,
					51584,
					457,
					264,
					2272,
					19,
					13180,
					436,
					1391,
					362,
					50600,
					362,
					512,
					4583,
					295,
					641,
					3209,
					300,
					393,
					312,
					1096,
					294,
					8952,
					293,
					362,
					15957,
					3142,
					3542,
					13,
					50836
				],
				"temperature": 0,
				"avg_logprob": -0.09370671,
				"compression_ratio": 1.659864,
				"no_speech_prob": 1.7283169e-12
			},
			{
				"id": 232,
				"seek": 765,
				"start": 775.08856,
				"end": 778.76855,
				"text": " So that is why it's not just important to go read the docs, but just to understand that there's some",
				"tokens": [
					50869,
					407,
					300,
					307,
					983,
					309,
					311,
					406,
					445,
					1021,
					281,
					352,
					1401,
					264,
					45623,
					11,
					457,
					445,
					281,
					1223,
					300,
					456,
					311,
					512,
					51053
				],
				"temperature": 0,
				"avg_logprob": -0.09370671,
				"compression_ratio": 1.659864,
				"no_speech_prob": 1.7283169e-12
			},
			{
				"id": 233,
				"seek": 765,
				"start": 778.76855,
				"end": 783.9285,
				"text": " architecture decision that everyone implementing these models is going to make that dramatically",
				"tokens": [
					51053,
					9482,
					3537,
					300,
					1518,
					18114,
					613,
					5245,
					307,
					516,
					281,
					652,
					300,
					17548,
					51311
				],
				"temperature": 0,
				"avg_logprob": -0.09370671,
				"compression_ratio": 1.659864,
				"no_speech_prob": 1.7283169e-12
			},
			{
				"id": 234,
				"seek": 765,
				"start": 783.9285,
				"end": 789.9885,
				"text": " changes how fast this works. And some of you might be wondering, why is it so much smaller in Opus",
				"tokens": [
					51311,
					2962,
					577,
					2370,
					341,
					1985,
					13,
					400,
					512,
					295,
					291,
					1062,
					312,
					6359,
					11,
					983,
					307,
					309,
					370,
					709,
					4356,
					294,
					12011,
					301,
					51614
				],
				"temperature": 0,
				"avg_logprob": -0.09370671,
				"compression_ratio": 1.659864,
				"no_speech_prob": 1.7283169e-12
			},
			{
				"id": 235,
				"seek": 3263,
				"start": 789.9885,
				"end": 796.9885,
				"text": " versus haiku. One intuition I could have, I don't know, can't guarantee it, is that it's possible",
				"tokens": [
					50365,
					5717,
					324,
					24320,
					13,
					1485,
					24002,
					286,
					727,
					362,
					11,
					286,
					500,
					380,
					458,
					11,
					393,
					380,
					10815,
					309,
					11,
					307,
					300,
					309,
					311,
					1944,
					50715
				],
				"temperature": 0,
				"avg_logprob": -0.044654477,
				"compression_ratio": 1.6423612,
				"no_speech_prob": 1.0239944e-12
			},
			{
				"id": 236,
				"seek": 3263,
				"start": 796.9885,
				"end": 801.68854,
				"text": " that these haiku models use a different token vocabulary that is much smaller than the original",
				"tokens": [
					50715,
					300,
					613,
					324,
					24320,
					5245,
					764,
					257,
					819,
					14862,
					19864,
					300,
					307,
					709,
					4356,
					813,
					264,
					3380,
					50950
				],
				"temperature": 0,
				"avg_logprob": -0.044654477,
				"compression_ratio": 1.6423612,
				"no_speech_prob": 1.0239944e-12
			},
			{
				"id": 237,
				"seek": 3263,
				"start": 801.68854,
				"end": 807.4285,
				"text": " models. Therefore, they're able to compress it better. Or these might be using float 16 instead",
				"tokens": [
					50950,
					5245,
					13,
					7504,
					11,
					436,
					434,
					1075,
					281,
					14778,
					309,
					1101,
					13,
					1610,
					613,
					1062,
					312,
					1228,
					15706,
					3165,
					2602,
					51237
				],
				"temperature": 0,
				"avg_logprob": -0.044654477,
				"compression_ratio": 1.6423612,
				"no_speech_prob": 1.0239944e-12
			},
			{
				"id": 238,
				"seek": 3263,
				"start": 807.4285,
				"end": 812.7886,
				"text": " of float 32 or float 64, so they can double compress it along the way in terms of how much",
				"tokens": [
					51237,
					295,
					15706,
					8858,
					420,
					15706,
					12145,
					11,
					370,
					436,
					393,
					3834,
					14778,
					309,
					2051,
					264,
					636,
					294,
					2115,
					295,
					577,
					709,
					51505
				],
				"temperature": 0,
				"avg_logprob": -0.044654477,
				"compression_ratio": 1.6423612,
				"no_speech_prob": 1.0239944e-12
			},
			{
				"id": 239,
				"seek": 3263,
				"start": 812.7886,
				"end": 818.02856,
				"text": " data they can stuff into the same amount of computational unit. But I think the point stands",
				"tokens": [
					51505,
					1412,
					436,
					393,
					1507,
					666,
					264,
					912,
					2372,
					295,
					28270,
					4985,
					13,
					583,
					286,
					519,
					264,
					935,
					7382,
					51767
				],
				"temperature": 0,
				"avg_logprob": -0.044654477,
				"compression_ratio": 1.6423612,
				"no_speech_prob": 1.0239944e-12
			},
			{
				"id": 240,
				"seek": 6067,
				"start": 818.02856,
				"end": 821.4885,
				"text": " is that there's some computation in every LLM",
				"tokens": [
					50365,
					307,
					300,
					456,
					311,
					512,
					24903,
					294,
					633,
					441,
					43,
					44,
					50538
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 241,
				"seek": 6067,
				"start": 821.4885,
				"end": 823.70856,
				"text": " that can be cached to some degree of it.",
				"tokens": [
					50538,
					300,
					393,
					312,
					269,
					15095,
					281,
					512,
					4314,
					295,
					309,
					13,
					50649
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 242,
				"seek": 6067,
				"start": 823.8486,
				"end": 826.20856,
				"text": " And that the amount of degree that can be done",
				"tokens": [
					50656,
					400,
					300,
					264,
					2372,
					295,
					4314,
					300,
					393,
					312,
					1096,
					50774
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 243,
				"seek": 6067,
				"start": 826.20856,
				"end": 828.00854,
				"text": " is based on how much similarity you have",
				"tokens": [
					50774,
					307,
					2361,
					322,
					577,
					709,
					32194,
					291,
					362,
					50864
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 244,
				"seek": 6067,
				"start": 828.00854,
				"end": 830.64856,
				"text": " and how big of an input space you have that is changing.",
				"tokens": [
					50864,
					293,
					577,
					955,
					295,
					364,
					4846,
					1901,
					291,
					362,
					300,
					307,
					4473,
					13,
					50996
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 245,
				"seek": 6067,
				"start": 831.6686,
				"end": 833.9885,
				"text": " But what that is to say",
				"tokens": [
					51047,
					583,
					437,
					300,
					307,
					281,
					584,
					51163
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 246,
				"seek": 6067,
				"start": 833.9885,
				"end": 836.12854,
				"text": " is that if you are constantly running a chat thread",
				"tokens": [
					51163,
					307,
					300,
					498,
					291,
					366,
					6460,
					2614,
					257,
					5081,
					7207,
					51270
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 247,
				"seek": 6067,
				"start": 836.12854,
				"end": 838.1085,
				"text": " and you're dynamically changing the system message",
				"tokens": [
					51270,
					293,
					291,
					434,
					43492,
					4473,
					264,
					1185,
					3636,
					51369
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 248,
				"seek": 6067,
				"start": 838.1085,
				"end": 839.88855,
				"text": " all the time, somewhere in here,",
				"tokens": [
					51369,
					439,
					264,
					565,
					11,
					4079,
					294,
					510,
					11,
					51458
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 249,
				"seek": 6067,
				"start": 840.24854,
				"end": 843.20856,
				"text": " you're basically breaking the entire KB cache",
				"tokens": [
					51476,
					291,
					434,
					1936,
					7697,
					264,
					2302,
					591,
					33,
					19459,
					51624
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 250,
				"seek": 6067,
				"start": 843.20856,
				"end": 844.46857,
				"text": " every single time.",
				"tokens": [
					51624,
					633,
					2167,
					565,
					13,
					51687
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 251,
				"seek": 6067,
				"start": 844.80853,
				"end": 846.90857,
				"text": " That means you're having to recompute all the work,",
				"tokens": [
					51704,
					663,
					1355,
					291,
					434,
					1419,
					281,
					48000,
					1169,
					439,
					264,
					589,
					11,
					51809
				],
				"temperature": 0,
				"avg_logprob": -0.11743389,
				"compression_ratio": 1.8273381,
				"no_speech_prob": 1.1072327e-12
			},
			{
				"id": 252,
				"seek": 8955,
				"start": 846.90857,
				"end": 852.1686,
				"text": " always and you can't take any benefit of the caching system that means just just to make this",
				"tokens": [
					50365,
					1009,
					293,
					291,
					393,
					380,
					747,
					604,
					5121,
					295,
					264,
					269,
					2834,
					1185,
					300,
					1355,
					445,
					445,
					281,
					652,
					341,
					50628
				],
				"temperature": 0,
				"avg_logprob": -0.05232333,
				"compression_ratio": 1.8846154,
				"no_speech_prob": 1.7691079e-12
			},
			{
				"id": 253,
				"seek": 8955,
				"start": 852.1686,
				"end": 857.44855,
				"text": " a little more concrete um can i like just draw a little bit more so like let's say you have your",
				"tokens": [
					50628,
					257,
					707,
					544,
					9859,
					1105,
					393,
					741,
					411,
					445,
					2642,
					257,
					707,
					857,
					544,
					370,
					411,
					718,
					311,
					584,
					291,
					362,
					428,
					50892
				],
				"temperature": 0,
				"avg_logprob": -0.05232333,
				"compression_ratio": 1.8846154,
				"no_speech_prob": 1.7691079e-12
			},
			{
				"id": 254,
				"seek": 8955,
				"start": 857.44855,
				"end": 864.7285,
				"text": " your system prompt here yeah uh system prompt and then you have your tools are technically like end",
				"tokens": [
					50892,
					428,
					1185,
					12391,
					510,
					1338,
					2232,
					1185,
					12391,
					293,
					550,
					291,
					362,
					428,
					3873,
					366,
					12120,
					411,
					917,
					51256
				],
				"temperature": 0,
				"avg_logprob": -0.05232333,
				"compression_ratio": 1.8846154,
				"no_speech_prob": 1.7691079e-12
			},
			{
				"id": 255,
				"seek": 8955,
				"start": 864.7285,
				"end": 868.8685,
				"text": " up part of the system prompt but i'll just put them separately the idea would be like if at some",
				"tokens": [
					51256,
					493,
					644,
					295,
					264,
					1185,
					12391,
					457,
					741,
					603,
					445,
					829,
					552,
					14759,
					264,
					1558,
					576,
					312,
					411,
					498,
					412,
					512,
					51463
				],
				"temperature": 0,
				"avg_logprob": -0.05232333,
				"compression_ratio": 1.8846154,
				"no_speech_prob": 1.7691079e-12
			},
			{
				"id": 256,
				"seek": 8955,
				"start": 868.8685,
				"end": 874.12854,
				"text": " point you wanted to tell the agent like okay based on some decision that's happening way over way over",
				"tokens": [
					51463,
					935,
					291,
					1415,
					281,
					980,
					264,
					9461,
					411,
					1392,
					2361,
					322,
					512,
					3537,
					300,
					311,
					2737,
					636,
					670,
					636,
					670,
					51726
				],
				"temperature": 0,
				"avg_logprob": -0.05232333,
				"compression_ratio": 1.8846154,
				"no_speech_prob": 1.7691079e-12
			},
			{
				"id": 257,
				"seek": 11677,
				"start": 874.12854,
				"end": 881.46857,
				"text": " on the right here uh we want to make sure that the tool set is like only browser-based tools or",
				"tokens": [
					50365,
					322,
					264,
					558,
					510,
					2232,
					321,
					528,
					281,
					652,
					988,
					300,
					264,
					2290,
					992,
					307,
					411,
					787,
					11185,
					12,
					6032,
					3873,
					420,
					50732
				],
				"temperature": 0,
				"avg_logprob": -0.07013928,
				"compression_ratio": 1.8458498,
				"no_speech_prob": 1.2065419e-12
			},
			{
				"id": 258,
				"seek": 11677,
				"start": 881.46857,
				"end": 887.32855,
				"text": " whatever it is you basically you're going to get a slower iteration speed on the max token that",
				"tokens": [
					50732,
					2035,
					309,
					307,
					291,
					1936,
					291,
					434,
					516,
					281,
					483,
					257,
					14009,
					24784,
					3073,
					322,
					264,
					11469,
					14862,
					300,
					51025
				],
				"temperature": 0,
				"avg_logprob": -0.07013928,
				"compression_ratio": 1.8458498,
				"no_speech_prob": 1.2065419e-12
			},
			{
				"id": 259,
				"seek": 11677,
				"start": 887.32855,
				"end": 891.06854,
				"text": " gets generated yeah so when you change this you completely if you just like even if you're just",
				"tokens": [
					51025,
					2170,
					10833,
					1338,
					370,
					562,
					291,
					1319,
					341,
					291,
					2584,
					498,
					291,
					445,
					411,
					754,
					498,
					291,
					434,
					445,
					51212
				],
				"temperature": 0,
				"avg_logprob": -0.07013928,
				"compression_ratio": 1.8458498,
				"no_speech_prob": 1.2065419e-12
			},
			{
				"id": 260,
				"seek": 11677,
				"start": 891.06854,
				"end": 896.14856,
				"text": " removing stuff when you change things you completely kill the cache and so this has to be",
				"tokens": [
					51212,
					12720,
					1507,
					562,
					291,
					1319,
					721,
					291,
					2584,
					1961,
					264,
					19459,
					293,
					370,
					341,
					575,
					281,
					312,
					51466
				],
				"temperature": 0,
				"avg_logprob": -0.07013928,
				"compression_ratio": 1.8458498,
				"no_speech_prob": 1.2065419e-12
			},
			{
				"id": 261,
				"seek": 11677,
				"start": 896.14856,
				"end": 900.2886,
				"text": " recomputed from scratch which is going to be way slower and also like four times the cost",
				"tokens": [
					51466,
					23334,
					2582,
					292,
					490,
					8459,
					597,
					307,
					516,
					281,
					312,
					636,
					14009,
					293,
					611,
					411,
					1451,
					1413,
					264,
					2063,
					51673
				],
				"temperature": 0,
				"avg_logprob": -0.07013928,
				"compression_ratio": 1.8458498,
				"no_speech_prob": 1.2065419e-12
			},
			{
				"id": 262,
				"seek": 14293,
				"start": 900.2886,
				"end": 906.52856,
				"text": " exactly there's another subtle thing that probably isn't even obvious here almost everyone i know",
				"tokens": [
					50365,
					2293,
					456,
					311,
					1071,
					13743,
					551,
					300,
					1391,
					1943,
					380,
					754,
					6322,
					510,
					1920,
					1518,
					741,
					458,
					50677
				],
				"temperature": 0,
				"avg_logprob": -0.102265075,
				"compression_ratio": 1.8359375,
				"no_speech_prob": 1.8042231e-12
			},
			{
				"id": 263,
				"seek": 14293,
				"start": 906.52856,
				"end": 913.06854,
				"text": " that's doing some sort of chatbot will say like date in here in their system prompt and yet that",
				"tokens": [
					50677,
					300,
					311,
					884,
					512,
					1333,
					295,
					5081,
					18870,
					486,
					584,
					411,
					4002,
					294,
					510,
					294,
					641,
					1185,
					12391,
					293,
					1939,
					300,
					51004
				],
				"temperature": 0,
				"avg_logprob": -0.102265075,
				"compression_ratio": 1.8359375,
				"no_speech_prob": 1.8042231e-12
			},
			{
				"id": 264,
				"seek": 14293,
				"start": 913.06854,
				"end": 919.4285,
				"text": " that's killing the kv cache for no reason you're literally just hurting the cache every single time",
				"tokens": [
					51004,
					300,
					311,
					8011,
					264,
					350,
					85,
					19459,
					337,
					572,
					1778,
					291,
					434,
					3736,
					445,
					17744,
					264,
					19459,
					633,
					2167,
					565,
					51322
				],
				"temperature": 0,
				"avg_logprob": -0.102265075,
				"compression_ratio": 1.8359375,
				"no_speech_prob": 1.8042231e-12
			},
			{
				"id": 265,
				"seek": 14293,
				"start": 919.4285,
				"end": 924.18854,
				"text": " i mean if you include the time right if you just put the date it might be okay right",
				"tokens": [
					51322,
					741,
					914,
					498,
					291,
					4090,
					264,
					565,
					558,
					498,
					291,
					445,
					829,
					264,
					4002,
					309,
					1062,
					312,
					1392,
					558,
					51560
				],
				"temperature": 0,
				"avg_logprob": -0.102265075,
				"compression_ratio": 1.8359375,
				"no_speech_prob": 1.8042231e-12
			},
			{
				"id": 266,
				"seek": 14293,
				"start": 924.18854,
				"end": 928.70856,
				"text": " yes if you're putting the date sorry i should be you are correct on that there's some time",
				"tokens": [
					51560,
					2086,
					498,
					291,
					434,
					3372,
					264,
					4002,
					2597,
					741,
					820,
					312,
					291,
					366,
					3006,
					322,
					300,
					456,
					311,
					512,
					565,
					51786
				],
				"temperature": 0,
				"avg_logprob": -0.102265075,
				"compression_ratio": 1.8359375,
				"no_speech_prob": 1.8042231e-12
			},
			{
				"id": 267,
				"seek": 17135,
				"start": 928.70856,
				"end": 933.08856,
				"text": " resolution that you're definitely killing the KV cache. But even if you don't include the date,",
				"tokens": [
					50365,
					8669,
					300,
					291,
					434,
					2138,
					8011,
					264,
					591,
					53,
					19459,
					13,
					583,
					754,
					498,
					291,
					500,
					380,
					4090,
					264,
					4002,
					11,
					50584
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 268,
				"seek": 17135,
				"start": 933.14856,
				"end": 937.26855,
				"text": " if you have a long running thread, like last time we talked about DRM that was coming through from",
				"tokens": [
					50587,
					498,
					291,
					362,
					257,
					938,
					2614,
					7207,
					11,
					411,
					1036,
					565,
					321,
					2825,
					466,
					12118,
					44,
					300,
					390,
					1348,
					807,
					490,
					50793
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 269,
				"seek": 17135,
				"start": 937.26855,
				"end": 942.80853,
				"text": " that last time, the problem with DRM is it's a long process. Decaying resolution memory. That",
				"tokens": [
					50793,
					300,
					1036,
					565,
					11,
					264,
					1154,
					365,
					12118,
					44,
					307,
					309,
					311,
					257,
					938,
					1399,
					13,
					12427,
					32600,
					8669,
					4675,
					13,
					663,
					51070
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 270,
				"seek": 17135,
				"start": 942.80853,
				"end": 946.8685,
				"text": " was like the context engineering for a model that can remember stuff that happened months ago.",
				"tokens": [
					51070,
					390,
					411,
					264,
					4319,
					7043,
					337,
					257,
					2316,
					300,
					393,
					1604,
					1507,
					300,
					2011,
					2493,
					2057,
					13,
					51273
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 271,
				"seek": 17135,
				"start": 947.24854,
				"end": 951.88855,
				"text": " Exactly. Not a model, but an agent. But in that case, today's date is going to break the KV cache",
				"tokens": [
					51292,
					7587,
					13,
					1726,
					257,
					2316,
					11,
					457,
					364,
					9461,
					13,
					583,
					294,
					300,
					1389,
					11,
					965,
					311,
					4002,
					307,
					516,
					281,
					1821,
					264,
					591,
					53,
					19459,
					51524
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 272,
				"seek": 17135,
				"start": 951.88855,
				"end": 955.46857,
				"text": " all the time. You're just going to be slower for the entire workload that's happening.",
				"tokens": [
					51524,
					439,
					264,
					565,
					13,
					509,
					434,
					445,
					516,
					281,
					312,
					14009,
					337,
					264,
					2302,
					20139,
					300,
					311,
					2737,
					13,
					51703
				],
				"temperature": 0,
				"avg_logprob": -0.10512711,
				"compression_ratio": 1.7476923,
				"no_speech_prob": 1.9205722e-12
			},
			{
				"id": 273,
				"seek": 19811,
				"start": 955.9885,
				"end": 957.90857,
				"text": " It's generally always going to be good",
				"tokens": [
					50391,
					467,
					311,
					5101,
					1009,
					516,
					281,
					312,
					665,
					50487
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 274,
				"seek": 19811,
				"start": 957.90857,
				"end": 959.68854,
				"text": " to put the dynamic stuff in your system",
				"tokens": [
					50487,
					281,
					829,
					264,
					8546,
					1507,
					294,
					428,
					1185,
					50576
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 275,
				"seek": 19811,
				"start": 959.68854,
				"end": 960.70856,
				"text": " as late as possible.",
				"tokens": [
					50576,
					382,
					3469,
					382,
					1944,
					13,
					50627
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 276,
				"seek": 19811,
				"start": 961.30853,
				"end": 963.44855,
				"text": " That means if you have a giant chat thread",
				"tokens": [
					50657,
					663,
					1355,
					498,
					291,
					362,
					257,
					7410,
					5081,
					7207,
					50764
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 277,
				"seek": 19811,
				"start": 963.44855,
				"end": 964.2886,
				"text": " that you're running on,",
				"tokens": [
					50764,
					300,
					291,
					434,
					2614,
					322,
					11,
					50806
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 278,
				"seek": 19811,
				"start": 964.96857,
				"end": 966.06854,
				"text": " it might actually be,",
				"tokens": [
					50840,
					309,
					1062,
					767,
					312,
					11,
					50895
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 279,
				"seek": 19811,
				"start": 966.24854,
				"end": 968.1686,
				"text": " if you want the best performance,",
				"tokens": [
					50904,
					498,
					291,
					528,
					264,
					1151,
					3389,
					11,
					51000
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 280,
				"seek": 19811,
				"start": 969.2886,
				"end": 970.6085,
				"text": " why can't I not draw a rectangle?",
				"tokens": [
					51056,
					983,
					393,
					380,
					286,
					406,
					2642,
					257,
					21930,
					30,
					51122
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 281,
				"seek": 19811,
				"start": 971.20856,
				"end": 972.6085,
				"text": " It's probably going to be better",
				"tokens": [
					51152,
					467,
					311,
					1391,
					516,
					281,
					312,
					1101,
					51222
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 282,
				"seek": 19811,
				"start": 972.6085,
				"end": 973.7886,
				"text": " for you to have system prompt",
				"tokens": [
					51222,
					337,
					291,
					281,
					362,
					1185,
					12391,
					51281
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 283,
				"seek": 19811,
				"start": 973.7886,
				"end": 975.30853,
				"text": " or your chat messages",
				"tokens": [
					51281,
					420,
					428,
					5081,
					7897,
					51357
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 284,
				"seek": 19811,
				"start": 975.30853,
				"end": 978.44855,
				"text": " and then your dynamic variables at the very end.",
				"tokens": [
					51357,
					293,
					550,
					428,
					8546,
					9102,
					412,
					264,
					588,
					917,
					13,
					51514
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 285,
				"seek": 19811,
				"start": 979.08856,
				"end": 981.18854,
				"text": " Because then as your chat message grows,",
				"tokens": [
					51546,
					1436,
					550,
					382,
					428,
					5081,
					3636,
					13156,
					11,
					51651
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 286,
				"seek": 19811,
				"start": 981.56854,
				"end": 983.50854,
				"text": " you're actually going to have the ability",
				"tokens": [
					51670,
					291,
					434,
					767,
					516,
					281,
					362,
					264,
					3485,
					51767
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 287,
				"seek": 19811,
				"start": 983.50854,
				"end": 985.40857,
				"text": " to KV cache your chat history",
				"tokens": [
					51767,
					281,
					591,
					53,
					19459,
					428,
					5081,
					2503,
					51862
				],
				"temperature": 0,
				"avg_logprob": -0.20661719,
				"compression_ratio": 1.7773851,
				"no_speech_prob": 1.2448557e-12
			},
			{
				"id": 288,
				"seek": 22805,
				"start": 985.40857,
				"end": 991.7285,
				"text": " as well along with everything else so it's better to say today's date at the very end of this",
				"tokens": [
					50365,
					382,
					731,
					2051,
					365,
					1203,
					1646,
					370,
					309,
					311,
					1101,
					281,
					584,
					965,
					311,
					4002,
					412,
					264,
					588,
					917,
					295,
					341,
					50681
				],
				"temperature": 0,
				"avg_logprob": -0.052659005,
				"compression_ratio": 1.8188976,
				"no_speech_prob": 1.7486897e-12
			},
			{
				"id": 289,
				"seek": 22805,
				"start": 991.7285,
				"end": 997.7285,
				"text": " rather than putting it anywhere beforehand even at the end of the system message",
				"tokens": [
					50681,
					2831,
					813,
					3372,
					309,
					4992,
					22893,
					754,
					412,
					264,
					917,
					295,
					264,
					1185,
					3636,
					50981
				],
				"temperature": 0,
				"avg_logprob": -0.052659005,
				"compression_ratio": 1.8188976,
				"no_speech_prob": 1.7486897e-12
			},
			{
				"id": 290,
				"seek": 22805,
				"start": 997.7285,
				"end": 1005.52856,
				"text": " i don't know these kinds of things i see align with what this is so i imagine you're going to",
				"tokens": [
					50981,
					741,
					500,
					380,
					458,
					613,
					3685,
					295,
					721,
					741,
					536,
					7975,
					365,
					437,
					341,
					307,
					370,
					741,
					3811,
					291,
					434,
					516,
					281,
					51371
				],
				"temperature": 0,
				"avg_logprob": -0.052659005,
				"compression_ratio": 1.8188976,
				"no_speech_prob": 1.7486897e-12
			},
			{
				"id": 291,
				"seek": 22805,
				"start": 1005.52856,
				"end": 1009.40857,
				"text": " go to the point of like oh if you really want to dynamically change the tools and you don't want",
				"tokens": [
					51371,
					352,
					281,
					264,
					935,
					295,
					411,
					1954,
					498,
					291,
					534,
					528,
					281,
					43492,
					1319,
					264,
					3873,
					293,
					291,
					500,
					380,
					528,
					51565
				],
				"temperature": 0,
				"avg_logprob": -0.052659005,
				"compression_ratio": 1.8188976,
				"no_speech_prob": 1.7486897e-12
			},
			{
				"id": 292,
				"seek": 22805,
				"start": 1009.40857,
				"end": 1014.1686,
				"text": " to think about what we're about to go to which is like using the like log probabilities and like",
				"tokens": [
					51565,
					281,
					519,
					466,
					437,
					321,
					434,
					466,
					281,
					352,
					281,
					597,
					307,
					411,
					1228,
					264,
					411,
					3565,
					33783,
					293,
					411,
					51803
				],
				"temperature": 0,
				"avg_logprob": -0.052659005,
				"compression_ratio": 1.8188976,
				"no_speech_prob": 1.7486897e-12
			},
			{
				"id": 293,
				"seek": 25681,
				"start": 1014.1686,
				"end": 1017.80853,
				"text": " zeroing things out to remove them from the tool set",
				"tokens": [
					50365,
					4018,
					278,
					721,
					484,
					281,
					4159,
					552,
					490,
					264,
					2290,
					992,
					50547
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 294,
				"seek": 25681,
				"start": 1017.80853,
				"end": 1019.46857,
				"text": " that you could change your tools",
				"tokens": [
					50547,
					300,
					291,
					727,
					1319,
					428,
					3873,
					50630
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 295,
				"seek": 25681,
				"start": 1019.46857,
				"end": 1021.30853,
				"text": " and put the tools at the end of the context window",
				"tokens": [
					50630,
					293,
					829,
					264,
					3873,
					412,
					264,
					917,
					295,
					264,
					4319,
					4910,
					50722
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 296,
				"seek": 25681,
				"start": 1021.30853,
				"end": 1022.02856,
				"text": " instead of at the beginning.",
				"tokens": [
					50722,
					2602,
					295,
					412,
					264,
					2863,
					13,
					50758
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 297,
				"seek": 25681,
				"start": 1023.7285,
				"end": 1024.1285,
				"text": " Exactly.",
				"tokens": [
					50843,
					7587,
					13,
					50863
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 298,
				"seek": 25681,
				"start": 1024.7485,
				"end": 1026.6686,
				"text": " So if you're not, same thing with a tool set.",
				"tokens": [
					50894,
					407,
					498,
					291,
					434,
					406,
					11,
					912,
					551,
					365,
					257,
					2290,
					992,
					13,
					50990
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 299,
				"seek": 25681,
				"start": 1026.8485,
				"end": 1028.5686,
				"text": " Like if you want to dynamically change your tools,",
				"tokens": [
					50999,
					1743,
					498,
					291,
					528,
					281,
					43492,
					1319,
					428,
					3873,
					11,
					51085
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 300,
				"seek": 25681,
				"start": 1028.9086,
				"end": 1029.7686,
				"text": " you got to put them at the bottom.",
				"tokens": [
					51102,
					291,
					658,
					281,
					829,
					552,
					412,
					264,
					2767,
					13,
					51145
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 301,
				"seek": 25681,
				"start": 1030.4086,
				"end": 1031.5885,
				"text": " If you put them at the top,",
				"tokens": [
					51177,
					759,
					291,
					829,
					552,
					412,
					264,
					1192,
					11,
					51236
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 302,
				"seek": 25681,
				"start": 1031.7485,
				"end": 1034.4885,
				"text": " you were just shooting yourself in the foot in this system.",
				"tokens": [
					51244,
					291,
					645,
					445,
					5942,
					1803,
					294,
					264,
					2671,
					294,
					341,
					1185,
					13,
					51381
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 303,
				"seek": 25681,
				"start": 1034.6085,
				"end": 1036.2886,
				"text": " You will just get slower API calls,",
				"tokens": [
					51387,
					509,
					486,
					445,
					483,
					14009,
					9362,
					5498,
					11,
					51471
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 304,
				"seek": 25681,
				"start": 1036.3885,
				"end": 1037.5286,
				"text": " more expensive API calls",
				"tokens": [
					51476,
					544,
					5124,
					9362,
					5498,
					51533
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 305,
				"seek": 25681,
				"start": 1037.5286,
				"end": 1039.5085,
				"text": " with no real benefit that you're really having.",
				"tokens": [
					51533,
					365,
					572,
					957,
					5121,
					300,
					291,
					434,
					534,
					1419,
					13,
					51632
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 306,
				"seek": 25681,
				"start": 1040.5885,
				"end": 1042.6285,
				"text": " There's a secondary benefit that you get for free,",
				"tokens": [
					51686,
					821,
					311,
					257,
					11396,
					5121,
					300,
					291,
					483,
					337,
					1737,
					11,
					51788
				],
				"temperature": 0,
				"avg_logprob": -0.15193288,
				"compression_ratio": 1.8433334,
				"no_speech_prob": 1.3779193e-12
			},
			{
				"id": 307,
				"seek": 28527,
				"start": 1042.6285,
				"end": 1045.3286,
				"text": " which is at the very, very end,",
				"tokens": [
					50365,
					597,
					307,
					412,
					264,
					588,
					11,
					588,
					917,
					11,
					50500
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 308,
				"seek": 28527,
				"start": 1045.7485,
				"end": 1046.9685,
				"text": " if you put things down here,",
				"tokens": [
					50521,
					498,
					291,
					829,
					721,
					760,
					510,
					11,
					50582
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 309,
				"seek": 28527,
				"start": 1047.0286,
				"end": 1048.8485,
				"text": " the model is just likely to pay more attention to it.",
				"tokens": [
					50585,
					264,
					2316,
					307,
					445,
					3700,
					281,
					1689,
					544,
					3202,
					281,
					309,
					13,
					50676
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 310,
				"seek": 28527,
				"start": 1048.9086,
				"end": 1051.2485,
				"text": " So anything that is actually relevant and highly dynamic,",
				"tokens": [
					50679,
					407,
					1340,
					300,
					307,
					767,
					7340,
					293,
					5405,
					8546,
					11,
					50796
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 311,
				"seek": 28527,
				"start": 1052.1885,
				"end": 1053.7285,
				"text": " it's way easier to guarantee the model",
				"tokens": [
					50843,
					309,
					311,
					636,
					3571,
					281,
					10815,
					264,
					2316,
					50920
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 312,
				"seek": 28527,
				"start": 1053.7285,
				"end": 1054.8086,
				"text": " won't accidentally forget it",
				"tokens": [
					50920,
					1582,
					380,
					15715,
					2870,
					309,
					50974
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 313,
				"seek": 28527,
				"start": 1054.8086,
				"end": 1056.5085,
				"text": " because it has a recency bias in general.",
				"tokens": [
					50974,
					570,
					309,
					575,
					257,
					850,
					3020,
					12577,
					294,
					2674,
					13,
					51059
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 314,
				"seek": 28527,
				"start": 1057.4685,
				"end": 1059.5486,
				"text": " So question like application of that.",
				"tokens": [
					51107,
					407,
					1168,
					411,
					3861,
					295,
					300,
					13,
					51211
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 315,
				"seek": 28527,
				"start": 1059.8286,
				"end": 1061.7085,
				"text": " And I think this is maybe also in the paper.",
				"tokens": [
					51225,
					400,
					286,
					519,
					341,
					307,
					1310,
					611,
					294,
					264,
					3035,
					13,
					51319
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 316,
				"seek": 28527,
				"start": 1061.9685,
				"end": 1065.1686,
				"text": " And I actually was doing some more reverse engineering",
				"tokens": [
					51332,
					400,
					286,
					767,
					390,
					884,
					512,
					544,
					9943,
					7043,
					51492
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 317,
				"seek": 28527,
				"start": 1065.1686,
				"end": 1066.9685,
				"text": " of cloud code with a proxy the other day,",
				"tokens": [
					51492,
					295,
					4588,
					3089,
					365,
					257,
					29690,
					264,
					661,
					786,
					11,
					51582
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 318,
				"seek": 28527,
				"start": 1067.0885,
				"end": 1069.7886,
				"text": " trying to figure out what the to-do tool does.",
				"tokens": [
					51588,
					1382,
					281,
					2573,
					484,
					437,
					264,
					281,
					12,
					2595,
					2290,
					775,
					13,
					51723
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 319,
				"seek": 28527,
				"start": 1070.1285,
				"end": 1071.2085,
				"text": " And what I would think would be,",
				"tokens": [
					51740,
					400,
					437,
					286,
					576,
					519,
					576,
					312,
					11,
					51794
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 320,
				"seek": 28527,
				"start": 1071.2886,
				"end": 1072.5286,
				"text": " hey, once you write to the to-dos,",
				"tokens": [
					51798,
					4177,
					11,
					1564,
					291,
					2464,
					281,
					264,
					281,
					12,
					33749,
					11,
					51860
				],
				"temperature": 0,
				"avg_logprob": -0.19075048,
				"compression_ratio": 1.7379518,
				"no_speech_prob": 2.4755937e-12
			},
			{
				"id": 321,
				"seek": 31527,
				"start": 1072.6285,
				"end": 1077.8885,
				"text": " the system will occasionally re-inject the list of to-dos near the end of the context window and say,",
				"tokens": [
					50365,
					264,
					1185,
					486,
					16895,
					319,
					12,
					259,
					1020,
					264,
					1329,
					295,
					281,
					12,
					33749,
					2651,
					264,
					917,
					295,
					264,
					4319,
					4910,
					293,
					584,
					11,
					50628
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 322,
				"seek": 31527,
				"start": 1077.9685,
				"end": 1080.8086,
				"text": " hey, by the way, here's what you said you were going to be working on.",
				"tokens": [
					50632,
					4177,
					11,
					538,
					264,
					636,
					11,
					510,
					311,
					437,
					291,
					848,
					291,
					645,
					516,
					281,
					312,
					1364,
					322,
					13,
					50774
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 323,
				"seek": 31527,
				"start": 1080.8086,
				"end": 1087.5085,
				"text": " And so you force the attention to be on that stuff rather than like hoping that Claude will remember it did that 15 turns ago.",
				"tokens": [
					50774,
					400,
					370,
					291,
					3464,
					264,
					3202,
					281,
					312,
					322,
					300,
					1507,
					2831,
					813,
					411,
					7159,
					300,
					12947,
					2303,
					486,
					1604,
					309,
					630,
					300,
					2119,
					4523,
					2057,
					13,
					51109
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 324,
				"seek": 31527,
				"start": 1088.5885,
				"end": 1092.2485,
				"text": " Exactly. And that's kind of what one of the steps down here talks about.",
				"tokens": [
					51163,
					7587,
					13,
					400,
					300,
					311,
					733,
					295,
					437,
					472,
					295,
					264,
					4439,
					760,
					510,
					6686,
					466,
					13,
					51346
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 325,
				"seek": 31527,
				"start": 1092.5085,
				"end": 1093.9885,
				"text": " Oh, bomb cashing doesn't really matter.",
				"tokens": [
					51359,
					876,
					11,
					7851,
					3058,
					571,
					1177,
					380,
					534,
					1871,
					13,
					51433
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 326,
				"seek": 31527,
				"start": 1094.7485,
				"end": 1098.8685,
				"text": " In the Manus paper, where they release some of the same stuff, which is.",
				"tokens": [
					51471,
					682,
					264,
					2458,
					301,
					3035,
					11,
					689,
					436,
					4374,
					512,
					295,
					264,
					912,
					1507,
					11,
					597,
					307,
					13,
					51677
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 327,
				"seek": 31527,
				"start": 1101.4485,
				"end": 1102.3685,
				"text": " Where is this?",
				"tokens": [
					51806,
					2305,
					307,
					341,
					30,
					51852
				],
				"temperature": 0,
				"avg_logprob": -0.1795612,
				"compression_ratio": 1.6611296,
				"no_speech_prob": 1.7900986e-12
			},
			{
				"id": 328,
				"seek": 34527,
				"start": 1102.6285,
				"end": 1109.3885,
				"text": " where's the repetition one oh your recitation so the whole point of this is saying the same",
				"tokens": [
					50365,
					689,
					311,
					264,
					30432,
					472,
					1954,
					428,
					850,
					4614,
					370,
					264,
					1379,
					935,
					295,
					341,
					307,
					1566,
					264,
					912,
					50703
				],
				"temperature": 0,
				"avg_logprob": -0.03267134,
				"compression_ratio": 1.780303,
				"no_speech_prob": 2.427796e-12
			},
			{
				"id": 329,
				"seek": 34527,
				"start": 1109.3885,
				"end": 1114.8086,
				"text": " thing oh yeah this is simply just stacking stuff all the way down eventually your information will",
				"tokens": [
					50703,
					551,
					1954,
					1338,
					341,
					307,
					2935,
					445,
					41376,
					1507,
					439,
					264,
					636,
					760,
					4728,
					428,
					1589,
					486,
					50974
				],
				"temperature": 0,
				"avg_logprob": -0.03267134,
				"compression_ratio": 1.780303,
				"no_speech_prob": 2.427796e-12
			},
			{
				"id": 330,
				"seek": 34527,
				"start": 1114.8086,
				"end": 1120.1686,
				"text": " get lost it's not a just like with a normal human if you gave them a giant to-do list of everything",
				"tokens": [
					50974,
					483,
					2731,
					309,
					311,
					406,
					257,
					445,
					411,
					365,
					257,
					2710,
					1952,
					498,
					291,
					2729,
					552,
					257,
					7410,
					281,
					12,
					2595,
					1329,
					295,
					1203,
					51242
				],
				"temperature": 0,
				"avg_logprob": -0.03267134,
				"compression_ratio": 1.780303,
				"no_speech_prob": 2.427796e-12
			},
			{
				"id": 331,
				"seek": 34527,
				"start": 1120.1686,
				"end": 1123.5286,
				"text": " to do and you have one single paper at the very front that they have to somehow remember",
				"tokens": [
					51242,
					281,
					360,
					293,
					291,
					362,
					472,
					2167,
					3035,
					412,
					264,
					588,
					1868,
					300,
					436,
					362,
					281,
					6063,
					1604,
					51410
				],
				"temperature": 0,
				"avg_logprob": -0.03267134,
				"compression_ratio": 1.780303,
				"no_speech_prob": 2.427796e-12
			},
			{
				"id": 332,
				"seek": 34527,
				"start": 1123.5286,
				"end": 1128.9685,
				"text": " they will forget steps we build processes in place to make sure that stuff gets duplicated",
				"tokens": [
					51410,
					436,
					486,
					2870,
					4439,
					321,
					1322,
					7555,
					294,
					1081,
					281,
					652,
					988,
					300,
					1507,
					2170,
					1581,
					564,
					3587,
					51682
				],
				"temperature": 0,
				"avg_logprob": -0.03267134,
				"compression_ratio": 1.780303,
				"no_speech_prob": 2.427796e-12
			},
			{
				"id": 333,
				"seek": 37161,
				"start": 1128.9685,
				"end": 1131.7285,
				"text": " and done more than once in like highly yield scenarios,",
				"tokens": [
					50365,
					293,
					1096,
					544,
					813,
					1564,
					294,
					411,
					5405,
					11257,
					15077,
					11,
					50503
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 334,
				"seek": 37161,
				"start": 1132.2285,
				"end": 1133.7686,
				"text": " like if I have a rocket ship that's going to launch,",
				"tokens": [
					50528,
					411,
					498,
					286,
					362,
					257,
					13012,
					5374,
					300,
					311,
					516,
					281,
					4025,
					11,
					50605
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 335,
				"seek": 37161,
				"start": 1134.0686,
				"end": 1137.2085,
				"text": " I will make sure 15 people make check every single thing",
				"tokens": [
					50620,
					286,
					486,
					652,
					988,
					2119,
					561,
					652,
					1520,
					633,
					2167,
					551,
					50777
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 336,
				"seek": 37161,
				"start": 1137.2085,
				"end": 1139.6085,
				"text": " because it's just way less likely that someone will slip up",
				"tokens": [
					50777,
					570,
					309,
					311,
					445,
					636,
					1570,
					3700,
					300,
					1580,
					486,
					11140,
					493,
					50897
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 337,
				"seek": 37161,
				"start": 1139.6085,
				"end": 1141.0486,
				"text": " and make a mistake on all different things.",
				"tokens": [
					50897,
					293,
					652,
					257,
					6146,
					322,
					439,
					819,
					721,
					13,
					50969
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 338,
				"seek": 37161,
				"start": 1141.9286,
				"end": 1143.7485,
				"text": " The model kind of behaves similarly as well.",
				"tokens": [
					51013,
					440,
					2316,
					733,
					295,
					36896,
					14138,
					382,
					731,
					13,
					51104
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 339,
				"seek": 37161,
				"start": 1144.3885,
				"end": 1146.5885,
				"text": " The more steps you put on it, and as the models get better,",
				"tokens": [
					51136,
					440,
					544,
					4439,
					291,
					829,
					322,
					309,
					11,
					293,
					382,
					264,
					5245,
					483,
					1101,
					11,
					51246
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 340,
				"seek": 37161,
				"start": 1146.9086,
				"end": 1149.0085,
				"text": " the distance between how long it can remember",
				"tokens": [
					51262,
					264,
					4560,
					1296,
					577,
					938,
					309,
					393,
					1604,
					51367
				],
				"temperature": 0,
				"avg_logprob": -0.16451721,
				"compression_ratio": 1.745509,
				"no_speech_prob": 1.264463e-12
			},
			{
				"id": 341,
				"seek": 1150,
				"start": 1149.0085,
				"end": 1160.7728,
				"text": " and what it can remember does go up That undeniably true But there always a distance at which it will never work as well So having some repetition in there is going to make a huge impact on your output quality",
				"tokens": [
					51367,
					293,
					437,
					309,
					393,
					1604,
					775,
					352,
					493,
					13,
					51431,
					51439,
					663,
					311,
					674,
					15711,
					1188,
					2074,
					13,
					51512,
					51555,
					583,
					456,
					311,
					1009,
					257,
					4560,
					412,
					597,
					309,
					486,
					1128,
					589,
					382,
					731,
					13,
					50658,
					50672,
					407,
					1419,
					512,
					30432,
					294,
					456,
					307,
					516,
					281,
					652,
					257,
					2603,
					2712,
					322,
					428,
					5598,
					3125,
					50903
				],
				"temperature": 0,
				"avg_logprob": -0.05824816,
				"compression_ratio": 1.6715976,
				"no_speech_prob": 2.2628943e-12
			},
			{
				"id": 342,
				"seek": 1150,
				"start": 1160.7728,
				"end": 1165.4928,
				"text": " for that same reason. Because what they're really saying here is they found that you can do about",
				"tokens": [
					50903,
					337,
					300,
					912,
					1778,
					13,
					1436,
					437,
					436,
					434,
					534,
					1566,
					510,
					307,
					436,
					1352,
					300,
					291,
					393,
					360,
					466,
					51139
				],
				"temperature": 0,
				"avg_logprob": -0.05824816,
				"compression_ratio": 1.6715976,
				"no_speech_prob": 2.2628943e-12
			},
			{
				"id": 343,
				"seek": 1150,
				"start": 1165.4928,
				"end": 1170.4528,
				"text": " 50 tool calls to actually produce their tasks. But most models will basically go off track",
				"tokens": [
					51139,
					2625,
					2290,
					5498,
					281,
					767,
					5258,
					641,
					9608,
					13,
					583,
					881,
					5245,
					486,
					1936,
					352,
					766,
					2837,
					51387
				],
				"temperature": 0,
				"avg_logprob": -0.05824816,
				"compression_ratio": 1.6715976,
				"no_speech_prob": 2.2628943e-12
			},
			{
				"id": 344,
				"seek": 1150,
				"start": 1170.4528,
				"end": 1176.0128,
				"text": " somewhere in the middle really, really fast. What we found in general, I'm like, why does this",
				"tokens": [
					51387,
					4079,
					294,
					264,
					2808,
					534,
					11,
					534,
					2370,
					13,
					708,
					321,
					1352,
					294,
					2674,
					11,
					286,
					478,
					411,
					11,
					983,
					775,
					341,
					51665
				],
				"temperature": 0,
				"avg_logprob": -0.05824816,
				"compression_ratio": 1.6715976,
				"no_speech_prob": 2.2628943e-12
			},
			{
				"id": 345,
				"seek": 1150,
				"start": 1176.0128,
				"end": 1179.5529,
				"text": " happen? Well, if you think about how attention works, you'll often see the needle in the haystack",
				"tokens": [
					51665,
					1051,
					30,
					1042,
					11,
					498,
					291,
					519,
					466,
					577,
					3202,
					1985,
					11,
					291,
					603,
					2049,
					536,
					264,
					11037,
					294,
					264,
					4842,
					372,
					501,
					51842
				],
				"temperature": 0,
				"avg_logprob": -0.05824816,
				"compression_ratio": 1.6715976,
				"no_speech_prob": 2.2628943e-12
			},
			{
				"id": 346,
				"seek": 4104,
				"start": 1179.5529,
				"end": 1185.7728,
				"text": " problem as a giant benchmark that people go and evaluate for well terrible benchmark i don't know",
				"tokens": [
					50365,
					1154,
					382,
					257,
					7410,
					18927,
					300,
					561,
					352,
					293,
					13059,
					337,
					731,
					6237,
					18927,
					741,
					500,
					380,
					458,
					50676
				],
				"temperature": 0,
				"avg_logprob": -0.04117317,
				"compression_ratio": 1.8423077,
				"no_speech_prob": 1.7623662e-12
			},
			{
				"id": 347,
				"seek": 4104,
				"start": 1185.7728,
				"end": 1191.4128,
				"text": " about terrible benchmark but like in general in english language it makes sense that words near",
				"tokens": [
					50676,
					466,
					6237,
					18927,
					457,
					411,
					294,
					2674,
					294,
					32169,
					2856,
					309,
					1669,
					2020,
					300,
					2283,
					2651,
					50958
				],
				"temperature": 0,
				"avg_logprob": -0.04117317,
				"compression_ratio": 1.8423077,
				"no_speech_prob": 1.7623662e-12
			},
			{
				"id": 348,
				"seek": 4104,
				"start": 1191.4128,
				"end": 1197.5529,
				"text": " each other have way more impact than words far away from each other it's just the any sort of",
				"tokens": [
					50958,
					1184,
					661,
					362,
					636,
					544,
					2712,
					813,
					2283,
					1400,
					1314,
					490,
					1184,
					661,
					309,
					311,
					445,
					264,
					604,
					1333,
					295,
					51265
				],
				"temperature": 0,
				"avg_logprob": -0.04117317,
				"compression_ratio": 1.8423077,
				"no_speech_prob": 1.7623662e-12
			},
			{
				"id": 349,
				"seek": 4104,
				"start": 1197.5529,
				"end": 1202.2928,
				"text": " data set that you're going to go train on is always going to have that the sentence i said at the very",
				"tokens": [
					51265,
					1412,
					992,
					300,
					291,
					434,
					516,
					281,
					352,
					3847,
					322,
					307,
					1009,
					516,
					281,
					362,
					300,
					264,
					8174,
					741,
					848,
					412,
					264,
					588,
					51502
				],
				"temperature": 0,
				"avg_logprob": -0.04117317,
				"compression_ratio": 1.8423077,
				"no_speech_prob": 1.7623662e-12
			},
			{
				"id": 350,
				"seek": 4104,
				"start": 1202.2928,
				"end": 1207.5128,
				"text": " beginning of this uh talk matters way less than the sentence i just said two minutes ago",
				"tokens": [
					51502,
					2863,
					295,
					341,
					2232,
					751,
					7001,
					636,
					1570,
					813,
					264,
					8174,
					741,
					445,
					848,
					732,
					2077,
					2057,
					51763
				],
				"temperature": 0,
				"avg_logprob": -0.04117317,
				"compression_ratio": 1.8423077,
				"no_speech_prob": 1.7623662e-12
			},
			{
				"id": 351,
				"seek": 6900,
				"start": 1207.5128,
				"end": 1213.3728,
				"text": " and that's the whole point. Well, it matters way less to what you're about to say next.",
				"tokens": [
					50365,
					293,
					300,
					311,
					264,
					1379,
					935,
					13,
					1042,
					11,
					309,
					7001,
					636,
					1570,
					281,
					437,
					291,
					434,
					466,
					281,
					584,
					958,
					13,
					50658
				],
				"temperature": 0,
				"avg_logprob": -0.10428377,
				"compression_ratio": 1.828125,
				"no_speech_prob": 1.3565605e-12
			},
			{
				"id": 352,
				"seek": 6900,
				"start": 1213.5128,
				"end": 1217.2528,
				"text": " Yes. Sorry. Yes, exactly. And that's the same thing that the model is going through.",
				"tokens": [
					50665,
					1079,
					13,
					4919,
					13,
					1079,
					11,
					2293,
					13,
					400,
					300,
					311,
					264,
					912,
					551,
					300,
					264,
					2316,
					307,
					516,
					807,
					13,
					50852
				],
				"temperature": 0,
				"avg_logprob": -0.10428377,
				"compression_ratio": 1.828125,
				"no_speech_prob": 1.3565605e-12
			},
			{
				"id": 353,
				"seek": 6900,
				"start": 1217.5728,
				"end": 1223.9529,
				"text": " The thing that I want to produce over here is way, way more likely to be related to something that",
				"tokens": [
					50868,
					440,
					551,
					300,
					286,
					528,
					281,
					5258,
					670,
					510,
					307,
					636,
					11,
					636,
					544,
					3700,
					281,
					312,
					4077,
					281,
					746,
					300,
					51187
				],
				"temperature": 0,
				"avg_logprob": -0.10428377,
				"compression_ratio": 1.828125,
				"no_speech_prob": 1.3565605e-12
			},
			{
				"id": 354,
				"seek": 6900,
				"start": 1223.9529,
				"end": 1228.2328,
				"text": " I'm linking over here. I don't know how to connect those than it is to be something I'm linking over",
				"tokens": [
					51187,
					286,
					478,
					25775,
					670,
					510,
					13,
					286,
					500,
					380,
					458,
					577,
					281,
					1745,
					729,
					813,
					309,
					307,
					281,
					312,
					746,
					286,
					478,
					25775,
					670,
					51401
				],
				"temperature": 0,
				"avg_logprob": -0.10428377,
				"compression_ratio": 1.828125,
				"no_speech_prob": 1.3565605e-12
			},
			{
				"id": 355,
				"seek": 6900,
				"start": 1228.2328,
				"end": 1236.7928,
				"text": " here. Now that it's true that system messages are special and I do want system messages treated",
				"tokens": [
					51401,
					510,
					13,
					823,
					300,
					309,
					311,
					2074,
					300,
					1185,
					7897,
					366,
					2121,
					293,
					286,
					360,
					528,
					1185,
					7897,
					8668,
					51829
				],
				"temperature": 0,
				"avg_logprob": -0.10428377,
				"compression_ratio": 1.828125,
				"no_speech_prob": 1.3565605e-12
			},
			{
				"id": 356,
				"seek": 9828,
				"start": 1236.7928,
				"end": 1240.3728,
				"text": " differently. And that's why the model trainers are working on training the model to understand",
				"tokens": [
					50365,
					7614,
					13,
					400,
					300,
					311,
					983,
					264,
					2316,
					35393,
					366,
					1364,
					322,
					3097,
					264,
					2316,
					281,
					1223,
					50544
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 357,
				"seek": 9828,
				"start": 1240.3728,
				"end": 1244.8328,
				"text": " that better. That system as it is do have impact throughout the entire span of the conversation.",
				"tokens": [
					50544,
					300,
					1101,
					13,
					663,
					1185,
					382,
					309,
					307,
					360,
					362,
					2712,
					3710,
					264,
					2302,
					16174,
					295,
					264,
					3761,
					13,
					50767
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 358,
				"seek": 9828,
				"start": 1245.3328,
				"end": 1250.0128,
				"text": " But that's a training task. And we can wait for the models to get trained to do that better,",
				"tokens": [
					50792,
					583,
					300,
					311,
					257,
					3097,
					5633,
					13,
					400,
					321,
					393,
					1699,
					337,
					264,
					5245,
					281,
					483,
					8895,
					281,
					360,
					300,
					1101,
					11,
					51026
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 359,
				"seek": 9828,
				"start": 1250.6929,
				"end": 1254.7728,
				"text": " but we don't have that guarantee. So the easiest way to provide the guarantee and just to get the",
				"tokens": [
					51060,
					457,
					321,
					500,
					380,
					362,
					300,
					10815,
					13,
					407,
					264,
					12889,
					636,
					281,
					2893,
					264,
					10815,
					293,
					445,
					281,
					483,
					264,
					51264
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 360,
				"seek": 9828,
				"start": 1254.7728,
				"end": 1259.7328,
				"text": " best results is to, again, just put things that are most relevant to what the user needs to do",
				"tokens": [
					51264,
					1151,
					3542,
					307,
					281,
					11,
					797,
					11,
					445,
					829,
					721,
					300,
					366,
					881,
					7340,
					281,
					437,
					264,
					4195,
					2203,
					281,
					360,
					51512
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 361,
				"seek": 9828,
				"start": 1259.7328,
				"end": 1265.1328,
				"text": " as close to what it needs to happen next. So if I need to complete a new task and I know that the",
				"tokens": [
					51512,
					382,
					1998,
					281,
					437,
					309,
					2203,
					281,
					1051,
					958,
					13,
					407,
					498,
					286,
					643,
					281,
					3566,
					257,
					777,
					5633,
					293,
					286,
					458,
					300,
					264,
					51782
				],
				"temperature": 0,
				"avg_logprob": -0.075533435,
				"compression_ratio": 1.879085,
				"no_speech_prob": 1.3942038e-12
			},
			{
				"id": 362,
				"seek": 12662,
				"start": 1265.1328,
				"end": 1270.5728,
				"text": " model just checked off. Let's say I'm building cloud code and the model just finished task one.",
				"tokens": [
					50365,
					2316,
					445,
					10033,
					766,
					13,
					961,
					311,
					584,
					286,
					478,
					2390,
					4588,
					3089,
					293,
					264,
					2316,
					445,
					4335,
					5633,
					472,
					13,
					50637
				],
				"temperature": 0,
				"avg_logprob": -0.0852755,
				"compression_ratio": 1.7230216,
				"no_speech_prob": 1.2894277e-12
			},
			{
				"id": 363,
				"seek": 12662,
				"start": 1271.7728,
				"end": 1276.5328,
				"text": " I should probably repeat the task list again, assuming I'm using a very dumb model to say,",
				"tokens": [
					50697,
					286,
					820,
					1391,
					7149,
					264,
					5633,
					1329,
					797,
					11,
					11926,
					286,
					478,
					1228,
					257,
					588,
					10316,
					2316,
					281,
					584,
					11,
					50935
				],
				"temperature": 0,
				"avg_logprob": -0.0852755,
				"compression_ratio": 1.7230216,
				"no_speech_prob": 1.2894277e-12
			},
			{
				"id": 364,
				"seek": 12662,
				"start": 1276.6929,
				"end": 1281.8528,
				"text": " this is the task, which task should you do next? It's just going to work better than just having",
				"tokens": [
					50943,
					341,
					307,
					264,
					5633,
					11,
					597,
					5633,
					820,
					291,
					360,
					958,
					30,
					467,
					311,
					445,
					516,
					281,
					589,
					1101,
					813,
					445,
					1419,
					51201
				],
				"temperature": 0,
				"avg_logprob": -0.0852755,
				"compression_ratio": 1.7230216,
				"no_speech_prob": 1.2894277e-12
			},
			{
				"id": 365,
				"seek": 12662,
				"start": 1281.8528,
				"end": 1286.6328,
				"text": " the task at the top, saying I completed task one. Now, what do I do next? Because the distance is",
				"tokens": [
					51201,
					264,
					5633,
					412,
					264,
					1192,
					11,
					1566,
					286,
					7365,
					5633,
					472,
					13,
					823,
					11,
					437,
					360,
					286,
					360,
					958,
					30,
					1436,
					264,
					4560,
					307,
					51440
				],
				"temperature": 0,
				"avg_logprob": -0.0852755,
				"compression_ratio": 1.7230216,
				"no_speech_prob": 1.2894277e-12
			},
			{
				"id": 366,
				"seek": 12662,
				"start": 1286.6328,
				"end": 1291.8129,
				"text": " just further. And in that case, I happen to know exactly what needs to happen, which is I need to",
				"tokens": [
					51440,
					445,
					3052,
					13,
					400,
					294,
					300,
					1389,
					11,
					286,
					1051,
					281,
					458,
					2293,
					437,
					2203,
					281,
					1051,
					11,
					597,
					307,
					286,
					643,
					281,
					51699
				],
				"temperature": 0,
				"avg_logprob": -0.0852755,
				"compression_ratio": 1.7230216,
				"no_speech_prob": 1.2894277e-12
			},
			{
				"id": 367,
				"seek": 15330,
				"start": 1291.8129,
				"end": 1298.0328,
				"text": " make progress on a task list. So it's easy for me to manually inject that. But the more I can do",
				"tokens": [
					50365,
					652,
					4205,
					322,
					257,
					5633,
					1329,
					13,
					407,
					309,
					311,
					1858,
					337,
					385,
					281,
					16945,
					10711,
					300,
					13,
					583,
					264,
					544,
					286,
					393,
					360,
					50676
				],
				"temperature": 0,
				"avg_logprob": -0.047404867,
				"compression_ratio": 1.6875,
				"no_speech_prob": 1.2448454e-12
			},
			{
				"id": 368,
				"seek": 15330,
				"start": 1298.0328,
				"end": 1306.2528,
				"text": " that sort of behavior, the more consistency I'll get out of my model outputs. And this just feels",
				"tokens": [
					50676,
					300,
					1333,
					295,
					5223,
					11,
					264,
					544,
					14416,
					286,
					603,
					483,
					484,
					295,
					452,
					2316,
					23930,
					13,
					400,
					341,
					445,
					3417,
					51087
				],
				"temperature": 0,
				"avg_logprob": -0.047404867,
				"compression_ratio": 1.6875,
				"no_speech_prob": 1.2448454e-12
			},
			{
				"id": 369,
				"seek": 15330,
				"start": 1306.2528,
				"end": 1310.6929,
				"text": " like another recitation of kind of that same theme of like, yeah, you could just go back and forth",
				"tokens": [
					51087,
					411,
					1071,
					850,
					4614,
					295,
					733,
					295,
					300,
					912,
					6314,
					295,
					411,
					11,
					1338,
					11,
					291,
					727,
					445,
					352,
					646,
					293,
					5220,
					51309
				],
				"temperature": 0,
				"avg_logprob": -0.047404867,
				"compression_ratio": 1.6875,
				"no_speech_prob": 1.2448454e-12
			},
			{
				"id": 370,
				"seek": 15330,
				"start": 1310.6929,
				"end": 1315.0928,
				"text": " with the model forever and use the default like tool call, tool, tool call, tool call, context",
				"tokens": [
					51309,
					365,
					264,
					2316,
					5680,
					293,
					764,
					264,
					7576,
					411,
					2290,
					818,
					11,
					2290,
					11,
					2290,
					818,
					11,
					2290,
					818,
					11,
					4319,
					51529
				],
				"temperature": 0,
				"avg_logprob": -0.047404867,
				"compression_ratio": 1.6875,
				"no_speech_prob": 1.2448454e-12
			},
			{
				"id": 371,
				"seek": 15330,
				"start": 1315.0928,
				"end": 1321.6528,
				"text": " window. But if it lets you push your accuracy by 5% or your speed or your cache efficiency by 5%,",
				"tokens": [
					51529,
					4910,
					13,
					583,
					498,
					309,
					6653,
					291,
					2944,
					428,
					14170,
					538,
					1025,
					4,
					420,
					428,
					3073,
					420,
					428,
					19459,
					10493,
					538,
					1025,
					8923,
					51857
				],
				"temperature": 0,
				"avg_logprob": -0.047404867,
				"compression_ratio": 1.6875,
				"no_speech_prob": 1.2448454e-12
			},
			{
				"id": 372,
				"seek": 18314,
				"start": 1321.6528,
				"end": 1325.4329,
				"text": " then why not engineer the thing to be as good as possible?",
				"tokens": [
					50365,
					550,
					983,
					406,
					11403,
					264,
					551,
					281,
					312,
					382,
					665,
					382,
					1944,
					30,
					50554
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 373,
				"seek": 18314,
				"start": 1326.0328,
				"end": 1326.4329,
				"text": " Exactly.",
				"tokens": [
					50584,
					7587,
					13,
					50604
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 374,
				"seek": 18314,
				"start": 1327.1528,
				"end": 1327.5529,
				"text": " Exactly.",
				"tokens": [
					50640,
					7587,
					13,
					50660
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 375,
				"seek": 18314,
				"start": 1329.4728,
				"end": 1331.0928,
				"text": " There's two questions really fast,",
				"tokens": [
					50756,
					821,
					311,
					732,
					1651,
					534,
					2370,
					11,
					50837
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 376,
				"seek": 18314,
				"start": 1331.1328,
				"end": 1333.0728,
				"text": " and I want to make sure we talk about them before we go too deep.",
				"tokens": [
					50839,
					293,
					286,
					528,
					281,
					652,
					988,
					321,
					751,
					466,
					552,
					949,
					321,
					352,
					886,
					2452,
					13,
					50936
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 377,
				"seek": 18314,
				"start": 1333.7928,
				"end": 1334.3528,
				"text": " Oh, sorry.",
				"tokens": [
					50972,
					876,
					11,
					2597,
					13,
					51000
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 378,
				"seek": 18314,
				"start": 1334.6528,
				"end": 1335.9329,
				"text": " I know I'm supposed to be on question duty.",
				"tokens": [
					51015,
					286,
					458,
					286,
					478,
					3442,
					281,
					312,
					322,
					1168,
					9776,
					13,
					51079
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 379,
				"seek": 18314,
				"start": 1336.5128,
				"end": 1338.3129,
				"text": " If you have users using the same models,",
				"tokens": [
					51108,
					759,
					291,
					362,
					5022,
					1228,
					264,
					912,
					5245,
					11,
					51198
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 380,
				"seek": 18314,
				"start": 1338.4128,
				"end": 1342.6729,
				"text": " how will this cache be preserved between the same user calls?",
				"tokens": [
					51203,
					577,
					486,
					341,
					19459,
					312,
					22242,
					1296,
					264,
					912,
					4195,
					5498,
					30,
					51416
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 381,
				"seek": 18314,
				"start": 1343.4529,
				"end": 1345.2928,
				"text": " Well, the way that the model provides it is,",
				"tokens": [
					51455,
					1042,
					11,
					264,
					636,
					300,
					264,
					2316,
					6417,
					309,
					307,
					11,
					51547
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 382,
				"seek": 18314,
				"start": 1345.3328,
				"end": 1346.4529,
				"text": " the caching is not something you own,",
				"tokens": [
					51549,
					264,
					269,
					2834,
					307,
					406,
					746,
					291,
					1065,
					11,
					51605
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 383,
				"seek": 18314,
				"start": 1346.5328,
				"end": 1348.0928,
				"text": " unless you're actually running it on inference.",
				"tokens": [
					51609,
					5969,
					291,
					434,
					767,
					2614,
					309,
					322,
					38253,
					13,
					51687
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 384,
				"seek": 18314,
				"start": 1348.1929,
				"end": 1350.3528,
				"text": " If you do, then you can control how long the cache runs.",
				"tokens": [
					51692,
					759,
					291,
					360,
					11,
					550,
					291,
					393,
					1969,
					577,
					938,
					264,
					19459,
					6676,
					13,
					51800
				],
				"temperature": 0,
				"avg_logprob": -0.19034116,
				"compression_ratio": 1.7035831,
				"no_speech_prob": 1.5984045e-12
			},
			{
				"id": 385,
				"seek": 21184,
				"start": 1350.3528,
				"end": 1354.4728,
				"text": " the model base the model providers basically compute some of this work ahead of time",
				"tokens": [
					50365,
					264,
					2316,
					3096,
					264,
					2316,
					11330,
					1936,
					14722,
					512,
					295,
					341,
					589,
					2286,
					295,
					565,
					50571
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 386,
				"seek": 21184,
				"start": 1354.4728,
				"end": 1358.2528,
				"text": " and they just save it into like some data structure probably a redis cache or something",
				"tokens": [
					50571,
					293,
					436,
					445,
					3155,
					309,
					666,
					411,
					512,
					1412,
					3877,
					1391,
					257,
					2182,
					271,
					19459,
					420,
					746,
					50760
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 387,
				"seek": 21184,
				"start": 1358.2528,
				"end": 1362.4928,
				"text": " i have no idea what they use under the hood and they just say hey if we get the sequence of tokens",
				"tokens": [
					50760,
					741,
					362,
					572,
					1558,
					437,
					436,
					764,
					833,
					264,
					13376,
					293,
					436,
					445,
					584,
					4177,
					498,
					321,
					483,
					264,
					8310,
					295,
					22667,
					50972
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 388,
				"seek": 21184,
				"start": 1362.4928,
				"end": 1368.8728,
				"text": " again just pull this data out it's almost independent of your user message because",
				"tokens": [
					50972,
					797,
					445,
					2235,
					341,
					1412,
					484,
					309,
					311,
					1920,
					6695,
					295,
					428,
					4195,
					3636,
					570,
					51291
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 389,
				"seek": 21184,
				"start": 1368.8728,
				"end": 1373.4128,
				"text": " uh dexter says it's all the time the only thing that impacts the model is actually like the tokens",
				"tokens": [
					51291,
					2232,
					368,
					36671,
					1619,
					309,
					311,
					439,
					264,
					565,
					264,
					787,
					551,
					300,
					11606,
					264,
					2316,
					307,
					767,
					411,
					264,
					22667,
					51518
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 390,
				"seek": 21184,
				"start": 1373.4128,
				"end": 1379.2928,
				"text": " in tokens out so like the fact that it's a user doesn't really matter the model doesn't know that",
				"tokens": [
					51518,
					294,
					22667,
					484,
					370,
					411,
					264,
					1186,
					300,
					309,
					311,
					257,
					4195,
					1177,
					380,
					534,
					1871,
					264,
					2316,
					1177,
					380,
					458,
					300,
					51812
				],
				"temperature": 0,
				"avg_logprob": -0.092031516,
				"compression_ratio": 1.9333333,
				"no_speech_prob": 1.2351643e-12
			},
			{
				"id": 391,
				"seek": 24078,
				"start": 1379.2928,
				"end": 1381.6328,
				"text": " OpenAI, Anthropic don't need to know that as well.",
				"tokens": [
					50365,
					7238,
					48698,
					11,
					12727,
					39173,
					500,
					380,
					643,
					281,
					458,
					300,
					382,
					731,
					13,
					50482
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 392,
				"seek": 24078,
				"start": 1382.6929,
				"end": 1385.1128,
				"text": " And that's how they kind of manage this.",
				"tokens": [
					50535,
					400,
					300,
					311,
					577,
					436,
					733,
					295,
					3067,
					341,
					13,
					50656
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 393,
				"seek": 24078,
				"start": 1386.2928,
				"end": 1390.8328,
				"text": " I think in general, most of them will not preserve the same cache between, like, technically,",
				"tokens": [
					50715,
					286,
					519,
					294,
					2674,
					11,
					881,
					295,
					552,
					486,
					406,
					15665,
					264,
					912,
					19459,
					1296,
					11,
					411,
					11,
					12120,
					11,
					50942
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 394,
				"seek": 24078,
				"start": 1390.9928,
				"end": 1394.8129,
				"text": " if it's content addressable, then like the hash of the content will always be the hash",
				"tokens": [
					50950,
					498,
					309,
					311,
					2701,
					2985,
					712,
					11,
					550,
					411,
					264,
					22019,
					295,
					264,
					2701,
					486,
					1009,
					312,
					264,
					22019,
					51141
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 395,
				"seek": 24078,
				"start": 1394.8129,
				"end": 1395.3129,
				"text": " of the content.",
				"tokens": [
					51141,
					295,
					264,
					2701,
					13,
					51166
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 396,
				"seek": 24078,
				"start": 1395.4329,
				"end": 1399.9928,
				"text": " So you're not leaking, you know, pre-computed attention between users.",
				"tokens": [
					51172,
					407,
					291,
					434,
					406,
					32856,
					11,
					291,
					458,
					11,
					659,
					12,
					1112,
					2582,
					292,
					3202,
					1296,
					5022,
					13,
					51400
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 397,
				"seek": 24078,
				"start": 1399.9928,
				"end": 1403.8928,
				"text": " But like, I think technically people still do segment it out where it's like, hey, you're",
				"tokens": [
					51400,
					583,
					411,
					11,
					286,
					519,
					12120,
					561,
					920,
					360,
					9469,
					309,
					484,
					689,
					309,
					311,
					411,
					11,
					4177,
					11,
					291,
					434,
					51595
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 398,
				"seek": 24078,
				"start": 1403.8928,
				"end": 1405.7528,
				"text": " never going to use the cache from another user.",
				"tokens": [
					51595,
					1128,
					516,
					281,
					764,
					264,
					19459,
					490,
					1071,
					4195,
					13,
					51688
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 399,
				"seek": 24078,
				"start": 1406.3728,
				"end": 1408.1328,
				"text": " Yeah, I assume that they did it.",
				"tokens": [
					51719,
					865,
					11,
					286,
					6552,
					300,
					436,
					630,
					309,
					13,
					51807
				],
				"temperature": 0,
				"avg_logprob": -0.17015116,
				"compression_ratio": 1.7549669,
				"no_speech_prob": 1.8112702e-12
			},
			{
				"id": 400,
				"seek": 27078,
				"start": 1409.2928,
				"end": 1414.8728,
				"text": " I assume that they did it because of, oh, Eugene.",
				"tokens": [
					50365,
					286,
					6552,
					300,
					436,
					630,
					309,
					570,
					295,
					11,
					1954,
					11,
					37059,
					13,
					50644
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 401,
				"seek": 27078,
				"start": 1415.3728,
				"end": 1416.6128,
				"text": " Yes, that is also true.",
				"tokens": [
					50669,
					1079,
					11,
					300,
					307,
					611,
					2074,
					13,
					50731
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 402,
				"seek": 27078,
				"start": 1416.6729,
				"end": 1418.9928,
				"text": " The model also has its own cache that computes stuff",
				"tokens": [
					50734,
					440,
					2316,
					611,
					575,
					1080,
					1065,
					19459,
					300,
					715,
					1819,
					1507,
					50850
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 403,
				"seek": 27078,
				"start": 1418.9928,
				"end": 1419.9128,
				"text": " because of the architecture.",
				"tokens": [
					50850,
					570,
					295,
					264,
					9482,
					13,
					50896
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 404,
				"seek": 27078,
				"start": 1420.5728,
				"end": 1422.8528,
				"text": " I'm assuming the caching the model providers are doing",
				"tokens": [
					50929,
					286,
					478,
					11926,
					264,
					269,
					2834,
					264,
					2316,
					11330,
					366,
					884,
					51043
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 405,
				"seek": 27078,
				"start": 1422.8528,
				"end": 1423.6328,
				"text": " are slightly different.",
				"tokens": [
					51043,
					366,
					4748,
					819,
					13,
					51082
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 406,
				"seek": 27078,
				"start": 1423.7129,
				"end": 1425.3728,
				"text": " I think that also helps quite a lot.",
				"tokens": [
					51086,
					286,
					519,
					300,
					611,
					3665,
					1596,
					257,
					688,
					13,
					51169
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 407,
				"seek": 27078,
				"start": 1425.9329,
				"end": 1427.8528,
				"text": " The actual inference time is also faster",
				"tokens": [
					51197,
					440,
					3539,
					38253,
					565,
					307,
					611,
					4663,
					51293
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 408,
				"seek": 27078,
				"start": 1427.8528,
				"end": 1429.0328,
				"text": " if you have repeated tokens.",
				"tokens": [
					51293,
					498,
					291,
					362,
					10477,
					22667,
					13,
					51352
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 409,
				"seek": 27078,
				"start": 1430.1128,
				"end": 1431.5328,
				"text": " Maybe that is what the paper was talking about,",
				"tokens": [
					51406,
					2704,
					300,
					307,
					437,
					264,
					3035,
					390,
					1417,
					466,
					11,
					51477
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 410,
				"seek": 27078,
				"start": 1431.5728,
				"end": 1432.5529,
				"text": " and I misinterpreted that.",
				"tokens": [
					51479,
					293,
					286,
					3346,
					41935,
					292,
					300,
					13,
					51528
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 411,
				"seek": 27078,
				"start": 1434.9928,
				"end": 1437.5529,
				"text": " But really quickly, I suspect the model providers",
				"tokens": [
					51650,
					583,
					534,
					2661,
					11,
					286,
					9091,
					264,
					2316,
					11330,
					51778
				],
				"temperature": 0,
				"avg_logprob": -0.17341949,
				"compression_ratio": 1.7132353,
				"no_speech_prob": 1.1738971e-12
			},
			{
				"id": 412,
				"seek": 29904,
				"start": 1437.5529,
				"end": 1440.5328,
				"text": " don't do the caching storage on a cross-user basis",
				"tokens": [
					50365,
					500,
					380,
					360,
					264,
					269,
					2834,
					6725,
					322,
					257,
					3278,
					12,
					18088,
					5143,
					50514
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 413,
				"seek": 29904,
				"start": 1440.5328,
				"end": 1441.9728,
				"text": " because they need to have TTLs.",
				"tokens": [
					50514,
					570,
					436,
					643,
					281,
					362,
					32576,
					43,
					82,
					13,
					50586
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 414,
				"seek": 29904,
				"start": 1442.4928,
				"end": 1445.0128,
				"text": " And those TTLs are really based on your personal usage,",
				"tokens": [
					50612,
					400,
					729,
					32576,
					43,
					82,
					366,
					534,
					2361,
					322,
					428,
					2973,
					14924,
					11,
					50738
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 415,
				"seek": 29904,
				"start": 1445.1328,
				"end": 1446.0128,
				"text": " not on anyone else's.",
				"tokens": [
					50744,
					406,
					322,
					2878,
					1646,
					311,
					13,
					50788
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 416,
				"seek": 29904,
				"start": 1447.4529,
				"end": 1450.5728,
				"text": " Raycast is a fantastic way to test this out quickly and locally.",
				"tokens": [
					50860,
					10883,
					3734,
					307,
					257,
					5456,
					636,
					281,
					1500,
					341,
					484,
					2661,
					293,
					16143,
					13,
					51016
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 417,
				"seek": 29904,
				"start": 1450.7928,
				"end": 1451.6929,
				"text": " Can you introduce...",
				"tokens": [
					51027,
					1664,
					291,
					5366,
					485,
					51072
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 418,
				"seek": 29904,
				"start": 1451.6929,
				"end": 1452.8928,
				"text": " You can introduce dynamic variables.",
				"tokens": [
					51072,
					509,
					393,
					5366,
					8546,
					9102,
					13,
					51132
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 419,
				"seek": 29904,
				"start": 1453.0928,
				"end": 1455.0128,
				"text": " Is there something else you would use for a similar purpose?",
				"tokens": [
					51142,
					1119,
					456,
					746,
					1646,
					291,
					576,
					764,
					337,
					257,
					2531,
					4334,
					30,
					51238
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 420,
				"seek": 29904,
				"start": 1455.1729,
				"end": 1456.9728,
				"text": " I don't know about something myself, Jens.",
				"tokens": [
					51246,
					286,
					500,
					380,
					458,
					466,
					746,
					2059,
					11,
					508,
					694,
					13,
					51336
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 421,
				"seek": 29904,
				"start": 1459.2328,
				"end": 1462.5728,
				"text": " I was just thinking about like a quick snippet copy-paste tool.",
				"tokens": [
					51449,
					286,
					390,
					445,
					1953,
					466,
					411,
					257,
					1702,
					35623,
					302,
					5055,
					12,
					79,
					9079,
					2290,
					13,
					51616
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 422,
				"seek": 29904,
				"start": 1462.7328,
				"end": 1464.5529,
				"text": " I don't know if you store prompts in anything else",
				"tokens": [
					51624,
					286,
					500,
					380,
					458,
					498,
					291,
					3531,
					41095,
					294,
					1340,
					1646,
					51715
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 423,
				"seek": 29904,
				"start": 1464.5529,
				"end": 1467.1128,
				"text": " or if you have a local thing that you guys use.",
				"tokens": [
					51715,
					420,
					498,
					291,
					362,
					257,
					2654,
					551,
					300,
					291,
					1074,
					764,
					13,
					51843
				],
				"temperature": 0,
				"avg_logprob": -0.2026776,
				"compression_ratio": 1.7027863,
				"no_speech_prob": 1.3941573e-12
			},
			{
				"id": 424,
				"seek": 32860,
				"start": 1467.1128,
				"end": 1470.5128,
				"text": " I have my own other setup myself, including that.",
				"tokens": [
					50365,
					286,
					362,
					452,
					1065,
					661,
					8657,
					2059,
					11,
					3009,
					300,
					13,
					50535
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 425,
				"seek": 32860,
				"start": 1470.6929,
				"end": 1471.2728,
				"text": " So just curious.",
				"tokens": [
					50544,
					407,
					445,
					6369,
					13,
					50573
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 426,
				"seek": 32860,
				"start": 1473.3127,
				"end": 1474.7528,
				"text": " I don't have anything, Dex.",
				"tokens": [
					50675,
					286,
					500,
					380,
					362,
					1340,
					11,
					1346,
					87,
					13,
					50747
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 427,
				"seek": 32860,
				"start": 1475.2928,
				"end": 1477.6528,
				"text": " Yeah, no, I mean, I think part of it is like,",
				"tokens": [
					50774,
					865,
					11,
					572,
					11,
					286,
					914,
					11,
					286,
					519,
					644,
					295,
					309,
					307,
					411,
					11,
					50892
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 428,
				"seek": 32860,
				"start": 1477.7328,
				"end": 1479.5529,
				"text": " this is a lot more about like building,",
				"tokens": [
					50896,
					341,
					307,
					257,
					688,
					544,
					466,
					411,
					2390,
					11,
					50987
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 429,
				"seek": 32860,
				"start": 1479.9329,
				"end": 1482.8528,
				"text": " like this is more applicable to like building software",
				"tokens": [
					51006,
					411,
					341,
					307,
					544,
					21142,
					281,
					411,
					2390,
					4722,
					51152
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 430,
				"seek": 32860,
				"start": 1482.8528,
				"end": 1486.2328,
				"text": " that interacts with models than for like how you prompt them.",
				"tokens": [
					51152,
					300,
					43582,
					365,
					5245,
					813,
					337,
					411,
					577,
					291,
					12391,
					552,
					13,
					51321
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 431,
				"seek": 32860,
				"start": 1486.8127,
				"end": 1488.5529,
				"text": " Yeah, yeah, no, I get that.",
				"tokens": [
					51350,
					865,
					11,
					1338,
					11,
					572,
					11,
					286,
					483,
					300,
					13,
					51437
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 432,
				"seek": 32860,
				"start": 1488.7928,
				"end": 1493.0529,
				"text": " But I guess as I iterate, I still test things locally",
				"tokens": [
					51449,
					583,
					286,
					2041,
					382,
					286,
					44497,
					11,
					286,
					920,
					1500,
					721,
					16143,
					51662
				],
				"temperature": 0,
				"avg_logprob": -0.17836177,
				"compression_ratio": 1.6478261,
				"no_speech_prob": 1.890783e-12
			},
			{
				"id": 433,
				"seek": 35454,
				"start": 1493.0529,
				"end": 1498.2328,
				"text": " before I would put them into what I'm actually going to use.",
				"tokens": [
					50365,
					949,
					286,
					576,
					829,
					552,
					666,
					437,
					286,
					478,
					767,
					516,
					281,
					764,
					13,
					50624
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 434,
				"seek": 35454,
				"start": 1498.3528,
				"end": 1501.0529,
				"text": " I don't know if that makes sense in the way I'm thinking of it,",
				"tokens": [
					50630,
					286,
					500,
					380,
					458,
					498,
					300,
					1669,
					2020,
					294,
					264,
					636,
					286,
					478,
					1953,
					295,
					309,
					11,
					50765
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 435,
				"seek": 35454,
				"start": 1501.2528,
				"end": 1505.1729,
				"text": " but if I'm iterating on a prompt essentially",
				"tokens": [
					50775,
					457,
					498,
					286,
					478,
					17138,
					990,
					322,
					257,
					12391,
					4476,
					50971
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 436,
				"seek": 35454,
				"start": 1505.1729,
				"end": 1508.5128,
				"text": " or a structure or workflow that I'm going to be implementing",
				"tokens": [
					50971,
					420,
					257,
					3877,
					420,
					20993,
					300,
					286,
					478,
					516,
					281,
					312,
					18114,
					51138
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 437,
				"seek": 35454,
				"start": 1508.5128,
				"end": 1511.1528,
				"text": " through a series of orchestrating agents,",
				"tokens": [
					51138,
					807,
					257,
					2638,
					295,
					14161,
					8754,
					12554,
					11,
					51270
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 438,
				"seek": 35454,
				"start": 1511.4529,
				"end": 1513.8728,
				"text": " I will iterate on each part of that separately",
				"tokens": [
					51285,
					286,
					486,
					44497,
					322,
					1184,
					644,
					295,
					300,
					14759,
					51406
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 439,
				"seek": 35454,
				"start": 1513.8728,
				"end": 1518.2528,
				"text": " through using some sort of way to iterate on the snippet basically.",
				"tokens": [
					51406,
					807,
					1228,
					512,
					1333,
					295,
					636,
					281,
					44497,
					322,
					264,
					35623,
					302,
					1936,
					13,
					51625
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 440,
				"seek": 35454,
				"start": 1519.3728,
				"end": 1520.9728,
				"text": " I mean, Vibeoff's got a lot of opinions",
				"tokens": [
					51681,
					286,
					914,
					11,
					6626,
					650,
					4506,
					311,
					658,
					257,
					688,
					295,
					11819,
					51761
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 441,
				"seek": 35454,
				"start": 1520.9728,
				"end": 1522.6929,
				"text": " on how to iterate on prompt snippets.",
				"tokens": [
					51761,
					322,
					577,
					281,
					44497,
					322,
					12391,
					35623,
					1385,
					13,
					51847
				],
				"temperature": 0,
				"avg_logprob": -0.13911521,
				"compression_ratio": 1.7613636,
				"no_speech_prob": 1.5133446e-12
			},
			{
				"id": 442,
				"seek": 38418,
				"start": 1522.6929,
				"end": 1526.5328,
				"text": " Yeah, so I guess that's where my mind's at right now. So sidetracked.",
				"tokens": [
					50365,
					865,
					11,
					370,
					286,
					2041,
					300,
					311,
					689,
					452,
					1575,
					311,
					412,
					558,
					586,
					13,
					407,
					20822,
					27965,
					25949,
					13,
					50557
				],
				"temperature": 0,
				"avg_logprob": -0.13796657,
				"compression_ratio": 1.631068,
				"no_speech_prob": 6.2841475e-13
			},
			{
				"id": 443,
				"seek": 38418,
				"start": 1527.0328,
				"end": 1532.3127,
				"text": " I would say like none of this, none of the stuff that we're talking about today really matters for prompt iteration speed.",
				"tokens": [
					50582,
					286,
					576,
					584,
					411,
					6022,
					295,
					341,
					11,
					6022,
					295,
					264,
					1507,
					300,
					321,
					434,
					1417,
					466,
					965,
					534,
					7001,
					337,
					12391,
					24784,
					3073,
					13,
					50846
				],
				"temperature": 0,
				"avg_logprob": -0.13796657,
				"compression_ratio": 1.631068,
				"no_speech_prob": 6.2841475e-13
			},
			{
				"id": 444,
				"seek": 38418,
				"start": 1532.8528,
				"end": 1540.0128,
				"text": " That really doesn't matter. This only matters for like you have a prompt, it's working, but you want to make that thing work faster and better.",
				"tokens": [
					50873,
					663,
					534,
					1177,
					380,
					1871,
					13,
					639,
					787,
					7001,
					337,
					411,
					291,
					362,
					257,
					12391,
					11,
					309,
					311,
					1364,
					11,
					457,
					291,
					528,
					281,
					652,
					300,
					551,
					589,
					4663,
					293,
					1101,
					13,
					51231
				],
				"temperature": 0,
				"avg_logprob": -0.13796657,
				"compression_ratio": 1.631068,
				"no_speech_prob": 6.2841475e-13
			}
		],
		"x_groq": {
			"id": "req_01k7hvnntqf30rprjv4z0bcs57"
		}
	},
	{
		"task": "transcribe",
		"language": "English",
		"duration": 1539.965374,
		"text": " How do you go do that? Cool. How do we know the task list is or isn't cached? Shouldn't this be managed as it changes? Yes, to some degree. But really, there's only one way to know that it's cached, which is if the model provider tells you that. If they don't actually give you that information, you can't possibly know. So it really depends on the inference provider. I mean, you can kind of proxy it because of latency, but not really definitively. So this is why me and Vaibov work well together, is he's really good at just talking through my random questions and finishing his point. So please keep doing that. Question though, like, are you going to show us today how to use an LLM client to actually look at the responses and see the, like, caching, like, the caching, like, headers that come back? Like, I think that would be super, super valuable to, like, untangle, like, the weather, family or whatever, untangle the response and see, like, hey, look, let's change the end of it and see how many cache tokens we got versus changing the beginning and see that it blows the cache. Yeah, that's impossible. Eugene had a really good point, actually, and I realized I actually had totally missed this, which is there's actually two types of caching that are going on here, which is one caching done by the actual model providers in terms of helping with encoder decoder blocks. And then the actual transformer architecture also allows you to do another type of cache in there. I think I can pull this up. Let's see if I can pull it up. It's a very famous image. But basically, let's see if I can find this. Basically, a lot of the actual nodes in here, a lot of the math can also be pre-computed along the way, at any point in the way. The part that I was talking about was actually this part, which is the encoder node of the transformer can actually be pre-computed based of the actual input tokens that are going in. And this whole layer can almost be cached because it's not really dependent on the output probabilities. And these are going to be able to take some advance of that. The second half of this that Eugene is talking about is also the KB. There's a KB cache in the actual model, like matrix multiplication areas. And those can also dramatically improve the amount of computation throughput you can get out of the actual transform architecture. That may be actually what the paper was talking about, Eugene. I may have missed that. Yeah, I think it is about the cache inside the model. Yes. So in that case, it's still the principle is still the same. Everything's the same except nothing's going into Redis, I think probably is the key detail. Yes. Well, no, that one's cutting down on the actual math that it has to go to. but the main difference the premise is still the same the number of continuity tokens you have dramatically changes the amount of caching that you're going to get anytime you go change the continuity of the tokens the more likely you are going to be to break the cache at any point and get a miss on the cache how do I how do I eval if my updated context handling will improve the quality I think we've had a couple talks on evals. It's actually very annoying to do evals and it's very frustrating, but I think the first step to do evals is just to define quality to some degree and what it means. Understand what is clearly good, what is clearly bad, what is mostly good, mostly bad. And then just like buy the eval in the beginning. And once you have a better understanding of that system, then you can go ahead and actually make these trade-offs. The only reason that likely can do this work or any engineering team can do this work is because they first have to go ahead and say, this is good and this is bad. that evals are useless and i'd say like most of the stuff we talk about about evals here is about quality of the outputs in terms of like accuracy host nations using the right context things like that um the quality improvements that we're getting here because like at the end of the day whether you cash it or not it's the same tokens in which means your chance of getting the right tokens out is probably about the same um so this is a really interesting kind of category of evals we probably haven't talked as much about, which is like, how do you evaluate the performance, not in terms of accuracy, but in terms of speed and cost? Yeah. I think Vijay has an interesting question. I guess Rajesh pointed out a different thing. If the model weights are the same, everything is new, how do you actually get caching? It's really hard to describe how the math on a GPU gets cached, but you can do caching on there. You should take my word for it. There's a beautiful video made by, If any of you are interested, this is probably one of the best videos I've ever seen. Hey, everyone. I'm by Bob. Sorry. I'll give you... I'm sorry. Every time I pull up the YouTube channel, I hear you introducing yourself. I'll post the video on here. There's like a 90-minute video that I watched that actually talked about how DeepSeek actually works under the hood and talks about the math behind it. I think it's actually this one. This video is... It's one of the best ones I've ever seen. It will describe it. It will probably teach you more about anything you could see. I wish I could make that. John, best video I've ever seen. Pulls up video of self. And I'll talk to you about how it's possible to get way better throughput than anyone else can on the same math that everyone else is doing. Context caching in our app thesis, the old code is open source for anyone. Okay, cool. There's just an example. If Duck has an example, then definitely share it, Duck, and we'd love to have people go be able to see it. Yeah, drop it in the chat. Vijay, we've got a question. We'll take that, and then we'll go to the next part of the system after that. So if I understood this correctly, for example, let's say our model has an input window of 256k tokens, and we're trying to, let's say, take advantage of that entire context window. So essentially what you're saying is whatever we input in that 256k context, as much as possible, that string should be unchanging or let's say static. It should be the same in next iteration so that you can get the next open quicker. And the end of that input context will probably be varying per call So I think the next step would be to understand how agents are iterating between different calls and see what is changing what is not changing in what we are sending the LLM and sort of restructure that entire input sequence. Is that the next step? Like, am I thinking about this correctly? I think maybe, just maybe I don't understand all the words you said perfectly. And it's probably because I woke up at 4. Okay, let me simplify it. So we are sending tokens to the LLM. Yes. The sequence of tokens, as much as it is unchanged, the faster we get a response. Yes. Therefore, our next step would obviously be to understand a couple of iterations of what we are sending the LLM, see what is changing, what is not changing, and try to restructure that. Correct? Yes, exactly. There are caveats, though, in terms of actual implementation details here that matter. If you own the model yourself and you're doing inference, it's a very different game than if you're actually using the existing inference providers. So, for example, Anthropic lets you actually control cache control with this cache control block. And where you put it dramatically matters. Because if you put it at the very beginning of a system, if you put it in your system message, it will dramatically change the throughput that you are personally able to get. if you have a really long chat thread and you just put it at the beginning of a chat thread and you don't break apart your chat threads manually you're going to get worse hits there's just nothing you can do about that because your chat thread is basically you put you put a cache control block on the first user message and then you in the same chat control block you put another user message it's just a cache miss it's a guaranteed cache miss you can't actually do anything about that so every time you're not reconstructing the prompt in the same way as close to possible you are basically getting a miss gemini i think i think the way open ai does caching is opaque so what that means is i i think they give you almost no information on how caching works but i think they actually what they do but it's like less control but they kind of just try to do it for you this is a lot of them just like do a prefix and try to guess how to do it i didn't But now they added a prompt cache key. So it's like prompt cache. And the reason that they do this, I'm certain... Replace the user. I don't know what this is. But I'm pretty certain that the reason they do this is people just want it more control. Because if you're able to have access to this and you can control this, you can just get better hits all the time. But it is important to go read this stuff. And the only way you can actually know if you're hitting this is just by looking at the usage. there's no other way to really know um along unless you own the inference and most people don't own inference as far as i know um and most people probably shouldn't own inference as well gemini is probably the most flexible thing i have seen for how they cache and eugene is right on that which is if you actually go look into how to get caches like you do it you do some crap to go write this code but the trade-off of doing this crap is you do get to go get complete control over what it's doing and you get to go tell it this and that helps you as a developer but it also hurts you because you have to go build this stuff yourself you also have to manage the detail on everything automatically so there's always a trade-off in how much engineering effort you want to put in versus what you want to do manually what you want to do automatically and generally if you do something manually you will always get better throughput if you know what you're doing than someone that does it automatically. But someone that does it automatically will always get better than someone that doesn't know what they're doing manually. Because if you blow the cache out of time, it'll be worse. Okay, so if you know what you're doing, use the cache keys. If you don't, hope for the prefix things. But don't try to mess with the cache keys if you're not willing to go learn how these systems work. Exactly. And lastly, if you don't have the time to go learn how they work. It's not a matter of like you don't want to. Sometimes you just don't have the time. We're all balancing a lot of things at once. But what it comes down to is if your cache isn't working, you now know why it isn't working. It's because your prefix key is not continuous and it's breaking somehow when whatever system you're using can break it. You're getting a cache miss for that reason. I think the next thing I want to talk about is actually talking about this one. We talked about recitation a little bit. So like recite your systems. Think about what makes sense in your workflows to recite this. And it can do all sorts of different things. So for example, in the example that they gave over here, what they're showing their visual is, hey, instead of producing context one and doing this, what I will say is before even producing action four, I will inject an objective into the statement. And now I can go and produce objective four. So there's different ways to go repeat yourself in the prompt. There's one is just add it to the very end. One is take your existing prompt and go inject it somewhere else in my history. and yes this is kind of contradictory to what we just talked about about breaking the cache key but you're doing this deliberately because you're trying to get accuracy so you're like okay cool I will lose the cache on this part of my prompt I will still preserve the cache on everything up until here but I will do that in favor of getting slightly better accuracy and helping the model understand what's going on and the premise here is that's objective it may be obvious to the model at this point that action three was best but when it's making action four the model might forget that because of this objective, action three was selected here. So I'm repeating that here's objectives and this is why you picked action three. Now pick action four. I can also just say- Zoom. Go ahead. Has anyone ever implemented this like kind of deterministic injection of a to-do list into the context window as it's going? I would be surprised if Cloud Code doesn't do something like this. That's what I'm messing with that idea right now is what I- Cool. Yeah, doing. Cool. So this one I think is really straightforward and easy to iterate on. So if you're finding that you have really, really long tool call sequences and they're diverting from or drifting from the main action, just like add some repetition there. And you might just get free throughput without having to do much work. And I think that's the cool thing about this trick. There's almost no effort. I want to talk about this. This is something that we've talked about a lot, which is a lot of times when in an agent loop, in an agent loop it try and go do many things and then it go ahead and go correct things along the way And in this case I think what they found is they found in their scenario keeping, if they're running two things in parallel, getting all the predictions, actually having the incorrect stuff helps them. I would just go experiment and try this. What we have found is having the incorrect stuff hurts in some scenarios, but it's possible in their use case because they're running like on average, at least 50 tool calls, that perhaps having incorrect sequences helps the model correct itself and not repeat the same behavior in the case of very repetitive actions. Eugene, you've got- It's funny because this is contrary to advice we give a lot, which is like be smart about sometimes pulling those out because it becomes very noisy. Yeah, Eugene, you've got a question. Yeah, I got a question. Yeah, I think this one on the incorrect observations is quite intuitive to me. One thing that's also not so intuitive to me is the tool call. Do you just store, do you just return to the main model, the main agent, the output of the tool call, or do you also keep the tool call itself? So I have different opinions on this. And I think the thing that they're saying here is, so I found that when I was reading this, I think the reason that they're seeing this, it's that often having a stat trace helps even a developer debug the problem. And I think what they're, I think the reason, because this feels very wrong to me as well, but I think what's happening is Manus has a very broad scope of what it's trying to do so many times a model will divert and not do the right thing and in that scenario assuming that it has to do that thing again or it's somewhat related to the final goal and it might have to repeat that task again it's more likely the model will get it right the first time around if it can kind of look at its stack trace and say what did it take to get to the final correct answer that said I'm not actually yet convinced that the best way to represent the tool calls are actually the tool calling systems that people have. I think there's a lot more... Really? You think there's a better way to represent tool calling? Well, I don't know if that's sarcastic or not. But like even... Sorry. I'm teeing you up, man. That I thought was really interesting. Wrong Discord. Sorry. I'm usually on a desktop monitor when I'm doing this and it's a little bit more showcase. I thought this was a fantastic thread here that really helped me understand kind of some of the implications of some of this stuff. So Prashant, who is one of the people that's been playing on both DSPy and BAML, learned something really interesting. What he did was he changed how DSPy represents their prompt format to be away from JSON schema and go more into like the BAML way of doing it. And what was really interesting is just a simple way of changing how you represent the tool call for every single model by default was just better than JSON schema. But when you add the two together, it got even better. And I think that goes into what Eugene is asking is like, what's the best way to represent a tool call? To be completely honest, I don't fucking know. I have no opinions on this, but I think in general, simplicity is best. So if you have a thing where you're generating a bunch of like Luma URLs, don't put uuids into the uh don't put uuids into the uh into the prompt it's just going to hurt even if your tool call generates one like replace that out we have a thing in our chat bot uh that we made recently and i i thought it was kind of funny that and we talk about this all the time but like if you ask it like um how do i do this thing like how do i build a super complex agent what I found really funny in here is one of our uh someone on my team built this and I've tried my best not to influence anyone on the team in any meaningful way I want them to discover their own agent assigned decisions which is how do I get help how do I get help sorry which was when they actually put our discord link in here they originally did this but this thing was inconsistent and they had to reprompt it to actually go ahead and actually change the boundaryml.com slash discord. In this case, this one was fine because it actually pulled this page as context info and fed that into there. So it was able to do a regurgitation. But when you left on the system prompt as the main way of getting help, it didn't work. So they actually changed. Sorry, this is the, you said the problem is because this is some weird long string token that's hard for the model to remember. And in this case, what they're doing, there's actually pruning it out and injecting this page in as the only contact information page. so it's really easy for it to regurgitate. It's a pretty beefy model. But what they found is we actually made a link called boundarymail.com slash discord and putting this in the system prompt works almost all the time. All right, and it's the same context. So it's like just thinking about how to make these systems work in terms of tool calls is I don't really know what the best answer is, but the better you can do to get towards simplicity, in general, the way more throughput you will get. Yeah, exactly. Random hashes are just not good. Like the model is just never going to be good at that. And I think it's the same thing here. Like I think in their scenario, it kind of works, but I would just eval this for your own use case. I suspect if you have a simple task and it did it wrong twice, you'll probably get better by just giving it the right output. If you're in a smaller model, you probably don't want to give a smaller model the entire stack trace because a smaller model is just going to get lost in the sauce. If you have a bigger model, it's more forgiving in general for all of this information. So trying to figure out exactly what you have to do both a parameterization of the problem space you're working in and the model you're using. And it's like a fine art of figuring out what that is exactly. But the fact that this works for them, like to me, this slightly changed my perspective of being completely rigid on only using observation 2b in the final prompt. Now what I do... Go ahead. Sorry, go ahead. No, finish your thought. Well, what I was going to say is next time I have a really hard problem and I use GPT-5, for example, I'll probably toss an entire stack trace and just see how well it does. because it's possible and models have changed last time i built this opinion was based off of like the gpt-40 models and perhaps the models are different now and it's good for me to be aware of that and like be open to changing my own perspectives interesting i mean so this is interesting it doesn't say so it's like air recovery but i'm not it's not clear saying like um like this picture says hey like use the observation to get it to get it correctly right or use the thing to get it to get correctly the first time but it doesn seem to have an opinion on like okay if you have four errors and then you get it right that you should clear out those four errors before you proceed yeah yeah it how do i think the what they saying is typically many people never store this information i think what you're making a case for is hey consider storing the information sometimes it might help yeah this is uh factor nine of 12 factor agents is like be thoughtful about it but yeah tell the model what it did wrong because it will probably fix it. And the models are getting way better. So like the stack traces are likely more likely to help, especially as the stuff gets bigger and bigger in terms of context windows and how well models perform over long form context. That said, same thing we talked about here. Proximity to the cache matters. Proximity to the observation, or where did it go? Proximity to the observations matter. I don't know where it went, that image, the repetition one. Proximity to the observation matter so like if you make it too big you'll probably get loss of performance at some point for your task um let's talk about few shot prompting i say this all the time don't do few shot prompting the reason few shot prompting is bad is for the same reason that we say all the time which is you're most people don't do few shot prompting correctly you're almost most likely going to bias the model away towards your few shot example rather than helping understand what it's actually meant to do well and literally building an agent is few shot prompting like everything in your past context window becomes like influences the next step and so if the model starts like starts a task and launches a sub-agent for the rest of that conversation the model will be more likely to launch a sub-agent and so like you have to really like understand and this is why we always say like use clear rather than re-steer if the model like starts doing something weird in your agentic chat whatever system it is obviously i care a lot about coding agents but like this is true for everything is like the model starts going about it in the wrong way you are much better off starting with a fresh context and adding two sentences of steering or two words of steering by the way don't do this then you are to be like stop do it a different way and then because then your prompt is like your context window says system prompt user message model tried this user re-steered it and so you're more likely for the model to like expect that the conversation continues who says model tries a bad thing, user re-steers. Model tries a bad, like you're telling the model it's okay to make a mistake and then get corrected when what you really want is the model to think, okay, cool, I made the right decision. I made the right decision. I made the right decision. That's how you craft like really good agentic context as you go. Also contextually, just remember undo is a pretty complex action. The fact that we can do command Z is nice on a computer, but it's actually really hard to undo sequences of actions. And that's why many apps, most SaaS apps you use don't build command Z. It's really freaking hard to command Z a lot of actions. And the model is just going to have a hard time too. Photoshop, for example, will just eventually forget that you're past a certain state. It'll just say you can't undo past this point. Same thing. In terms of few shot examples, there are ways to do it correctly. It's just that you have to be really thoughtful about it. In this case, if I have a class object where I want product to mean only people that actually actively write code, then someone that's listed as director of an eng team, I can just write a small thing in here that says, because they don't code themselves category product. And now the model can kind of, this is perfectly evident that this is an example to the model. I'm not trying to trick it. I'm not trying to do anything. It's just like this one is an example very clearly. I use dot, dot, dot, simplify that. I'm showing the schema without showing the full schema. The model cannot possibly confuse Vaibhav Gupta as that person because, I mean, the really, really dumb model can, but modern models are just not going to have that problem. So if you're going to do few shot prompting, be clever about it. Think about what you're really trying to have the few shot example explain to the model and only put the minimum number of tokens you need to explain that concept itself. And not just number of tokens, but like meaning of tokens, right? Like that dot dot token means placeholder to the model. Exactly. Yeah. And to almost anyone else reading it, it's the same thing. Same with dynamic few shot prompting. Sometimes you might want to put few shots that are actually similar to your example. Sometimes you might want to put few shots because they're completely the opposite of your example. And sometimes you might want to pick a few shots that are unrelated to your example. Let's say you're in a healthcare setting and you're processing doctor-patient conversations. You probably don't want to talk about like, I don't know, like leg fractures if you're talking very commonly, if the person currently has a leg fracture. That's just going to mess up the accuracy of the system, no matter what you do. You probably don't want to put the person's name as the same name as your patient in your system. You probably don't want the doctor's name to be the same. You probably don't want the hospital name to be the same. You probably don't want to confuse it with dates in case of continuity of any kind. So finding this sort of stuff out and picking the right FUTHOT example can be good, but as you're probably figuring this out, it's a lot of work to be FUTHOT prompting correctly. So most people are better off not doing it rather than doing it. Let's go on to the next topic. Unless there's more questions. Feel free to keep typing questions in the chat if you have any. We might have to do a follow-up session and do the coding side because I don't know if we're going to have time. Yes. I really want to see an example of how to look at the responses. I'll check on the code. Okay, cool. I did want to talk about all these topics. I thought they were really, really interesting. I think the last one that I thought is worth... Actually, we'll talk about this one first because it's really easy. What I read is very much about context compression, at least the way I interpreted this one, which wasn't about use the faucets in the context. That's one way to do it. But what I was hearing was, instead of putting in the entire chunk all the time at all points, see if you can break down the system and only put in the least amount of context that's relevant. So for example, if every uh and like i think that's the whole point everything is meant to be restorable the idea is that if you're having a whole website here instead of actually putting the website over here just put the url over here and replace the original observation action two with just the url and say from this url i got this action from this url i got this action from this url i got this action and then oh i see so the model the model may even load the thing into context and then you choose the next action and then you stitch the actual content back and pull",
		"segments": [
			{
				"id": 0,
				"seek": 0,
				"start": 0,
				"end": 0.78,
				"text": " How do you go do that?",
				"tokens": [
					50365,
					1012,
					360,
					291,
					352,
					360,
					300,
					30,
					50404
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 1,
				"seek": 0,
				"start": 2.26,
				"end": 2.46,
				"text": " Cool.",
				"tokens": [
					50478,
					8561,
					13,
					50488
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 2,
				"seek": 0,
				"start": 3.56,
				"end": 5.96,
				"text": " How do we know the task list is or isn't cached?",
				"tokens": [
					50543,
					1012,
					360,
					321,
					458,
					264,
					5633,
					1329,
					307,
					420,
					1943,
					380,
					269,
					15095,
					30,
					50663
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 3,
				"seek": 0,
				"start": 6.92,
				"end": 8.78,
				"text": " Shouldn't this be managed as it changes?",
				"tokens": [
					50711,
					34170,
					380,
					341,
					312,
					6453,
					382,
					309,
					2962,
					30,
					50804
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 4,
				"seek": 0,
				"start": 9.56,
				"end": 11.36,
				"text": " Yes, to some degree.",
				"tokens": [
					50843,
					1079,
					11,
					281,
					512,
					4314,
					13,
					50933
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 5,
				"seek": 0,
				"start": 11.7,
				"end": 13.98,
				"text": " But really, there's only one way to know that it's cached,",
				"tokens": [
					50950,
					583,
					534,
					11,
					456,
					311,
					787,
					472,
					636,
					281,
					458,
					300,
					309,
					311,
					269,
					15095,
					11,
					51064
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 6,
				"seek": 0,
				"start": 14.06,
				"end": 15.44,
				"text": " which is if the model provider tells you that.",
				"tokens": [
					51068,
					597,
					307,
					498,
					264,
					2316,
					12398,
					5112,
					291,
					300,
					13,
					51137
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 7,
				"seek": 0,
				"start": 15.5,
				"end": 16.7,
				"text": " If they don't actually give you that information,",
				"tokens": [
					51140,
					759,
					436,
					500,
					380,
					767,
					976,
					291,
					300,
					1589,
					11,
					51200
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 8,
				"seek": 0,
				"start": 16.82,
				"end": 18.02,
				"text": " you can't possibly know.",
				"tokens": [
					51206,
					291,
					393,
					380,
					6264,
					458,
					13,
					51266
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 9,
				"seek": 0,
				"start": 18.96,
				"end": 20.54,
				"text": " So it really depends on the inference provider.",
				"tokens": [
					51313,
					407,
					309,
					534,
					5946,
					322,
					264,
					38253,
					12398,
					13,
					51392
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 10,
				"seek": 0,
				"start": 21.44,
				"end": 23.94,
				"text": " I mean, you can kind of proxy it because of latency,",
				"tokens": [
					51437,
					286,
					914,
					11,
					291,
					393,
					733,
					295,
					29690,
					309,
					570,
					295,
					27043,
					11,
					51562
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 11,
				"seek": 0,
				"start": 24.32,
				"end": 26.3,
				"text": " but not really definitively.",
				"tokens": [
					51581,
					457,
					406,
					534,
					28152,
					356,
					13,
					51680
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 12,
				"seek": 0,
				"start": 27.2,
				"end": 29.38,
				"text": " So this is why me and Vaibov work well together,",
				"tokens": [
					51725,
					407,
					341,
					307,
					983,
					385,
					293,
					16822,
					897,
					5179,
					589,
					731,
					1214,
					11,
					51834
				],
				"temperature": 0,
				"avg_logprob": -0.22305457,
				"compression_ratio": 1.6523179,
				"no_speech_prob": 2.2277937e-12
			},
			{
				"id": 13,
				"seek": 2938,
				"start": 29.38,
				"end": 33.58,
				"text": " is he's really good at just talking through my random questions and finishing his point.",
				"tokens": [
					50365,
					307,
					415,
					311,
					534,
					665,
					412,
					445,
					1417,
					807,
					452,
					4974,
					1651,
					293,
					12693,
					702,
					935,
					13,
					50575
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 14,
				"seek": 2938,
				"start": 33.82,
				"end": 34.82,
				"text": " So please keep doing that.",
				"tokens": [
					50587,
					407,
					1767,
					1066,
					884,
					300,
					13,
					50637
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 15,
				"seek": 2938,
				"start": 35.86,
				"end": 42.28,
				"text": " Question though, like, are you going to show us today how to use an LLM client to actually",
				"tokens": [
					50689,
					14464,
					1673,
					11,
					411,
					11,
					366,
					291,
					516,
					281,
					855,
					505,
					965,
					577,
					281,
					764,
					364,
					441,
					43,
					44,
					6423,
					281,
					767,
					51010
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 16,
				"seek": 2938,
				"start": 42.28,
				"end": 47.68,
				"text": " look at the responses and see the, like, caching, like, the caching, like, headers that come",
				"tokens": [
					51010,
					574,
					412,
					264,
					13019,
					293,
					536,
					264,
					11,
					411,
					11,
					269,
					2834,
					11,
					411,
					11,
					264,
					269,
					2834,
					11,
					411,
					11,
					45101,
					300,
					808,
					51280
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 17,
				"seek": 2938,
				"start": 47.68,
				"end": 47.96,
				"text": " back?",
				"tokens": [
					51280,
					646,
					30,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 18,
				"seek": 2938,
				"start": 48.1,
				"end": 52.84,
				"text": " Like, I think that would be super, super valuable to, like, untangle, like, the weather, family",
				"tokens": [
					51301,
					1743,
					11,
					286,
					519,
					300,
					576,
					312,
					1687,
					11,
					1687,
					8263,
					281,
					11,
					411,
					11,
					1701,
					7846,
					11,
					411,
					11,
					264,
					5503,
					11,
					1605,
					51538
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 19,
				"seek": 2938,
				"start": 52.84,
				"end": 57,
				"text": " or whatever, untangle the response and see, like, hey, look, let's change the end of it",
				"tokens": [
					51538,
					420,
					2035,
					11,
					1701,
					7846,
					264,
					4134,
					293,
					536,
					11,
					411,
					11,
					4177,
					11,
					574,
					11,
					718,
					311,
					1319,
					264,
					917,
					295,
					309,
					51746
				],
				"temperature": 0,
				"avg_logprob": -0.121250905,
				"compression_ratio": 1.7977941,
				"no_speech_prob": 2.3347678e-12
			},
			{
				"id": 20,
				"seek": 5700,
				"start": 57,
				"end": 58.94,
				"text": " and see how many cache tokens we got",
				"tokens": [
					50365,
					293,
					536,
					577,
					867,
					19459,
					22667,
					321,
					658,
					50462
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 21,
				"seek": 5700,
				"start": 58.94,
				"end": 60.08,
				"text": " versus changing the beginning",
				"tokens": [
					50462,
					5717,
					4473,
					264,
					2863,
					50519
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 22,
				"seek": 5700,
				"start": 60.08,
				"end": 61.38,
				"text": " and see that it blows the cache.",
				"tokens": [
					50519,
					293,
					536,
					300,
					309,
					18458,
					264,
					19459,
					13,
					50584
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 23,
				"seek": 5700,
				"start": 62.32,
				"end": 63.86,
				"text": " Yeah, that's impossible.",
				"tokens": [
					50631,
					865,
					11,
					300,
					311,
					6243,
					13,
					50708
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 24,
				"seek": 5700,
				"start": 64.68,
				"end": 66.46,
				"text": " Eugene had a really good point, actually,",
				"tokens": [
					50749,
					37059,
					632,
					257,
					534,
					665,
					935,
					11,
					767,
					11,
					50838
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 25,
				"seek": 5700,
				"start": 66.56,
				"end": 68.76,
				"text": " and I realized I actually had totally missed this,",
				"tokens": [
					50843,
					293,
					286,
					5334,
					286,
					767,
					632,
					3879,
					6721,
					341,
					11,
					50953
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 26,
				"seek": 5700,
				"start": 68.84,
				"end": 71.3,
				"text": " which is there's actually two types of caching",
				"tokens": [
					50957,
					597,
					307,
					456,
					311,
					767,
					732,
					3467,
					295,
					269,
					2834,
					51080
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 27,
				"seek": 5700,
				"start": 71.3,
				"end": 72.14,
				"text": " that are going on here,",
				"tokens": [
					51080,
					300,
					366,
					516,
					322,
					510,
					11,
					51122
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 28,
				"seek": 5700,
				"start": 72.46,
				"end": 76.58,
				"text": " which is one caching done by the actual model providers",
				"tokens": [
					51138,
					597,
					307,
					472,
					269,
					2834,
					1096,
					538,
					264,
					3539,
					2316,
					11330,
					51344
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 29,
				"seek": 5700,
				"start": 76.58,
				"end": 78.92,
				"text": " in terms of helping with encoder decoder blocks.",
				"tokens": [
					51344,
					294,
					2115,
					295,
					4315,
					365,
					2058,
					19866,
					979,
					19866,
					8474,
					13,
					51461
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 30,
				"seek": 5700,
				"start": 79.08,
				"end": 81.16,
				"text": " And then the actual transformer architecture",
				"tokens": [
					51469,
					400,
					550,
					264,
					3539,
					31782,
					9482,
					51573
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 31,
				"seek": 5700,
				"start": 81.16,
				"end": 83.92,
				"text": " also allows you to do another type of cache in there.",
				"tokens": [
					51573,
					611,
					4045,
					291,
					281,
					360,
					1071,
					2010,
					295,
					19459,
					294,
					456,
					13,
					51711
				],
				"temperature": 0,
				"avg_logprob": -0.17740454,
				"compression_ratio": 1.7826087,
				"no_speech_prob": 2.0284325e-12
			},
			{
				"id": 32,
				"seek": 8392,
				"start": 83.92,
				"end": 86.92,
				"text": " I think I can pull this up.",
				"tokens": [
					50365,
					286,
					519,
					286,
					393,
					2235,
					341,
					493,
					13,
					50515
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 33,
				"seek": 8392,
				"start": 90.92,
				"end": 92.92,
				"text": " Let's see if I can pull it up.",
				"tokens": [
					50715,
					961,
					311,
					536,
					498,
					286,
					393,
					2235,
					309,
					493,
					13,
					50815
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 34,
				"seek": 8392,
				"start": 92.92,
				"end": 94.92,
				"text": " It's a very famous image.",
				"tokens": [
					50815,
					467,
					311,
					257,
					588,
					4618,
					3256,
					13,
					50915
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 35,
				"seek": 8392,
				"start": 97.92,
				"end": 101.92,
				"text": " But basically, let's see if I can find this.",
				"tokens": [
					51065,
					583,
					1936,
					11,
					718,
					311,
					536,
					498,
					286,
					393,
					915,
					341,
					13,
					51265
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 36,
				"seek": 8392,
				"start": 104.92,
				"end": 106.92,
				"text": " Basically, a lot of the actual nodes in here,",
				"tokens": [
					51415,
					8537,
					11,
					257,
					688,
					295,
					264,
					3539,
					13891,
					294,
					510,
					11,
					51515
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 37,
				"seek": 8392,
				"start": 106.92,
				"end": 109.92,
				"text": " a lot of the math can also be pre-computed along the way,",
				"tokens": [
					51515,
					257,
					688,
					295,
					264,
					5221,
					393,
					611,
					312,
					659,
					12,
					1112,
					2582,
					292,
					2051,
					264,
					636,
					11,
					51665
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 38,
				"seek": 8392,
				"start": 109.92,
				"end": 111.92,
				"text": " at any point in the way.",
				"tokens": [
					51665,
					412,
					604,
					935,
					294,
					264,
					636,
					13,
					51765
				],
				"temperature": 0,
				"avg_logprob": -0.18966573,
				"compression_ratio": 1.6645161,
				"no_speech_prob": 7.062129e-13
			},
			{
				"id": 39,
				"seek": 11192,
				"start": 111.92,
				"end": 114.26,
				"text": " The part that I was talking about was actually this part,",
				"tokens": [
					50365,
					440,
					644,
					300,
					286,
					390,
					1417,
					466,
					390,
					767,
					341,
					644,
					11,
					50482
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 40,
				"seek": 11192,
				"start": 114.26,
				"end": 117.34,
				"text": " which is the encoder node of the transformer",
				"tokens": [
					50482,
					597,
					307,
					264,
					2058,
					19866,
					9984,
					295,
					264,
					31782,
					50636
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 41,
				"seek": 11192,
				"start": 117.34,
				"end": 118.62,
				"text": " can actually be pre-computed based",
				"tokens": [
					50636,
					393,
					767,
					312,
					659,
					12,
					1112,
					2582,
					292,
					2361,
					50700
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 42,
				"seek": 11192,
				"start": 118.62,
				"end": 122.58,
				"text": " of the actual input tokens that are going in.",
				"tokens": [
					50700,
					295,
					264,
					3539,
					4846,
					22667,
					300,
					366,
					516,
					294,
					13,
					50898
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 43,
				"seek": 11192,
				"start": 122.58,
				"end": 124.68,
				"text": " And this whole layer can almost be cached",
				"tokens": [
					50898,
					400,
					341,
					1379,
					4583,
					393,
					1920,
					312,
					269,
					15095,
					51003
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 44,
				"seek": 11192,
				"start": 124.68,
				"end": 127.24,
				"text": " because it's not really dependent on the output probabilities.",
				"tokens": [
					51003,
					570,
					309,
					311,
					406,
					534,
					12334,
					322,
					264,
					5598,
					33783,
					13,
					51131
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 45,
				"seek": 11192,
				"start": 127.24,
				"end": 130.54,
				"text": " And these are going to be able to take some advance of that.",
				"tokens": [
					51131,
					400,
					613,
					366,
					516,
					281,
					312,
					1075,
					281,
					747,
					512,
					7295,
					295,
					300,
					13,
					51296
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 46,
				"seek": 11192,
				"start": 130.54,
				"end": 133.02,
				"text": " The second half of this that Eugene is talking about",
				"tokens": [
					51296,
					440,
					1150,
					1922,
					295,
					341,
					300,
					37059,
					307,
					1417,
					466,
					51420
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 47,
				"seek": 11192,
				"start": 133.02,
				"end": 134.44,
				"text": " is also the KB.",
				"tokens": [
					51420,
					307,
					611,
					264,
					591,
					33,
					13,
					51491
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 48,
				"seek": 11192,
				"start": 134.44,
				"end": 136.68,
				"text": " There's a KB cache in the actual model,",
				"tokens": [
					51491,
					821,
					311,
					257,
					591,
					33,
					19459,
					294,
					264,
					3539,
					2316,
					11,
					51603
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 49,
				"seek": 11192,
				"start": 136.68,
				"end": 138.32,
				"text": " like matrix multiplication areas.",
				"tokens": [
					51603,
					411,
					8141,
					27290,
					3179,
					13,
					51685
				],
				"temperature": 0,
				"avg_logprob": -0.17794122,
				"compression_ratio": 1.7508897,
				"no_speech_prob": 1.5613565e-12
			},
			{
				"id": 50,
				"seek": 13832,
				"start": 138.32,
				"end": 143.48,
				"text": " And those can also dramatically improve the amount of computation throughput you can get",
				"tokens": [
					50365,
					400,
					729,
					393,
					611,
					17548,
					3470,
					264,
					2372,
					295,
					24903,
					44629,
					291,
					393,
					483,
					50623
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 51,
				"seek": 13832,
				"start": 143.48,
				"end": 145.44,
				"text": " out of the actual transform architecture.",
				"tokens": [
					50623,
					484,
					295,
					264,
					3539,
					4088,
					9482,
					13,
					50721
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 52,
				"seek": 13832,
				"start": 146.18,
				"end": 147.92,
				"text": " That may be actually what the paper was talking about, Eugene.",
				"tokens": [
					50758,
					663,
					815,
					312,
					767,
					437,
					264,
					3035,
					390,
					1417,
					466,
					11,
					37059,
					13,
					50845
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 53,
				"seek": 13832,
				"start": 148,
				"end": 149.16,
				"text": " I may have missed that.",
				"tokens": [
					50849,
					286,
					815,
					362,
					6721,
					300,
					13,
					50907
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 54,
				"seek": 13832,
				"start": 149.58,
				"end": 153.06,
				"text": " Yeah, I think it is about the cache inside the model.",
				"tokens": [
					50928,
					865,
					11,
					286,
					519,
					309,
					307,
					466,
					264,
					19459,
					1854,
					264,
					2316,
					13,
					51102
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 55,
				"seek": 13832,
				"start": 153.34,
				"end": 153.66,
				"text": " Yes.",
				"tokens": [
					51116,
					1079,
					13,
					51132
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 56,
				"seek": 13832,
				"start": 154.32,
				"end": 156.92,
				"text": " So in that case, it's still the principle is still the same.",
				"tokens": [
					51165,
					407,
					294,
					300,
					1389,
					11,
					309,
					311,
					920,
					264,
					8665,
					307,
					920,
					264,
					912,
					13,
					51295
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 57,
				"seek": 13832,
				"start": 157.66,
				"end": 162,
				"text": " Everything's the same except nothing's going into Redis, I think probably is the key detail.",
				"tokens": [
					51332,
					5471,
					311,
					264,
					912,
					3993,
					1825,
					311,
					516,
					666,
					4477,
					271,
					11,
					286,
					519,
					1391,
					307,
					264,
					2141,
					2607,
					13,
					51549
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 58,
				"seek": 13832,
				"start": 162.78,
				"end": 163.04,
				"text": " Yes.",
				"tokens": [
					51588,
					1079,
					13,
					51601
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 59,
				"seek": 13832,
				"start": 163.2,
				"end": 165.92,
				"text": " Well, no, that one's cutting down on the actual math that it has to go to.",
				"tokens": [
					51609,
					1042,
					11,
					572,
					11,
					300,
					472,
					311,
					6492,
					760,
					322,
					264,
					3539,
					5221,
					300,
					309,
					575,
					281,
					352,
					281,
					13,
					51745
				],
				"temperature": 0,
				"avg_logprob": -0.1778851,
				"compression_ratio": 1.7171717,
				"no_speech_prob": 1.2944762e-12
			},
			{
				"id": 60,
				"seek": 16592,
				"start": 165.92,
				"end": 167.14,
				"text": " but the main difference",
				"tokens": [
					50365,
					457,
					264,
					2135,
					2649,
					50426
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 61,
				"seek": 16592,
				"start": 167.14,
				"end": 169.52,
				"text": " the premise is still the same",
				"tokens": [
					50426,
					264,
					22045,
					307,
					920,
					264,
					912,
					50545
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 62,
				"seek": 16592,
				"start": 169.52,
				"end": 171.88,
				"text": " the number of continuity tokens you have",
				"tokens": [
					50545,
					264,
					1230,
					295,
					23807,
					22667,
					291,
					362,
					50663
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 63,
				"seek": 16592,
				"start": 171.88,
				"end": 174.3,
				"text": " dramatically changes the amount of",
				"tokens": [
					50663,
					17548,
					2962,
					264,
					2372,
					295,
					50784
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 64,
				"seek": 16592,
				"start": 174.3,
				"end": 175.9,
				"text": " caching that you're going to get",
				"tokens": [
					50784,
					269,
					2834,
					300,
					291,
					434,
					516,
					281,
					483,
					50864
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 65,
				"seek": 16592,
				"start": 175.9,
				"end": 178.52,
				"text": " anytime you go change the continuity",
				"tokens": [
					50864,
					13038,
					291,
					352,
					1319,
					264,
					23807,
					50995
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 66,
				"seek": 16592,
				"start": 178.52,
				"end": 180.4,
				"text": " of the tokens the more likely you are",
				"tokens": [
					50995,
					295,
					264,
					22667,
					264,
					544,
					3700,
					291,
					366,
					51089
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 67,
				"seek": 16592,
				"start": 180.4,
				"end": 182.48,
				"text": " going to be to break the cache at any point",
				"tokens": [
					51089,
					516,
					281,
					312,
					281,
					1821,
					264,
					19459,
					412,
					604,
					935,
					51193
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 68,
				"seek": 16592,
				"start": 182.48,
				"end": 184.36,
				"text": " and get a miss on the cache",
				"tokens": [
					51193,
					293,
					483,
					257,
					1713,
					322,
					264,
					19459,
					51287
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 69,
				"seek": 16592,
				"start": 184.36,
				"end": 186.56,
				"text": " how do I",
				"tokens": [
					51287,
					577,
					360,
					286,
					51397
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 70,
				"seek": 16592,
				"start": 186.56,
				"end": 188.46,
				"text": " how do I eval if my updated context",
				"tokens": [
					51397,
					577,
					360,
					286,
					1073,
					304,
					498,
					452,
					10588,
					4319,
					51492
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 71,
				"seek": 16592,
				"start": 188.46,
				"end": 189.94,
				"text": " handling will",
				"tokens": [
					51492,
					13175,
					486,
					51566
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 72,
				"seek": 16592,
				"start": 189.94,
				"end": 192.6,
				"text": " improve the quality",
				"tokens": [
					51566,
					3470,
					264,
					3125,
					51699
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 73,
				"seek": 16592,
				"start": 192.6,
				"end": 194.68,
				"text": " I think",
				"tokens": [
					51699,
					286,
					519,
					51803
				],
				"temperature": 0,
				"avg_logprob": -0.1442309,
				"compression_ratio": 1.8082192,
				"no_speech_prob": 8.7579266e-13
			},
			{
				"id": 74,
				"seek": 19468,
				"start": 194.68,
				"end": 196.02,
				"text": " we've had a couple talks on evals.",
				"tokens": [
					50365,
					321,
					600,
					632,
					257,
					1916,
					6686,
					322,
					1073,
					1124,
					13,
					50432
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 75,
				"seek": 19468,
				"start": 196.18,
				"end": 199,
				"text": " It's actually very annoying to do evals",
				"tokens": [
					50440,
					467,
					311,
					767,
					588,
					11304,
					281,
					360,
					1073,
					1124,
					50581
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 76,
				"seek": 19468,
				"start": 199,
				"end": 200.18,
				"text": " and it's very frustrating,",
				"tokens": [
					50581,
					293,
					309,
					311,
					588,
					16522,
					11,
					50640
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 77,
				"seek": 19468,
				"start": 200.32,
				"end": 201.58,
				"text": " but I think the first step to do evals",
				"tokens": [
					50647,
					457,
					286,
					519,
					264,
					700,
					1823,
					281,
					360,
					1073,
					1124,
					50710
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 78,
				"seek": 19468,
				"start": 201.58,
				"end": 203.56,
				"text": " is just to define quality to some degree",
				"tokens": [
					50710,
					307,
					445,
					281,
					6964,
					3125,
					281,
					512,
					4314,
					50809
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 79,
				"seek": 19468,
				"start": 203.56,
				"end": 204.22,
				"text": " and what it means.",
				"tokens": [
					50809,
					293,
					437,
					309,
					1355,
					13,
					50842
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 80,
				"seek": 19468,
				"start": 204.7,
				"end": 205.92,
				"text": " Understand what is clearly good,",
				"tokens": [
					50866,
					26093,
					437,
					307,
					4448,
					665,
					11,
					50927
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 81,
				"seek": 19468,
				"start": 205.98,
				"end": 206.7,
				"text": " what is clearly bad,",
				"tokens": [
					50930,
					437,
					307,
					4448,
					1578,
					11,
					50966
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 82,
				"seek": 19468,
				"start": 206.76,
				"end": 208.12,
				"text": " what is mostly good, mostly bad.",
				"tokens": [
					50969,
					437,
					307,
					5240,
					665,
					11,
					5240,
					1578,
					13,
					51037
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 83,
				"seek": 19468,
				"start": 208.68,
				"end": 210.5,
				"text": " And then just like buy the eval in the beginning.",
				"tokens": [
					51065,
					400,
					550,
					445,
					411,
					2256,
					264,
					1073,
					304,
					294,
					264,
					2863,
					13,
					51156
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 84,
				"seek": 19468,
				"start": 210.74,
				"end": 212.12,
				"text": " And once you have a better understanding",
				"tokens": [
					51168,
					400,
					1564,
					291,
					362,
					257,
					1101,
					3701,
					51237
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 85,
				"seek": 19468,
				"start": 212.12,
				"end": 213.04,
				"text": " of that system,",
				"tokens": [
					51237,
					295,
					300,
					1185,
					11,
					51283
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 86,
				"seek": 19468,
				"start": 213.26,
				"end": 213.9,
				"text": " then you can go ahead",
				"tokens": [
					51294,
					550,
					291,
					393,
					352,
					2286,
					51326
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 87,
				"seek": 19468,
				"start": 213.9,
				"end": 214.94,
				"text": " and actually make these trade-offs.",
				"tokens": [
					51326,
					293,
					767,
					652,
					613,
					4923,
					12,
					19231,
					13,
					51378
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 88,
				"seek": 19468,
				"start": 215.38,
				"end": 217.56,
				"text": " The only reason that likely can do this work",
				"tokens": [
					51400,
					440,
					787,
					1778,
					300,
					3700,
					393,
					360,
					341,
					589,
					51509
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 89,
				"seek": 19468,
				"start": 217.56,
				"end": 219.28,
				"text": " or any engineering team can do this work",
				"tokens": [
					51509,
					420,
					604,
					7043,
					1469,
					393,
					360,
					341,
					589,
					51595
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 90,
				"seek": 19468,
				"start": 219.28,
				"end": 220.98,
				"text": " is because they first have to go ahead and say,",
				"tokens": [
					51595,
					307,
					570,
					436,
					700,
					362,
					281,
					352,
					2286,
					293,
					584,
					11,
					51680
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 91,
				"seek": 19468,
				"start": 221.14,
				"end": 222.66,
				"text": " this is good and this is bad.",
				"tokens": [
					51688,
					341,
					307,
					665,
					293,
					341,
					307,
					1578,
					13,
					51764
				],
				"temperature": 0,
				"avg_logprob": -0.17723085,
				"compression_ratio": 1.9190031,
				"no_speech_prob": 2.9513022e-12
			},
			{
				"id": 92,
				"seek": 22266,
				"start": 222.66,
				"end": 229.94,
				"text": " that evals are useless and i'd say like most of the stuff we talk about about evals here is about",
				"tokens": [
					50365,
					300,
					1073,
					1124,
					366,
					14115,
					293,
					741,
					1116,
					584,
					411,
					881,
					295,
					264,
					1507,
					321,
					751,
					466,
					466,
					1073,
					1124,
					510,
					307,
					466,
					50729
				],
				"temperature": 0,
				"avg_logprob": -0.06806614,
				"compression_ratio": 1.8610039,
				"no_speech_prob": 1.9431253e-12
			},
			{
				"id": 93,
				"seek": 22266,
				"start": 229.94,
				"end": 235.76,
				"text": " quality of the outputs in terms of like accuracy host nations using the right context things like",
				"tokens": [
					50729,
					3125,
					295,
					264,
					23930,
					294,
					2115,
					295,
					411,
					14170,
					3975,
					11035,
					1228,
					264,
					558,
					4319,
					721,
					411,
					51020
				],
				"temperature": 0,
				"avg_logprob": -0.06806614,
				"compression_ratio": 1.8610039,
				"no_speech_prob": 1.9431253e-12
			},
			{
				"id": 94,
				"seek": 22266,
				"start": 235.76,
				"end": 240.6,
				"text": " that um the quality improvements that we're getting here because like at the end of the day",
				"tokens": [
					51020,
					300,
					1105,
					264,
					3125,
					13797,
					300,
					321,
					434,
					1242,
					510,
					570,
					411,
					412,
					264,
					917,
					295,
					264,
					786,
					51262
				],
				"temperature": 0,
				"avg_logprob": -0.06806614,
				"compression_ratio": 1.8610039,
				"no_speech_prob": 1.9431253e-12
			},
			{
				"id": 95,
				"seek": 22266,
				"start": 240.6,
				"end": 244.14,
				"text": " whether you cash it or not it's the same tokens in which means your chance of getting the right",
				"tokens": [
					51262,
					1968,
					291,
					6388,
					309,
					420,
					406,
					309,
					311,
					264,
					912,
					22667,
					294,
					597,
					1355,
					428,
					2931,
					295,
					1242,
					264,
					558,
					51439
				],
				"temperature": 0,
				"avg_logprob": -0.06806614,
				"compression_ratio": 1.8610039,
				"no_speech_prob": 1.9431253e-12
			},
			{
				"id": 96,
				"seek": 22266,
				"start": 244.14,
				"end": 248.96,
				"text": " tokens out is probably about the same um so this is a really interesting kind of category of evals",
				"tokens": [
					51439,
					22667,
					484,
					307,
					1391,
					466,
					264,
					912,
					1105,
					370,
					341,
					307,
					257,
					534,
					1880,
					733,
					295,
					7719,
					295,
					1073,
					1124,
					51680
				],
				"temperature": 0,
				"avg_logprob": -0.06806614,
				"compression_ratio": 1.8610039,
				"no_speech_prob": 1.9431253e-12
			},
			{
				"id": 97,
				"seek": 24896,
				"start": 248.96,
				"end": 250.28,
				"text": " we probably haven't talked as much about,",
				"tokens": [
					50365,
					321,
					1391,
					2378,
					380,
					2825,
					382,
					709,
					466,
					11,
					50431
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 98,
				"seek": 24896,
				"start": 250.36,
				"end": 252.68,
				"text": " which is like, how do you evaluate the performance,",
				"tokens": [
					50435,
					597,
					307,
					411,
					11,
					577,
					360,
					291,
					13059,
					264,
					3389,
					11,
					50551
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 99,
				"seek": 24896,
				"start": 252.68,
				"end": 254.08,
				"text": " not in terms of accuracy,",
				"tokens": [
					50551,
					406,
					294,
					2115,
					295,
					14170,
					11,
					50621
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 100,
				"seek": 24896,
				"start": 254.46,
				"end": 257.44,
				"text": " but in terms of speed and cost?",
				"tokens": [
					50640,
					457,
					294,
					2115,
					295,
					3073,
					293,
					2063,
					30,
					50789
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 101,
				"seek": 24896,
				"start": 258.36,
				"end": 258.44,
				"text": " Yeah.",
				"tokens": [
					50835,
					865,
					13,
					50839
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 102,
				"seek": 24896,
				"start": 259.54,
				"end": 261.3,
				"text": " I think Vijay has an interesting question.",
				"tokens": [
					50894,
					286,
					519,
					41201,
					320,
					575,
					364,
					1880,
					1168,
					13,
					50982
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 103,
				"seek": 24896,
				"start": 261.58,
				"end": 263.62,
				"text": " I guess Rajesh pointed out a different thing.",
				"tokens": [
					50996,
					286,
					2041,
					16745,
					14935,
					10932,
					484,
					257,
					819,
					551,
					13,
					51098
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 104,
				"seek": 24896,
				"start": 263.64,
				"end": 264.66,
				"text": " If the model weights are the same,",
				"tokens": [
					51099,
					759,
					264,
					2316,
					17443,
					366,
					264,
					912,
					11,
					51150
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 105,
				"seek": 24896,
				"start": 264.76,
				"end": 265.38,
				"text": " everything is new,",
				"tokens": [
					51155,
					1203,
					307,
					777,
					11,
					51186
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 106,
				"seek": 24896,
				"start": 265.46,
				"end": 266.34,
				"text": " how do you actually get caching?",
				"tokens": [
					51190,
					577,
					360,
					291,
					767,
					483,
					269,
					2834,
					30,
					51234
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 107,
				"seek": 24896,
				"start": 267.8,
				"end": 269.34,
				"text": " It's really hard to describe",
				"tokens": [
					51307,
					467,
					311,
					534,
					1152,
					281,
					6786,
					51384
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 108,
				"seek": 24896,
				"start": 269.34,
				"end": 271.8,
				"text": " how the math on a GPU gets cached,",
				"tokens": [
					51384,
					577,
					264,
					5221,
					322,
					257,
					18407,
					2170,
					269,
					15095,
					11,
					51507
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 109,
				"seek": 24896,
				"start": 271.98,
				"end": 274.02,
				"text": " but you can do caching on there.",
				"tokens": [
					51516,
					457,
					291,
					393,
					360,
					269,
					2834,
					322,
					456,
					13,
					51618
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 110,
				"seek": 24896,
				"start": 274.4,
				"end": 275.22,
				"text": " You should take my word for it.",
				"tokens": [
					51637,
					509,
					820,
					747,
					452,
					1349,
					337,
					309,
					13,
					51678
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 111,
				"seek": 24896,
				"start": 275.24,
				"end": 277.18,
				"text": " There's a beautiful video made by,",
				"tokens": [
					51679,
					821,
					311,
					257,
					2238,
					960,
					1027,
					538,
					11,
					51776
				],
				"temperature": 0,
				"avg_logprob": -0.16508725,
				"compression_ratio": 1.640264,
				"no_speech_prob": 1.9355635e-12
			},
			{
				"id": 112,
				"seek": 27718,
				"start": 277.18,
				"end": 283.22,
				"text": " If any of you are interested, this is probably one of the best videos I've ever seen.",
				"tokens": [
					50365,
					759,
					604,
					295,
					291,
					366,
					3102,
					11,
					341,
					307,
					1391,
					472,
					295,
					264,
					1151,
					2145,
					286,
					600,
					1562,
					1612,
					13,
					50667
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 113,
				"seek": 27718,
				"start": 284.74,
				"end": 285.34,
				"text": " Hey, everyone.",
				"tokens": [
					50743,
					1911,
					11,
					1518,
					13,
					50773
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 114,
				"seek": 27718,
				"start": 285.6,
				"end": 286.1,
				"text": " I'm by Bob.",
				"tokens": [
					50786,
					286,
					478,
					538,
					6085,
					13,
					50811
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 115,
				"seek": 27718,
				"start": 286.14,
				"end": 286.36,
				"text": " Sorry.",
				"tokens": [
					50813,
					4919,
					13,
					50824
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 116,
				"seek": 27718,
				"start": 286.5,
				"end": 287.66,
				"text": " I'll give you...",
				"tokens": [
					50831,
					286,
					603,
					976,
					291,
					485,
					50889
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 117,
				"seek": 27718,
				"start": 287.66,
				"end": 288.12,
				"text": " I'm sorry.",
				"tokens": [
					50889,
					286,
					478,
					2597,
					13,
					50912
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 118,
				"seek": 27718,
				"start": 288.98,
				"end": 293.94,
				"text": " Every time I pull up the YouTube channel, I hear you introducing yourself.",
				"tokens": [
					50955,
					2048,
					565,
					286,
					2235,
					493,
					264,
					3088,
					2269,
					11,
					286,
					1568,
					291,
					15424,
					1803,
					13,
					51203
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 119,
				"seek": 27718,
				"start": 293.94,
				"end": 294.46,
				"text": " I'll post the video on here.",
				"tokens": [
					51203,
					286,
					603,
					2183,
					264,
					960,
					322,
					510,
					13,
					51229
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 120,
				"seek": 27718,
				"start": 294.56,
				"end": 301.6,
				"text": " There's like a 90-minute video that I watched that actually talked about how DeepSeek actually works under the hood and talks about the math behind it.",
				"tokens": [
					51234,
					821,
					311,
					411,
					257,
					4289,
					12,
					18256,
					960,
					300,
					286,
					6337,
					300,
					767,
					2825,
					466,
					577,
					14895,
					10637,
					916,
					767,
					1985,
					833,
					264,
					13376,
					293,
					6686,
					466,
					264,
					5221,
					2261,
					309,
					13,
					51586
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 121,
				"seek": 27718,
				"start": 302.22,
				"end": 303.6,
				"text": " I think it's actually this one.",
				"tokens": [
					51617,
					286,
					519,
					309,
					311,
					767,
					341,
					472,
					13,
					51686
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 122,
				"seek": 27718,
				"start": 304.14,
				"end": 305.12,
				"text": " This video is...",
				"tokens": [
					51713,
					639,
					960,
					307,
					485,
					51762
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 123,
				"seek": 27718,
				"start": 305.12,
				"end": 306.42,
				"text": " It's one of the best ones I've ever seen.",
				"tokens": [
					51762,
					467,
					311,
					472,
					295,
					264,
					1151,
					2306,
					286,
					600,
					1562,
					1612,
					13,
					51827
				],
				"temperature": 0,
				"avg_logprob": -0.29115662,
				"compression_ratio": 1.7212543,
				"no_speech_prob": 3.2035453e-12
			},
			{
				"id": 124,
				"seek": 30642,
				"start": 306.42,
				"end": 307.22,
				"text": " It will describe it.",
				"tokens": [
					50365,
					467,
					486,
					6786,
					309,
					13,
					50405
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 125,
				"seek": 30642,
				"start": 307.28,
				"end": 312.04,
				"text": " It will probably teach you more about anything you could see.",
				"tokens": [
					50408,
					467,
					486,
					1391,
					2924,
					291,
					544,
					466,
					1340,
					291,
					727,
					536,
					13,
					50646
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 126,
				"seek": 30642,
				"start": 312.38,
				"end": 313.18,
				"text": " I wish I could make that.",
				"tokens": [
					50663,
					286,
					3172,
					286,
					727,
					652,
					300,
					13,
					50703
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 127,
				"seek": 30642,
				"start": 314.98,
				"end": 317.02,
				"text": " John, best video I've ever seen.",
				"tokens": [
					50793,
					2619,
					11,
					1151,
					960,
					286,
					600,
					1562,
					1612,
					13,
					50895
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 128,
				"seek": 30642,
				"start": 317.16,
				"end": 318.32,
				"text": " Pulls up video of self.",
				"tokens": [
					50902,
					15074,
					82,
					493,
					960,
					295,
					2698,
					13,
					50960
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 129,
				"seek": 30642,
				"start": 319.02,
				"end": 325.2,
				"text": " And I'll talk to you about how it's possible to get way better throughput than anyone else can on the same math that everyone else is doing.",
				"tokens": [
					50995,
					400,
					286,
					603,
					751,
					281,
					291,
					466,
					577,
					309,
					311,
					1944,
					281,
					483,
					636,
					1101,
					44629,
					813,
					2878,
					1646,
					393,
					322,
					264,
					912,
					5221,
					300,
					1518,
					1646,
					307,
					884,
					13,
					51304
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 130,
				"seek": 30642,
				"start": 328.54,
				"end": 331.3,
				"text": " Context caching in our app thesis, the old code is open source for anyone.",
				"tokens": [
					51471,
					4839,
					3828,
					269,
					2834,
					294,
					527,
					724,
					22288,
					11,
					264,
					1331,
					3089,
					307,
					1269,
					4009,
					337,
					2878,
					13,
					51609
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 131,
				"seek": 30642,
				"start": 331.5,
				"end": 332.12,
				"text": " Okay, cool.",
				"tokens": [
					51619,
					1033,
					11,
					1627,
					13,
					51650
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 132,
				"seek": 30642,
				"start": 332.26,
				"end": 333.12,
				"text": " There's just an example.",
				"tokens": [
					51657,
					821,
					311,
					445,
					364,
					1365,
					13,
					51700
				],
				"temperature": 0,
				"avg_logprob": -0.25993514,
				"compression_ratio": 1.5833334,
				"no_speech_prob": 2.227789e-12
			},
			{
				"id": 133,
				"seek": 33312,
				"start": 333.12,
				"end": 337.74,
				"text": " If Duck has an example, then definitely share it, Duck, and we'd love to have people go be able to see it.",
				"tokens": [
					50365,
					759,
					29266,
					575,
					364,
					1365,
					11,
					550,
					2138,
					2073,
					309,
					11,
					29266,
					11,
					293,
					321,
					1116,
					959,
					281,
					362,
					561,
					352,
					312,
					1075,
					281,
					536,
					309,
					13,
					50596
				],
				"temperature": 0,
				"avg_logprob": -0.16494335,
				"compression_ratio": 1.610687,
				"no_speech_prob": 2.0047776e-12
			},
			{
				"id": 134,
				"seek": 33312,
				"start": 338.42,
				"end": 339.48,
				"text": " Yeah, drop it in the chat.",
				"tokens": [
					50630,
					865,
					11,
					3270,
					309,
					294,
					264,
					5081,
					13,
					50683
				],
				"temperature": 0,
				"avg_logprob": -0.16494335,
				"compression_ratio": 1.610687,
				"no_speech_prob": 2.0047776e-12
			},
			{
				"id": 135,
				"seek": 33312,
				"start": 339.94,
				"end": 343.68,
				"text": " Vijay, we've got a question. We'll take that, and then we'll go to the next part of the system after that.",
				"tokens": [
					50706,
					41201,
					320,
					11,
					321,
					600,
					658,
					257,
					1168,
					13,
					492,
					603,
					747,
					300,
					11,
					293,
					550,
					321,
					603,
					352,
					281,
					264,
					958,
					644,
					295,
					264,
					1185,
					934,
					300,
					13,
					50893
				],
				"temperature": 0,
				"avg_logprob": -0.16494335,
				"compression_ratio": 1.610687,
				"no_speech_prob": 2.0047776e-12
			},
			{
				"id": 136,
				"seek": 33312,
				"start": 345.1,
				"end": 354.48,
				"text": " So if I understood this correctly, for example, let's say our model has an input window of 256k tokens,",
				"tokens": [
					50964,
					407,
					498,
					286,
					7320,
					341,
					8944,
					11,
					337,
					1365,
					11,
					718,
					311,
					584,
					527,
					2316,
					575,
					364,
					4846,
					4910,
					295,
					38882,
					74,
					22667,
					11,
					51433
				],
				"temperature": 0,
				"avg_logprob": -0.16494335,
				"compression_ratio": 1.610687,
				"no_speech_prob": 2.0047776e-12
			},
			{
				"id": 137,
				"seek": 33312,
				"start": 355.04,
				"end": 358.98,
				"text": " and we're trying to, let's say, take advantage of that entire context window.",
				"tokens": [
					51461,
					293,
					321,
					434,
					1382,
					281,
					11,
					718,
					311,
					584,
					11,
					747,
					5002,
					295,
					300,
					2302,
					4319,
					4910,
					13,
					51658
				],
				"temperature": 0,
				"avg_logprob": -0.16494335,
				"compression_ratio": 1.610687,
				"no_speech_prob": 2.0047776e-12
			},
			{
				"id": 138,
				"seek": 35898,
				"start": 358.98,
				"end": 365.98,
				"text": " So essentially what you're saying is whatever we input in that 256k context, as much as possible,",
				"tokens": [
					50365,
					407,
					4476,
					437,
					291,
					434,
					1566,
					307,
					2035,
					321,
					4846,
					294,
					300,
					38882,
					74,
					4319,
					11,
					382,
					709,
					382,
					1944,
					11,
					50715
				],
				"temperature": 0,
				"avg_logprob": -0.16838965,
				"compression_ratio": 1.6260163,
				"no_speech_prob": 7.433643e-13
			},
			{
				"id": 139,
				"seek": 35898,
				"start": 365.98,
				"end": 371.98,
				"text": " that string should be unchanging or let's say static.",
				"tokens": [
					50715,
					300,
					6798,
					820,
					312,
					517,
					27123,
					420,
					718,
					311,
					584,
					13437,
					13,
					51015
				],
				"temperature": 0,
				"avg_logprob": -0.16838965,
				"compression_ratio": 1.6260163,
				"no_speech_prob": 7.433643e-13
			},
			{
				"id": 140,
				"seek": 35898,
				"start": 371.98,
				"end": 375.98,
				"text": " It should be the same in next iteration so that you can get the next open quicker.",
				"tokens": [
					51015,
					467,
					820,
					312,
					264,
					912,
					294,
					958,
					24784,
					370,
					300,
					291,
					393,
					483,
					264,
					958,
					1269,
					16255,
					13,
					51215
				],
				"temperature": 0,
				"avg_logprob": -0.16838965,
				"compression_ratio": 1.6260163,
				"no_speech_prob": 7.433643e-13
			},
			{
				"id": 141,
				"seek": 379,
				"start": 375.98,
				"end": 391.03845,
				"text": " And the end of that input context will probably be varying per call So I think the next step would be to understand how agents are iterating between different calls and see what is changing what is not changing",
				"tokens": [
					51215,
					400,
					264,
					917,
					295,
					300,
					4846,
					4319,
					486,
					1391,
					312,
					22984,
					680,
					818,
					13,
					51515,
					51515,
					407,
					286,
					519,
					264,
					958,
					1823,
					576,
					312,
					281,
					1223,
					577,
					50634,
					577,
					12554,
					366,
					17138,
					990,
					1296,
					819,
					5498,
					50794,
					50794,
					293,
					536,
					437,
					307,
					4473,
					11,
					437,
					307,
					406,
					4473,
					50917
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 142,
				"seek": 379,
				"start": 391.03845,
				"end": 392.71844,
				"text": " in what we are sending the LLM",
				"tokens": [
					50917,
					294,
					437,
					321,
					366,
					7750,
					264,
					441,
					43,
					44,
					51001
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 143,
				"seek": 379,
				"start": 392.71844,
				"end": 396.73843,
				"text": " and sort of restructure that entire input sequence.",
				"tokens": [
					51001,
					293,
					1333,
					295,
					1472,
					2885,
					300,
					2302,
					4846,
					8310,
					13,
					51202
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 144,
				"seek": 379,
				"start": 396.73843,
				"end": 398.77844,
				"text": " Is that the next step?",
				"tokens": [
					51202,
					1119,
					300,
					264,
					958,
					1823,
					30,
					51304
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 145,
				"seek": 379,
				"start": 398.77844,
				"end": 401.65845,
				"text": " Like, am I thinking about this correctly?",
				"tokens": [
					51304,
					1743,
					11,
					669,
					286,
					1953,
					466,
					341,
					8944,
					30,
					51448
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 146,
				"seek": 379,
				"start": 401.65845,
				"end": 405.33844,
				"text": " I think maybe, just maybe I don't understand",
				"tokens": [
					51448,
					286,
					519,
					1310,
					11,
					445,
					1310,
					286,
					500,
					380,
					1223,
					51632
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 147,
				"seek": 379,
				"start": 405.33844,
				"end": 406.69846,
				"text": " all the words you said perfectly.",
				"tokens": [
					51632,
					439,
					264,
					2283,
					291,
					848,
					6239,
					13,
					51700
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 148,
				"seek": 379,
				"start": 406.69846,
				"end": 408.75845,
				"text": " And it's probably because I woke up at 4.",
				"tokens": [
					51700,
					400,
					309,
					311,
					1391,
					570,
					286,
					12852,
					493,
					412,
					1017,
					13,
					51803
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 149,
				"seek": 379,
				"start": 408.75845,
				"end": 409.91846,
				"text": " Okay, let me simplify it.",
				"tokens": [
					51803,
					1033,
					11,
					718,
					385,
					20460,
					309,
					13,
					51861
				],
				"temperature": 0,
				"avg_logprob": -0.16836837,
				"compression_ratio": 1.6877193,
				"no_speech_prob": 2.0205662e-12
			},
			{
				"id": 150,
				"seek": 3371,
				"start": 409.91846,
				"end": 412.57843,
				"text": " So we are sending tokens to the LLM.",
				"tokens": [
					50365,
					407,
					321,
					366,
					7750,
					22667,
					281,
					264,
					441,
					43,
					44,
					13,
					50498
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 151,
				"seek": 3371,
				"start": 412.77844,
				"end": 413.03845,
				"text": " Yes.",
				"tokens": [
					50508,
					1079,
					13,
					50521
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 152,
				"seek": 3371,
				"start": 413.75845,
				"end": 417.17844,
				"text": " The sequence of tokens, as much as it is unchanged,",
				"tokens": [
					50557,
					440,
					8310,
					295,
					22667,
					11,
					382,
					709,
					382,
					309,
					307,
					44553,
					11,
					50728
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 153,
				"seek": 3371,
				"start": 417.61844,
				"end": 419.55844,
				"text": " the faster we get a response.",
				"tokens": [
					50750,
					264,
					4663,
					321,
					483,
					257,
					4134,
					13,
					50847
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 154,
				"seek": 3371,
				"start": 419.79843,
				"end": 420.05844,
				"text": " Yes.",
				"tokens": [
					50859,
					1079,
					13,
					50872
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 155,
				"seek": 3371,
				"start": 420.41846,
				"end": 423.81845,
				"text": " Therefore, our next step would obviously be to understand",
				"tokens": [
					50890,
					7504,
					11,
					527,
					958,
					1823,
					576,
					2745,
					312,
					281,
					1223,
					51060
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 156,
				"seek": 3371,
				"start": 423.81845,
				"end": 427.53845,
				"text": " a couple of iterations of what we are sending the LLM,",
				"tokens": [
					51060,
					257,
					1916,
					295,
					36540,
					295,
					437,
					321,
					366,
					7750,
					264,
					441,
					43,
					44,
					11,
					51246
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 157,
				"seek": 3371,
				"start": 427.99844,
				"end": 429.73843,
				"text": " see what is changing, what is not changing,",
				"tokens": [
					51269,
					536,
					437,
					307,
					4473,
					11,
					437,
					307,
					406,
					4473,
					11,
					51356
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 158,
				"seek": 3371,
				"start": 429.85846,
				"end": 431.27844,
				"text": " and try to restructure that.",
				"tokens": [
					51362,
					293,
					853,
					281,
					1472,
					2885,
					300,
					13,
					51433
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 159,
				"seek": 3371,
				"start": 431.53845,
				"end": 431.71844,
				"text": " Correct?",
				"tokens": [
					51446,
					12753,
					30,
					51455
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 160,
				"seek": 3371,
				"start": 432.27844,
				"end": 433.19846,
				"text": " Yes, exactly.",
				"tokens": [
					51483,
					1079,
					11,
					2293,
					13,
					51529
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 161,
				"seek": 3371,
				"start": 434.03845,
				"end": 435.67844,
				"text": " There are caveats, though,",
				"tokens": [
					51571,
					821,
					366,
					11730,
					1720,
					11,
					1673,
					11,
					51653
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 162,
				"seek": 3371,
				"start": 435.71844,
				"end": 438.05844,
				"text": " in terms of actual implementation details here that matter.",
				"tokens": [
					51655,
					294,
					2115,
					295,
					3539,
					11420,
					4365,
					510,
					300,
					1871,
					13,
					51772
				],
				"temperature": 0,
				"avg_logprob": -0.21266118,
				"compression_ratio": 1.6825397,
				"no_speech_prob": 1.3301643e-12
			},
			{
				"id": 163,
				"seek": 6185,
				"start": 438.05844,
				"end": 444.01843,
				"text": " If you own the model yourself and you're doing inference, it's a very different game than if you're actually using the existing inference providers.",
				"tokens": [
					50365,
					759,
					291,
					1065,
					264,
					2316,
					1803,
					293,
					291,
					434,
					884,
					38253,
					11,
					309,
					311,
					257,
					588,
					819,
					1216,
					813,
					498,
					291,
					434,
					767,
					1228,
					264,
					6741,
					38253,
					11330,
					13,
					50663
				],
				"temperature": 0,
				"avg_logprob": -0.10740616,
				"compression_ratio": 1.8196079,
				"no_speech_prob": 1.4326703e-12
			},
			{
				"id": 164,
				"seek": 6185,
				"start": 444.51843,
				"end": 449.51843,
				"text": " So, for example, Anthropic lets you actually control cache control with this cache control block.",
				"tokens": [
					50688,
					407,
					11,
					337,
					1365,
					11,
					12727,
					39173,
					6653,
					291,
					767,
					1969,
					19459,
					1969,
					365,
					341,
					19459,
					1969,
					3461,
					13,
					50938
				],
				"temperature": 0,
				"avg_logprob": -0.10740616,
				"compression_ratio": 1.8196079,
				"no_speech_prob": 1.4326703e-12
			},
			{
				"id": 165,
				"seek": 6185,
				"start": 449.95844,
				"end": 452.21844,
				"text": " And where you put it dramatically matters.",
				"tokens": [
					50960,
					400,
					689,
					291,
					829,
					309,
					17548,
					7001,
					13,
					51073
				],
				"temperature": 0,
				"avg_logprob": -0.10740616,
				"compression_ratio": 1.8196079,
				"no_speech_prob": 1.4326703e-12
			},
			{
				"id": 166,
				"seek": 6185,
				"start": 452.75845,
				"end": 459.69843,
				"text": " Because if you put it at the very beginning of a system, if you put it in your system message, it will dramatically change the throughput that you are personally able to get.",
				"tokens": [
					51100,
					1436,
					498,
					291,
					829,
					309,
					412,
					264,
					588,
					2863,
					295,
					257,
					1185,
					11,
					498,
					291,
					829,
					309,
					294,
					428,
					1185,
					3636,
					11,
					309,
					486,
					17548,
					1319,
					264,
					44629,
					300,
					291,
					366,
					5665,
					1075,
					281,
					483,
					13,
					51447
				],
				"temperature": 0,
				"avg_logprob": -0.10740616,
				"compression_ratio": 1.8196079,
				"no_speech_prob": 1.4326703e-12
			},
			{
				"id": 167,
				"seek": 8349,
				"start": 459.69843,
				"end": 463.99844,
				"text": " if you have a really long chat thread and you just put it at the beginning of a chat thread and you",
				"tokens": [
					50365,
					498,
					291,
					362,
					257,
					534,
					938,
					5081,
					7207,
					293,
					291,
					445,
					829,
					309,
					412,
					264,
					2863,
					295,
					257,
					5081,
					7207,
					293,
					291,
					50580
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 168,
				"seek": 8349,
				"start": 463.99844,
				"end": 469.45844,
				"text": " don't break apart your chat threads manually you're going to get worse hits there's just nothing you",
				"tokens": [
					50580,
					500,
					380,
					1821,
					4936,
					428,
					5081,
					19314,
					16945,
					291,
					434,
					516,
					281,
					483,
					5324,
					8664,
					456,
					311,
					445,
					1825,
					291,
					50853
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 169,
				"seek": 8349,
				"start": 469.45844,
				"end": 473.51843,
				"text": " can do about that because your chat thread is basically you put you put a cache control block",
				"tokens": [
					50853,
					393,
					360,
					466,
					300,
					570,
					428,
					5081,
					7207,
					307,
					1936,
					291,
					829,
					291,
					829,
					257,
					19459,
					1969,
					3461,
					51056
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 170,
				"seek": 8349,
				"start": 473.51843,
				"end": 478.03845,
				"text": " on the first user message and then you in the same chat control block you put another user message",
				"tokens": [
					51056,
					322,
					264,
					700,
					4195,
					3636,
					293,
					550,
					291,
					294,
					264,
					912,
					5081,
					1969,
					3461,
					291,
					829,
					1071,
					4195,
					3636,
					51282
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 171,
				"seek": 8349,
				"start": 478.03845,
				"end": 482.83844,
				"text": " it's just a cache miss it's a guaranteed cache miss you can't actually do anything about that",
				"tokens": [
					51282,
					309,
					311,
					445,
					257,
					19459,
					1713,
					309,
					311,
					257,
					18031,
					19459,
					1713,
					291,
					393,
					380,
					767,
					360,
					1340,
					466,
					300,
					51522
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 172,
				"seek": 8349,
				"start": 482.83844,
				"end": 487.99844,
				"text": " so every time you're not reconstructing the prompt in the same way as close to possible",
				"tokens": [
					51522,
					370,
					633,
					565,
					291,
					434,
					406,
					31499,
					278,
					264,
					12391,
					294,
					264,
					912,
					636,
					382,
					1998,
					281,
					1944,
					51780
				],
				"temperature": 0,
				"avg_logprob": -0.06759493,
				"compression_ratio": 2.0758123,
				"no_speech_prob": 1.5432419e-12
			},
			{
				"id": 173,
				"seek": 11179,
				"start": 487.99844,
				"end": 493.79846,
				"text": " you are basically getting a miss gemini i think i think the way open ai does caching",
				"tokens": [
					50365,
					291,
					366,
					1936,
					1242,
					257,
					1713,
					7173,
					3812,
					741,
					519,
					741,
					519,
					264,
					636,
					1269,
					9783,
					775,
					269,
					2834,
					50655
				],
				"temperature": 0,
				"avg_logprob": -0.073789,
				"compression_ratio": 1.7857143,
				"no_speech_prob": 1.4272679e-12
			},
			{
				"id": 174,
				"seek": 11179,
				"start": 493.79846,
				"end": 500.77844,
				"text": " is opaque so what that means is i i think they give you almost no information on how caching",
				"tokens": [
					50655,
					307,
					42687,
					370,
					437,
					300,
					1355,
					307,
					741,
					741,
					519,
					436,
					976,
					291,
					1920,
					572,
					1589,
					322,
					577,
					269,
					2834,
					51004
				],
				"temperature": 0,
				"avg_logprob": -0.073789,
				"compression_ratio": 1.7857143,
				"no_speech_prob": 1.4272679e-12
			},
			{
				"id": 175,
				"seek": 11179,
				"start": 500.77844,
				"end": 506.85846,
				"text": " works but i think they actually what they do but it's like less control but they kind of just try",
				"tokens": [
					51004,
					1985,
					457,
					741,
					519,
					436,
					767,
					437,
					436,
					360,
					457,
					309,
					311,
					411,
					1570,
					1969,
					457,
					436,
					733,
					295,
					445,
					853,
					51308
				],
				"temperature": 0,
				"avg_logprob": -0.073789,
				"compression_ratio": 1.7857143,
				"no_speech_prob": 1.4272679e-12
			},
			{
				"id": 176,
				"seek": 11179,
				"start": 506.85846,
				"end": 511.03845,
				"text": " to do it for you this is a lot of them just like do a prefix and try to guess how to do it i didn't",
				"tokens": [
					51308,
					281,
					360,
					309,
					337,
					291,
					341,
					307,
					257,
					688,
					295,
					552,
					445,
					411,
					360,
					257,
					46969,
					293,
					853,
					281,
					2041,
					577,
					281,
					360,
					309,
					741,
					994,
					380,
					51517
				],
				"temperature": 0,
				"avg_logprob": -0.073789,
				"compression_ratio": 1.7857143,
				"no_speech_prob": 1.4272679e-12
			},
			{
				"id": 177,
				"seek": 13483,
				"start": 511.03845,
				"end": 514.89844,
				"text": " But now they added a prompt cache key.",
				"tokens": [
					50365,
					583,
					586,
					436,
					3869,
					257,
					12391,
					19459,
					2141,
					13,
					50558
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 178,
				"seek": 13483,
				"start": 516.27844,
				"end": 518.97845,
				"text": " So it's like prompt cache.",
				"tokens": [
					50627,
					407,
					309,
					311,
					411,
					12391,
					19459,
					13,
					50762
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 179,
				"seek": 13483,
				"start": 519.27844,
				"end": 521.95844,
				"text": " And the reason that they do this, I'm certain...",
				"tokens": [
					50777,
					400,
					264,
					1778,
					300,
					436,
					360,
					341,
					11,
					286,
					478,
					1629,
					485,
					50911
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 180,
				"seek": 13483,
				"start": 521.95844,
				"end": 523.33844,
				"text": " Replace the user.",
				"tokens": [
					50911,
					1300,
					6742,
					264,
					4195,
					13,
					50980
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 181,
				"seek": 13483,
				"start": 523.4984,
				"end": 524.09845,
				"text": " I don't know what this is.",
				"tokens": [
					50988,
					286,
					500,
					380,
					458,
					437,
					341,
					307,
					13,
					51018
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 182,
				"seek": 13483,
				"start": 524.5585,
				"end": 526.3784,
				"text": " But I'm pretty certain that the reason they do this",
				"tokens": [
					51041,
					583,
					286,
					478,
					1238,
					1629,
					300,
					264,
					1778,
					436,
					360,
					341,
					51132
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 183,
				"seek": 13483,
				"start": 526.3784,
				"end": 527.6184,
				"text": " is people just want it more control.",
				"tokens": [
					51132,
					307,
					561,
					445,
					528,
					309,
					544,
					1969,
					13,
					51194
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 184,
				"seek": 13483,
				"start": 527.77844,
				"end": 529.6184,
				"text": " Because if you're able to have access to this",
				"tokens": [
					51202,
					1436,
					498,
					291,
					434,
					1075,
					281,
					362,
					2105,
					281,
					341,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 185,
				"seek": 13483,
				"start": 529.6184,
				"end": 530.39844,
				"text": " and you can control this,",
				"tokens": [
					51294,
					293,
					291,
					393,
					1969,
					341,
					11,
					51333
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 186,
				"seek": 13483,
				"start": 530.4385,
				"end": 532.67847,
				"text": " you can just get better hits all the time.",
				"tokens": [
					51335,
					291,
					393,
					445,
					483,
					1101,
					8664,
					439,
					264,
					565,
					13,
					51447
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 187,
				"seek": 13483,
				"start": 534.15845,
				"end": 536.17847,
				"text": " But it is important to go read this stuff.",
				"tokens": [
					51521,
					583,
					309,
					307,
					1021,
					281,
					352,
					1401,
					341,
					1507,
					13,
					51622
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 188,
				"seek": 13483,
				"start": 536.3185,
				"end": 537.4385,
				"text": " And the only way you can actually know",
				"tokens": [
					51629,
					400,
					264,
					787,
					636,
					291,
					393,
					767,
					458,
					51685
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 189,
				"seek": 13483,
				"start": 537.4385,
				"end": 539.03845,
				"text": " if you're hitting this is just by looking at the usage.",
				"tokens": [
					51685,
					498,
					291,
					434,
					8850,
					341,
					307,
					445,
					538,
					1237,
					412,
					264,
					14924,
					13,
					51765
				],
				"temperature": 0,
				"avg_logprob": -0.21040949,
				"compression_ratio": 1.8284671,
				"no_speech_prob": 1.728267e-12
			},
			{
				"id": 190,
				"seek": 16283,
				"start": 539.03845,
				"end": 545.4385,
				"text": " there's no other way to really know um along unless you own the inference and most people",
				"tokens": [
					50365,
					456,
					311,
					572,
					661,
					636,
					281,
					534,
					458,
					1105,
					2051,
					5969,
					291,
					1065,
					264,
					38253,
					293,
					881,
					561,
					50685
				],
				"temperature": 0,
				"avg_logprob": -0.079331845,
				"compression_ratio": 1.8190476,
				"no_speech_prob": 1.4495668e-12
			},
			{
				"id": 191,
				"seek": 16283,
				"start": 545.4385,
				"end": 549.73846,
				"text": " don't own inference as far as i know um and most people probably shouldn't own inference as well",
				"tokens": [
					50685,
					500,
					380,
					1065,
					38253,
					382,
					1400,
					382,
					741,
					458,
					1105,
					293,
					881,
					561,
					1391,
					4659,
					380,
					1065,
					38253,
					382,
					731,
					50900
				],
				"temperature": 0,
				"avg_logprob": -0.079331845,
				"compression_ratio": 1.8190476,
				"no_speech_prob": 1.4495668e-12
			},
			{
				"id": 192,
				"seek": 16283,
				"start": 549.73846,
				"end": 556.89844,
				"text": " gemini is probably the most flexible thing i have seen for how they cache and eugene is right on",
				"tokens": [
					50900,
					7173,
					3812,
					307,
					1391,
					264,
					881,
					11358,
					551,
					741,
					362,
					1612,
					337,
					577,
					436,
					19459,
					293,
					308,
					33103,
					307,
					558,
					322,
					51258
				],
				"temperature": 0,
				"avg_logprob": -0.079331845,
				"compression_ratio": 1.8190476,
				"no_speech_prob": 1.4495668e-12
			},
			{
				"id": 193,
				"seek": 16283,
				"start": 556.89844,
				"end": 562.9385,
				"text": " that which is if you actually go look into how to get caches like you do it you do some crap to go",
				"tokens": [
					51258,
					300,
					597,
					307,
					498,
					291,
					767,
					352,
					574,
					666,
					577,
					281,
					483,
					269,
					13272,
					411,
					291,
					360,
					309,
					291,
					360,
					512,
					12426,
					281,
					352,
					51560
				],
				"temperature": 0,
				"avg_logprob": -0.079331845,
				"compression_ratio": 1.8190476,
				"no_speech_prob": 1.4495668e-12
			},
			{
				"id": 194,
				"seek": 18673,
				"start": 562.9385,
				"end": 569.6384,
				"text": " write this code but the trade-off of doing this crap is you do get to go get complete control over",
				"tokens": [
					50365,
					2464,
					341,
					3089,
					457,
					264,
					4923,
					12,
					4506,
					295,
					884,
					341,
					12426,
					307,
					291,
					360,
					483,
					281,
					352,
					483,
					3566,
					1969,
					670,
					50700
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 195,
				"seek": 18673,
				"start": 569.6384,
				"end": 576.1184,
				"text": " what it's doing and you get to go tell it this and that helps you as a developer but it also",
				"tokens": [
					50700,
					437,
					309,
					311,
					884,
					293,
					291,
					483,
					281,
					352,
					980,
					309,
					341,
					293,
					300,
					3665,
					291,
					382,
					257,
					10754,
					457,
					309,
					611,
					51024
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 196,
				"seek": 18673,
				"start": 576.1184,
				"end": 580.91846,
				"text": " hurts you because you have to go build this stuff yourself you also have to manage the detail on",
				"tokens": [
					51024,
					11051,
					291,
					570,
					291,
					362,
					281,
					352,
					1322,
					341,
					1507,
					1803,
					291,
					611,
					362,
					281,
					3067,
					264,
					2607,
					322,
					51264
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 197,
				"seek": 18673,
				"start": 580.91846,
				"end": 583.89844,
				"text": " everything automatically so there's always a trade-off in how much engineering effort you",
				"tokens": [
					51264,
					1203,
					6772,
					370,
					456,
					311,
					1009,
					257,
					4923,
					12,
					4506,
					294,
					577,
					709,
					7043,
					4630,
					291,
					51413
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 198,
				"seek": 18673,
				"start": 583.89844,
				"end": 588.51843,
				"text": " want to put in versus what you want to do manually what you want to do automatically and generally",
				"tokens": [
					51413,
					528,
					281,
					829,
					294,
					5717,
					437,
					291,
					528,
					281,
					360,
					16945,
					437,
					291,
					528,
					281,
					360,
					6772,
					293,
					5101,
					51644
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 199,
				"seek": 18673,
				"start": 588.51843,
				"end": 591.65845,
				"text": " if you do something manually you will always get better throughput if you know what you're doing",
				"tokens": [
					51644,
					498,
					291,
					360,
					746,
					16945,
					291,
					486,
					1009,
					483,
					1101,
					44629,
					498,
					291,
					458,
					437,
					291,
					434,
					884,
					51801
				],
				"temperature": 0,
				"avg_logprob": -0.045130644,
				"compression_ratio": 2.006993,
				"no_speech_prob": 1.6234193e-12
			},
			{
				"id": 200,
				"seek": 21545,
				"start": 591.65845,
				"end": 593.51843,
				"text": " than someone that does it automatically.",
				"tokens": [
					50365,
					813,
					1580,
					300,
					775,
					309,
					6772,
					13,
					50458
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 201,
				"seek": 21545,
				"start": 594.21844,
				"end": 595.5784,
				"text": " But someone that does it automatically",
				"tokens": [
					50493,
					583,
					1580,
					300,
					775,
					309,
					6772,
					50561
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 202,
				"seek": 21545,
				"start": 595.5784,
				"end": 596.83844,
				"text": " will always get better than someone",
				"tokens": [
					50561,
					486,
					1009,
					483,
					1101,
					813,
					1580,
					50624
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 203,
				"seek": 21545,
				"start": 596.83844,
				"end": 598.0585,
				"text": " that doesn't know what they're doing manually.",
				"tokens": [
					50624,
					300,
					1177,
					380,
					458,
					437,
					436,
					434,
					884,
					16945,
					13,
					50685
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 204,
				"seek": 21545,
				"start": 599.91846,
				"end": 601.51843,
				"text": " Because if you blow the cache out of time,",
				"tokens": [
					50778,
					1436,
					498,
					291,
					6327,
					264,
					19459,
					484,
					295,
					565,
					11,
					50858
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 205,
				"seek": 21545,
				"start": 601.5784,
				"end": 602.0784,
				"text": " it'll be worse.",
				"tokens": [
					50861,
					309,
					603,
					312,
					5324,
					13,
					50886
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 206,
				"seek": 21545,
				"start": 602.9385,
				"end": 604.4385,
				"text": " Okay, so if you know what you're doing,",
				"tokens": [
					50929,
					1033,
					11,
					370,
					498,
					291,
					458,
					437,
					291,
					434,
					884,
					11,
					51004
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 207,
				"seek": 21545,
				"start": 604.4984,
				"end": 605.2584,
				"text": " use the cache keys.",
				"tokens": [
					51007,
					764,
					264,
					19459,
					9317,
					13,
					51045
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 208,
				"seek": 21545,
				"start": 605.39844,
				"end": 607.51843,
				"text": " If you don't, hope for the prefix things.",
				"tokens": [
					51052,
					759,
					291,
					500,
					380,
					11,
					1454,
					337,
					264,
					46969,
					721,
					13,
					51158
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 209,
				"seek": 21545,
				"start": 607.6384,
				"end": 609.0784,
				"text": " But don't try to mess with the cache keys",
				"tokens": [
					51164,
					583,
					500,
					380,
					853,
					281,
					2082,
					365,
					264,
					19459,
					9317,
					51236
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 210,
				"seek": 21545,
				"start": 609.0784,
				"end": 611.0784,
				"text": " if you're not willing to go learn",
				"tokens": [
					51236,
					498,
					291,
					434,
					406,
					4950,
					281,
					352,
					1466,
					51336
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 211,
				"seek": 21545,
				"start": 611.0784,
				"end": 612.0784,
				"text": " how these systems work.",
				"tokens": [
					51336,
					577,
					613,
					3652,
					589,
					13,
					51386
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 212,
				"seek": 21545,
				"start": 612.77844,
				"end": 613.09845,
				"text": " Exactly.",
				"tokens": [
					51421,
					7587,
					13,
					51437
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 213,
				"seek": 21545,
				"start": 613.39844,
				"end": 615.45844,
				"text": " And lastly, if you don't have the time",
				"tokens": [
					51452,
					400,
					16386,
					11,
					498,
					291,
					500,
					380,
					362,
					264,
					565,
					51555
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 214,
				"seek": 21545,
				"start": 615.45844,
				"end": 616.23846,
				"text": " to go learn how they work.",
				"tokens": [
					51555,
					281,
					352,
					1466,
					577,
					436,
					589,
					13,
					51594
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 215,
				"seek": 21545,
				"start": 616.41846,
				"end": 617.79846,
				"text": " It's not a matter of like you don't want to.",
				"tokens": [
					51603,
					467,
					311,
					406,
					257,
					1871,
					295,
					411,
					291,
					500,
					380,
					528,
					281,
					13,
					51672
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 216,
				"seek": 21545,
				"start": 617.85846,
				"end": 618.83844,
				"text": " Sometimes you just don't have the time.",
				"tokens": [
					51675,
					4803,
					291,
					445,
					500,
					380,
					362,
					264,
					565,
					13,
					51724
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 217,
				"seek": 21545,
				"start": 618.91846,
				"end": 620.6384,
				"text": " We're all balancing a lot of things at once.",
				"tokens": [
					51728,
					492,
					434,
					439,
					22495,
					257,
					688,
					295,
					721,
					412,
					1564,
					13,
					51814
				],
				"temperature": 0,
				"avg_logprob": -0.1639308,
				"compression_ratio": 2.0128205,
				"no_speech_prob": 1.5312326e-12
			},
			{
				"id": 218,
				"seek": 24545,
				"start": 621.65845,
				"end": 624.27844,
				"text": " But what it comes down to is if your cache isn't working,",
				"tokens": [
					50365,
					583,
					437,
					309,
					1487,
					760,
					281,
					307,
					498,
					428,
					19459,
					1943,
					380,
					1364,
					11,
					50496
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 219,
				"seek": 24545,
				"start": 624.53845,
				"end": 625.89844,
				"text": " you now know why it isn't working.",
				"tokens": [
					50509,
					291,
					586,
					458,
					983,
					309,
					1943,
					380,
					1364,
					13,
					50577
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 220,
				"seek": 24545,
				"start": 626.01843,
				"end": 628.0784,
				"text": " It's because your prefix key is not continuous",
				"tokens": [
					50583,
					467,
					311,
					570,
					428,
					46969,
					2141,
					307,
					406,
					10957,
					50686
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 221,
				"seek": 24545,
				"start": 628.0784,
				"end": 629.41846,
				"text": " and it's breaking somehow",
				"tokens": [
					50686,
					293,
					309,
					311,
					7697,
					6063,
					50753
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 222,
				"seek": 24545,
				"start": 629.41846,
				"end": 631.1184,
				"text": " when whatever system you're using can break it.",
				"tokens": [
					50753,
					562,
					2035,
					1185,
					291,
					434,
					1228,
					393,
					1821,
					309,
					13,
					50838
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 223,
				"seek": 24545,
				"start": 631.17847,
				"end": 632.6984,
				"text": " You're getting a cache miss for that reason.",
				"tokens": [
					50841,
					509,
					434,
					1242,
					257,
					19459,
					1713,
					337,
					300,
					1778,
					13,
					50917
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 224,
				"seek": 24545,
				"start": 634.3185,
				"end": 635.9385,
				"text": " I think the next thing I want to talk about",
				"tokens": [
					50998,
					286,
					519,
					264,
					958,
					551,
					286,
					528,
					281,
					751,
					466,
					51079
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 225,
				"seek": 24545,
				"start": 635.9385,
				"end": 640.29846,
				"text": " is actually talking about this one.",
				"tokens": [
					51079,
					307,
					767,
					1417,
					466,
					341,
					472,
					13,
					51297
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 226,
				"seek": 24545,
				"start": 640.59845,
				"end": 642.1384,
				"text": " We talked about recitation a little bit.",
				"tokens": [
					51312,
					492,
					2825,
					466,
					850,
					4614,
					257,
					707,
					857,
					13,
					51389
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 227,
				"seek": 24545,
				"start": 642.29846,
				"end": 644.01843,
				"text": " So like recite your systems.",
				"tokens": [
					51397,
					407,
					411,
					39434,
					428,
					3652,
					13,
					51483
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 228,
				"seek": 24545,
				"start": 644.17847,
				"end": 646.95844,
				"text": " Think about what makes sense in your workflows to recite this.",
				"tokens": [
					51491,
					6557,
					466,
					437,
					1669,
					2020,
					294,
					428,
					43461,
					281,
					39434,
					341,
					13,
					51630
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 229,
				"seek": 24545,
				"start": 647.83844,
				"end": 649.6184,
				"text": " And it can do all sorts of different things.",
				"tokens": [
					51674,
					400,
					309,
					393,
					360,
					439,
					7527,
					295,
					819,
					721,
					13,
					51763
				],
				"temperature": 0,
				"avg_logprob": -0.15200898,
				"compression_ratio": 1.7731959,
				"no_speech_prob": 8.5204006e-13
			},
			{
				"id": 230,
				"seek": 27341,
				"start": 649.6184,
				"end": 663.7584,
				"text": " So for example, in the example that they gave over here, what they're showing their visual is, hey, instead of producing context one and doing this, what I will say is before even producing action four, I will inject an objective into the statement.",
				"tokens": [
					50365,
					407,
					337,
					1365,
					11,
					294,
					264,
					1365,
					300,
					436,
					2729,
					670,
					510,
					11,
					437,
					436,
					434,
					4099,
					641,
					5056,
					307,
					11,
					4177,
					11,
					2602,
					295,
					10501,
					4319,
					472,
					293,
					884,
					341,
					11,
					437,
					286,
					486,
					584,
					307,
					949,
					754,
					10501,
					3069,
					1451,
					11,
					286,
					486,
					10711,
					364,
					10024,
					666,
					264,
					5629,
					13,
					51072
				],
				"temperature": 0,
				"avg_logprob": -0.114067614,
				"compression_ratio": 1.7427536,
				"no_speech_prob": 1.4272533e-12
			},
			{
				"id": 231,
				"seek": 27341,
				"start": 664.59845,
				"end": 667.0784,
				"text": " And now I can go and produce objective four.",
				"tokens": [
					51114,
					400,
					586,
					286,
					393,
					352,
					293,
					5258,
					10024,
					1451,
					13,
					51238
				],
				"temperature": 0,
				"avg_logprob": -0.114067614,
				"compression_ratio": 1.7427536,
				"no_speech_prob": 1.4272533e-12
			},
			{
				"id": 232,
				"seek": 27341,
				"start": 667.47845,
				"end": 670.5585,
				"text": " So there's different ways to go repeat yourself in the prompt.",
				"tokens": [
					51258,
					407,
					456,
					311,
					819,
					2098,
					281,
					352,
					7149,
					1803,
					294,
					264,
					12391,
					13,
					51412
				],
				"temperature": 0,
				"avg_logprob": -0.114067614,
				"compression_ratio": 1.7427536,
				"no_speech_prob": 1.4272533e-12
			},
			{
				"id": 233,
				"seek": 27341,
				"start": 670.95844,
				"end": 672.6184,
				"text": " There's one is just add it to the very end.",
				"tokens": [
					51432,
					821,
					311,
					472,
					307,
					445,
					909,
					309,
					281,
					264,
					588,
					917,
					13,
					51515
				],
				"temperature": 0,
				"avg_logprob": -0.114067614,
				"compression_ratio": 1.7427536,
				"no_speech_prob": 1.4272533e-12
			},
			{
				"id": 234,
				"seek": 27341,
				"start": 673.09845,
				"end": 677.95844,
				"text": " One is take your existing prompt and go inject it somewhere else in my history.",
				"tokens": [
					51539,
					1485,
					307,
					747,
					428,
					6741,
					12391,
					293,
					352,
					10711,
					309,
					4079,
					1646,
					294,
					452,
					2503,
					13,
					51782
				],
				"temperature": 0,
				"avg_logprob": -0.114067614,
				"compression_ratio": 1.7427536,
				"no_speech_prob": 1.4272533e-12
			},
			{
				"id": 235,
				"seek": 30175,
				"start": 677.95844,
				"end": 682.27844,
				"text": " and yes this is kind of contradictory to what we just talked about about breaking the cache key",
				"tokens": [
					50365,
					293,
					2086,
					341,
					307,
					733,
					295,
					49555,
					281,
					437,
					321,
					445,
					2825,
					466,
					466,
					7697,
					264,
					19459,
					2141,
					50581
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 236,
				"seek": 30175,
				"start": 682.27844,
				"end": 686.8185,
				"text": " but you're doing this deliberately because you're trying to get accuracy so you're like okay cool I",
				"tokens": [
					50581,
					457,
					291,
					434,
					884,
					341,
					23506,
					570,
					291,
					434,
					1382,
					281,
					483,
					14170,
					370,
					291,
					434,
					411,
					1392,
					1627,
					286,
					50808
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 237,
				"seek": 30175,
				"start": 686.8185,
				"end": 690.9385,
				"text": " will lose the cache on this part of my prompt I will still preserve the cache on everything up",
				"tokens": [
					50808,
					486,
					3624,
					264,
					19459,
					322,
					341,
					644,
					295,
					452,
					12391,
					286,
					486,
					920,
					15665,
					264,
					19459,
					322,
					1203,
					493,
					51014
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 238,
				"seek": 30175,
				"start": 690.9385,
				"end": 695.5585,
				"text": " until here but I will do that in favor of getting slightly better accuracy and helping the model",
				"tokens": [
					51014,
					1826,
					510,
					457,
					286,
					486,
					360,
					300,
					294,
					2294,
					295,
					1242,
					4748,
					1101,
					14170,
					293,
					4315,
					264,
					2316,
					51245
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 239,
				"seek": 30175,
				"start": 695.5585,
				"end": 702.0585,
				"text": " understand what's going on and the premise here is that's objective it may be obvious to the model",
				"tokens": [
					51245,
					1223,
					437,
					311,
					516,
					322,
					293,
					264,
					22045,
					510,
					307,
					300,
					311,
					10024,
					309,
					815,
					312,
					6322,
					281,
					264,
					2316,
					51570
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 240,
				"seek": 30175,
				"start": 702.0585,
				"end": 706.95844,
				"text": " at this point that action three was best but when it's making action four the model might forget that",
				"tokens": [
					51570,
					412,
					341,
					935,
					300,
					3069,
					1045,
					390,
					1151,
					457,
					562,
					309,
					311,
					1455,
					3069,
					1451,
					264,
					2316,
					1062,
					2870,
					300,
					51815
				],
				"temperature": 0,
				"avg_logprob": -0.061689787,
				"compression_ratio": 1.9029126,
				"no_speech_prob": 1.2159027e-12
			},
			{
				"id": 241,
				"seek": 33075,
				"start": 706.95844,
				"end": 709.7584,
				"text": " because of this objective, action three was selected here.",
				"tokens": [
					50365,
					570,
					295,
					341,
					10024,
					11,
					3069,
					1045,
					390,
					8209,
					510,
					13,
					50505
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 242,
				"seek": 33075,
				"start": 709.95844,
				"end": 712.09845,
				"text": " So I'm repeating that here's objectives",
				"tokens": [
					50515,
					407,
					286,
					478,
					18617,
					300,
					510,
					311,
					15961,
					50622
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 243,
				"seek": 33075,
				"start": 712.09845,
				"end": 713.6184,
				"text": " and this is why you picked action three.",
				"tokens": [
					50622,
					293,
					341,
					307,
					983,
					291,
					6183,
					3069,
					1045,
					13,
					50698
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 244,
				"seek": 33075,
				"start": 714.0585,
				"end": 715.01843,
				"text": " Now pick action four.",
				"tokens": [
					50720,
					823,
					1888,
					3069,
					1451,
					13,
					50768
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 245,
				"seek": 33075,
				"start": 715.8784,
				"end": 716.91846,
				"text": " I can also just say-",
				"tokens": [
					50811,
					286,
					393,
					611,
					445,
					584,
					12,
					50863
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 246,
				"seek": 33075,
				"start": 716.91846,
				"end": 716.9385,
				"text": " Zoom.",
				"tokens": [
					50863,
					13453,
					13,
					50864
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 247,
				"seek": 33075,
				"start": 717.27844,
				"end": 717.5585,
				"text": " Go ahead.",
				"tokens": [
					50881,
					1037,
					2286,
					13,
					50895
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 248,
				"seek": 33075,
				"start": 718.29846,
				"end": 719.95844,
				"text": " Has anyone ever implemented this",
				"tokens": [
					50932,
					8646,
					2878,
					1562,
					12270,
					341,
					51015
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 249,
				"seek": 33075,
				"start": 719.95844,
				"end": 723.59845,
				"text": " like kind of deterministic injection of a to-do list",
				"tokens": [
					51015,
					411,
					733,
					295,
					15957,
					3142,
					22873,
					295,
					257,
					281,
					12,
					2595,
					1329,
					51197
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 250,
				"seek": 33075,
				"start": 723.59845,
				"end": 726.1384,
				"text": " into the context window as it's going?",
				"tokens": [
					51197,
					666,
					264,
					4319,
					4910,
					382,
					309,
					311,
					516,
					30,
					51324
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 251,
				"seek": 33075,
				"start": 726.71844,
				"end": 728.29846,
				"text": " I would be surprised if Cloud Code",
				"tokens": [
					51353,
					286,
					576,
					312,
					6100,
					498,
					8061,
					15549,
					51432
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 252,
				"seek": 33075,
				"start": 728.29846,
				"end": 729.4385,
				"text": " doesn't do something like this.",
				"tokens": [
					51432,
					1177,
					380,
					360,
					746,
					411,
					341,
					13,
					51489
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 253,
				"seek": 33075,
				"start": 729.91846,
				"end": 733.41846,
				"text": " That's what I'm messing with that idea right now",
				"tokens": [
					51513,
					663,
					311,
					437,
					286,
					478,
					23258,
					365,
					300,
					1558,
					558,
					586,
					51688
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 254,
				"seek": 33075,
				"start": 733.41846,
				"end": 734.47845,
				"text": " is what I-",
				"tokens": [
					51688,
					307,
					437,
					286,
					12,
					51741
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 255,
				"seek": 33075,
				"start": 734.47845,
				"end": 734.65845,
				"text": " Cool.",
				"tokens": [
					51741,
					8561,
					13,
					51750
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 256,
				"seek": 33075,
				"start": 735.09845,
				"end": 735.97845,
				"text": " Yeah, doing.",
				"tokens": [
					51772,
					865,
					11,
					884,
					13,
					51816
				],
				"temperature": 0,
				"avg_logprob": -0.2683021,
				"compression_ratio": 1.5952381,
				"no_speech_prob": 1.854147e-12
			},
			{
				"id": 257,
				"seek": 36075,
				"start": 736.95844,
				"end": 739.95844,
				"text": " Cool.",
				"tokens": [
					50365,
					8561,
					13,
					50515
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 258,
				"seek": 36075,
				"start": 739.95844,
				"end": 742.95844,
				"text": " So this one I think is really straightforward",
				"tokens": [
					50515,
					407,
					341,
					472,
					286,
					519,
					307,
					534,
					15325,
					50665
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 259,
				"seek": 36075,
				"start": 742.95844,
				"end": 743.95844,
				"text": " and easy to iterate on.",
				"tokens": [
					50665,
					293,
					1858,
					281,
					44497,
					322,
					13,
					50715
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 260,
				"seek": 36075,
				"start": 743.95844,
				"end": 745.95844,
				"text": " So if you're finding that you have really, really long tool call",
				"tokens": [
					50715,
					407,
					498,
					291,
					434,
					5006,
					300,
					291,
					362,
					534,
					11,
					534,
					938,
					2290,
					818,
					50815
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 261,
				"seek": 36075,
				"start": 745.95844,
				"end": 748.95844,
				"text": " sequences and they're diverting from or drifting from the main action,",
				"tokens": [
					50815,
					22978,
					293,
					436,
					434,
					18558,
					783,
					490,
					420,
					37973,
					490,
					264,
					2135,
					3069,
					11,
					50965
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 262,
				"seek": 36075,
				"start": 748.95844,
				"end": 750.95844,
				"text": " just like add some repetition there.",
				"tokens": [
					50965,
					445,
					411,
					909,
					512,
					30432,
					456,
					13,
					51065
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 263,
				"seek": 36075,
				"start": 750.95844,
				"end": 753.95844,
				"text": " And you might just get free throughput without having to do much work.",
				"tokens": [
					51065,
					400,
					291,
					1062,
					445,
					483,
					1737,
					44629,
					1553,
					1419,
					281,
					360,
					709,
					589,
					13,
					51215
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 264,
				"seek": 36075,
				"start": 753.95844,
				"end": 755.95844,
				"text": " And I think that's the cool thing about this trick.",
				"tokens": [
					51215,
					400,
					286,
					519,
					300,
					311,
					264,
					1627,
					551,
					466,
					341,
					4282,
					13,
					51315
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 265,
				"seek": 36075,
				"start": 755.95844,
				"end": 757.95844,
				"text": " There's almost no effort.",
				"tokens": [
					51315,
					821,
					311,
					1920,
					572,
					4630,
					13,
					51415
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 266,
				"seek": 36075,
				"start": 759.95844,
				"end": 760.95844,
				"text": " I want to talk about this.",
				"tokens": [
					51515,
					286,
					528,
					281,
					751,
					466,
					341,
					13,
					51565
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 267,
				"seek": 36075,
				"start": 760.95844,
				"end": 762.95844,
				"text": " This is something that we've talked about a lot,",
				"tokens": [
					51565,
					639,
					307,
					746,
					300,
					321,
					600,
					2825,
					466,
					257,
					688,
					11,
					51665
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 268,
				"seek": 36075,
				"start": 762.95844,
				"end": 764.95844,
				"text": " which is a lot of times when in an agent loop,",
				"tokens": [
					51665,
					597,
					307,
					257,
					688,
					295,
					1413,
					562,
					294,
					364,
					9461,
					6367,
					11,
					51765
				],
				"temperature": 0,
				"avg_logprob": -0.17695828,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2592079e-12
			},
			{
				"id": 269,
				"seek": 764,
				"start": 764.95844,
				"end": 774.9969,
				"text": " in an agent loop it try and go do many things and then it go ahead and go correct things along the way And in this case I think what they found",
				"tokens": [
					50365,
					294,
					364,
					9461,
					6367,
					11,
					309,
					603,
					853,
					293,
					352,
					360,
					867,
					721,
					11,
					50487,
					50487,
					293,
					550,
					309,
					603,
					352,
					2286,
					293,
					352,
					3006,
					721,
					2051,
					264,
					636,
					13,
					50659,
					50715,
					400,
					294,
					341,
					1389,
					11,
					286,
					519,
					437,
					436,
					1352,
					50865
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 270,
				"seek": 764,
				"start": 774.9969,
				"end": 777.15686,
				"text": " is they found in their scenario keeping,",
				"tokens": [
					50865,
					307,
					436,
					1352,
					294,
					641,
					9005,
					5145,
					11,
					50973
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 271,
				"seek": 764,
				"start": 777.15686,
				"end": 778.5569,
				"text": " if they're running two things in parallel,",
				"tokens": [
					50973,
					498,
					436,
					434,
					2614,
					732,
					721,
					294,
					8952,
					11,
					51043
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 272,
				"seek": 764,
				"start": 778.5569,
				"end": 779.9369,
				"text": " getting all the predictions,",
				"tokens": [
					51043,
					1242,
					439,
					264,
					21264,
					11,
					51112
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 273,
				"seek": 764,
				"start": 779.9369,
				"end": 782.3169,
				"text": " actually having the incorrect stuff helps them.",
				"tokens": [
					51112,
					767,
					1419,
					264,
					18424,
					1507,
					3665,
					552,
					13,
					51231
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 274,
				"seek": 764,
				"start": 783.4969,
				"end": 785.3769,
				"text": " I would just go experiment and try this.",
				"tokens": [
					51290,
					286,
					576,
					445,
					352,
					5120,
					293,
					853,
					341,
					13,
					51384
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 275,
				"seek": 764,
				"start": 785.3769,
				"end": 788.21686,
				"text": " What we have found is having the incorrect stuff hurts",
				"tokens": [
					51384,
					708,
					321,
					362,
					1352,
					307,
					1419,
					264,
					18424,
					1507,
					11051,
					51526
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 276,
				"seek": 764,
				"start": 788.21686,
				"end": 790.7369,
				"text": " in some scenarios, but it's possible in their use case",
				"tokens": [
					51526,
					294,
					512,
					15077,
					11,
					457,
					309,
					311,
					1944,
					294,
					641,
					764,
					1389,
					51652
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 277,
				"seek": 764,
				"start": 790.7369,
				"end": 792.5169,
				"text": " because they're running like on average,",
				"tokens": [
					51652,
					570,
					436,
					434,
					2614,
					411,
					322,
					4274,
					11,
					51741
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 278,
				"seek": 764,
				"start": 792.5169,
				"end": 793.9969,
				"text": " at least 50 tool calls,",
				"tokens": [
					51741,
					412,
					1935,
					2625,
					2290,
					5498,
					11,
					51815
				],
				"temperature": 0,
				"avg_logprob": -0.14201476,
				"compression_ratio": 1.8426573,
				"no_speech_prob": 1.15581e-12
			},
			{
				"id": 279,
				"seek": 3664,
				"start": 793.9969,
				"end": 795.6369,
				"text": " that perhaps having incorrect sequences",
				"tokens": [
					50365,
					300,
					4317,
					1419,
					18424,
					22978,
					50447
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 280,
				"seek": 3664,
				"start": 795.6369,
				"end": 796.8969,
				"text": " helps the model correct itself",
				"tokens": [
					50447,
					3665,
					264,
					2316,
					3006,
					2564,
					50510
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 281,
				"seek": 3664,
				"start": 796.8969,
				"end": 798.2369,
				"text": " and not repeat the same behavior",
				"tokens": [
					50510,
					293,
					406,
					7149,
					264,
					912,
					5223,
					50577
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 282,
				"seek": 3664,
				"start": 798.2369,
				"end": 800.7169,
				"text": " in the case of very repetitive actions.",
				"tokens": [
					50577,
					294,
					264,
					1389,
					295,
					588,
					29404,
					5909,
					13,
					50701
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 283,
				"seek": 3664,
				"start": 801.2169,
				"end": 801.6169,
				"text": " Eugene, you've got-",
				"tokens": [
					50726,
					37059,
					11,
					291,
					600,
					658,
					12,
					50746
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 284,
				"seek": 3664,
				"start": 801.6169,
				"end": 804.97687,
				"text": " It's funny because this is contrary to advice we give a lot,",
				"tokens": [
					50746,
					467,
					311,
					4074,
					570,
					341,
					307,
					19506,
					281,
					5192,
					321,
					976,
					257,
					688,
					11,
					50914
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 285,
				"seek": 3664,
				"start": 805.0769,
				"end": 807.5569,
				"text": " which is like be smart about sometimes pulling those out",
				"tokens": [
					50919,
					597,
					307,
					411,
					312,
					4069,
					466,
					2171,
					8407,
					729,
					484,
					51043
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 286,
				"seek": 3664,
				"start": 807.5569,
				"end": 808.8769,
				"text": " because it becomes very noisy.",
				"tokens": [
					51043,
					570,
					309,
					3643,
					588,
					24518,
					13,
					51109
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 287,
				"seek": 3664,
				"start": 809.9369,
				"end": 811.1169,
				"text": " Yeah, Eugene, you've got a question.",
				"tokens": [
					51162,
					865,
					11,
					37059,
					11,
					291,
					600,
					658,
					257,
					1168,
					13,
					51221
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 288,
				"seek": 3664,
				"start": 811.6369,
				"end": 812.3769,
				"text": " Yeah, I got a question.",
				"tokens": [
					51247,
					865,
					11,
					286,
					658,
					257,
					1168,
					13,
					51284
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 289,
				"seek": 3664,
				"start": 813.15686,
				"end": 815.8769,
				"text": " Yeah, I think this one on the incorrect observations",
				"tokens": [
					51323,
					865,
					11,
					286,
					519,
					341,
					472,
					322,
					264,
					18424,
					18163,
					51459
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 290,
				"seek": 3664,
				"start": 815.8769,
				"end": 816.97687,
				"text": " is quite intuitive to me.",
				"tokens": [
					51459,
					307,
					1596,
					21769,
					281,
					385,
					13,
					51514
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 291,
				"seek": 3664,
				"start": 817.15686,
				"end": 818.97687,
				"text": " One thing that's also not so intuitive to me",
				"tokens": [
					51523,
					1485,
					551,
					300,
					311,
					611,
					406,
					370,
					21769,
					281,
					385,
					51614
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 292,
				"seek": 3664,
				"start": 818.97687,
				"end": 819.9169,
				"text": " is the tool call.",
				"tokens": [
					51614,
					307,
					264,
					2290,
					818,
					13,
					51661
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 293,
				"seek": 3664,
				"start": 820.7969,
				"end": 821.7969,
				"text": " Do you just store,",
				"tokens": [
					51705,
					1144,
					291,
					445,
					3531,
					11,
					51755
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 294,
				"seek": 3664,
				"start": 821.97687,
				"end": 823.7369,
				"text": " do you just return to the main model,",
				"tokens": [
					51764,
					360,
					291,
					445,
					2736,
					281,
					264,
					2135,
					2316,
					11,
					51852
				],
				"temperature": 0,
				"avg_logprob": -0.18106358,
				"compression_ratio": 1.8392283,
				"no_speech_prob": 1.790177e-12
			},
			{
				"id": 295,
				"seek": 6638,
				"start": 823.7369,
				"end": 829.65686,
				"text": " the main agent, the output of the tool call, or do you also keep the tool call itself?",
				"tokens": [
					50365,
					264,
					2135,
					9461,
					11,
					264,
					5598,
					295,
					264,
					2290,
					818,
					11,
					420,
					360,
					291,
					611,
					1066,
					264,
					2290,
					818,
					2564,
					30,
					50661
				],
				"temperature": 0,
				"avg_logprob": -0.17101313,
				"compression_ratio": 1.8297873,
				"no_speech_prob": 1.1786427e-12
			},
			{
				"id": 296,
				"seek": 6638,
				"start": 830.6969,
				"end": 835.5569,
				"text": " So I have different opinions on this. And I think the thing that they're saying here is,",
				"tokens": [
					50713,
					407,
					286,
					362,
					819,
					11819,
					322,
					341,
					13,
					400,
					286,
					519,
					264,
					551,
					300,
					436,
					434,
					1566,
					510,
					307,
					11,
					50956
				],
				"temperature": 0,
				"avg_logprob": -0.17101313,
				"compression_ratio": 1.8297873,
				"no_speech_prob": 1.1786427e-12
			},
			{
				"id": 297,
				"seek": 6638,
				"start": 835.6769,
				"end": 840.4169,
				"text": " so I found that when I was reading this, I think the reason that they're seeing this,",
				"tokens": [
					50962,
					370,
					286,
					1352,
					300,
					562,
					286,
					390,
					3760,
					341,
					11,
					286,
					519,
					264,
					1778,
					300,
					436,
					434,
					2577,
					341,
					11,
					51199
				],
				"temperature": 0,
				"avg_logprob": -0.17101313,
				"compression_ratio": 1.8297873,
				"no_speech_prob": 1.1786427e-12
			},
			{
				"id": 298,
				"seek": 6638,
				"start": 840.65686,
				"end": 845.3569,
				"text": " it's that often having a stat trace helps even a developer debug the problem.",
				"tokens": [
					51211,
					309,
					311,
					300,
					2049,
					1419,
					257,
					2219,
					13508,
					3665,
					754,
					257,
					10754,
					24083,
					264,
					1154,
					13,
					51446
				],
				"temperature": 0,
				"avg_logprob": -0.17101313,
				"compression_ratio": 1.8297873,
				"no_speech_prob": 1.1786427e-12
			},
			{
				"id": 299,
				"seek": 6638,
				"start": 846.6169,
				"end": 851.09686,
				"text": " And I think what they're, I think the reason, because this feels very wrong to me as well,",
				"tokens": [
					51509,
					400,
					286,
					519,
					437,
					436,
					434,
					11,
					286,
					519,
					264,
					1778,
					11,
					570,
					341,
					3417,
					588,
					2085,
					281,
					385,
					382,
					731,
					11,
					51733
				],
				"temperature": 0,
				"avg_logprob": -0.17101313,
				"compression_ratio": 1.8297873,
				"no_speech_prob": 1.1786427e-12
			},
			{
				"id": 300,
				"seek": 9374,
				"start": 851.09686,
				"end": 855.5769,
				"text": " but I think what's happening is Manus has a very broad scope of what it's trying to do",
				"tokens": [
					50365,
					457,
					286,
					519,
					437,
					311,
					2737,
					307,
					2458,
					301,
					575,
					257,
					588,
					4152,
					11923,
					295,
					437,
					309,
					311,
					1382,
					281,
					360,
					50589
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 301,
				"seek": 9374,
				"start": 855.5769,
				"end": 861.3569,
				"text": " so many times a model will divert and not do the right thing and in that scenario assuming that it",
				"tokens": [
					50589,
					370,
					867,
					1413,
					257,
					2316,
					486,
					23781,
					293,
					406,
					360,
					264,
					558,
					551,
					293,
					294,
					300,
					9005,
					11926,
					300,
					309,
					50878
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 302,
				"seek": 9374,
				"start": 861.3569,
				"end": 865.3969,
				"text": " has to do that thing again or it's somewhat related to the final goal and it might have",
				"tokens": [
					50878,
					575,
					281,
					360,
					300,
					551,
					797,
					420,
					309,
					311,
					8344,
					4077,
					281,
					264,
					2572,
					3387,
					293,
					309,
					1062,
					362,
					51080
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 303,
				"seek": 9374,
				"start": 865.3969,
				"end": 869.6769,
				"text": " to repeat that task again it's more likely the model will get it right the first time around if",
				"tokens": [
					51080,
					281,
					7149,
					300,
					5633,
					797,
					309,
					311,
					544,
					3700,
					264,
					2316,
					486,
					483,
					309,
					558,
					264,
					700,
					565,
					926,
					498,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 304,
				"seek": 9374,
				"start": 869.6769,
				"end": 873.6969,
				"text": " it can kind of look at its stack trace and say what did it take to get to the final correct",
				"tokens": [
					51294,
					309,
					393,
					733,
					295,
					574,
					412,
					1080,
					8630,
					13508,
					293,
					584,
					437,
					630,
					309,
					747,
					281,
					483,
					281,
					264,
					2572,
					3006,
					51495
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 305,
				"seek": 9374,
				"start": 873.6969,
				"end": 880.5769,
				"text": " answer that said I'm not actually yet convinced that the best way to represent the tool calls",
				"tokens": [
					51495,
					1867,
					300,
					848,
					286,
					478,
					406,
					767,
					1939,
					12561,
					300,
					264,
					1151,
					636,
					281,
					2906,
					264,
					2290,
					5498,
					51839
				],
				"temperature": 0,
				"avg_logprob": -0.06558522,
				"compression_ratio": 1.875,
				"no_speech_prob": 1.2018424e-12
			},
			{
				"id": 306,
				"seek": 12322,
				"start": 880.5769,
				"end": 882.9169,
				"text": " are actually the tool calling systems that people have.",
				"tokens": [
					50365,
					366,
					767,
					264,
					2290,
					5141,
					3652,
					300,
					561,
					362,
					13,
					50482
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 307,
				"seek": 12322,
				"start": 883.9169,
				"end": 885.03687,
				"text": " I think there's a lot more...",
				"tokens": [
					50532,
					286,
					519,
					456,
					311,
					257,
					688,
					544,
					485,
					50588
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 308,
				"seek": 12322,
				"start": 885.03687,
				"end": 885.3969,
				"text": " Really?",
				"tokens": [
					50588,
					4083,
					30,
					50606
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 309,
				"seek": 12322,
				"start": 885.9969,
				"end": 888.27686,
				"text": " You think there's a better way to represent tool calling?",
				"tokens": [
					50636,
					509,
					519,
					456,
					311,
					257,
					1101,
					636,
					281,
					2906,
					2290,
					5141,
					30,
					50750
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 310,
				"seek": 12322,
				"start": 889.1969,
				"end": 890.77686,
				"text": " Well, I don't know if that's sarcastic or not.",
				"tokens": [
					50796,
					1042,
					11,
					286,
					500,
					380,
					458,
					498,
					300,
					311,
					36836,
					2750,
					420,
					406,
					13,
					50875
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 311,
				"seek": 12322,
				"start": 890.9369,
				"end": 891.7369,
				"text": " But like even...",
				"tokens": [
					50883,
					583,
					411,
					754,
					485,
					50923
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 312,
				"seek": 12322,
				"start": 891.7369,
				"end": 893.53687,
				"text": " Sorry.",
				"tokens": [
					50923,
					4919,
					13,
					51013
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 313,
				"seek": 12322,
				"start": 894.6769,
				"end": 895.9369,
				"text": " I'm teeing you up, man.",
				"tokens": [
					51070,
					286,
					478,
					535,
					14667,
					291,
					493,
					11,
					587,
					13,
					51133
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 314,
				"seek": 12322,
				"start": 897.1969,
				"end": 898.65686,
				"text": " That I thought was really interesting.",
				"tokens": [
					51196,
					663,
					286,
					1194,
					390,
					534,
					1880,
					13,
					51269
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 315,
				"seek": 12322,
				"start": 898.97687,
				"end": 899.7169,
				"text": " Wrong Discord.",
				"tokens": [
					51285,
					28150,
					32623,
					13,
					51322
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 316,
				"seek": 12322,
				"start": 901.5969,
				"end": 902.03687,
				"text": " Sorry.",
				"tokens": [
					51416,
					4919,
					13,
					51438
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 317,
				"seek": 12322,
				"start": 903.03687,
				"end": 905.3369,
				"text": " I'm usually on a desktop monitor when I'm doing this",
				"tokens": [
					51488,
					286,
					478,
					2673,
					322,
					257,
					14502,
					6002,
					562,
					286,
					478,
					884,
					341,
					51603
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 318,
				"seek": 12322,
				"start": 905.3369,
				"end": 907.6769,
				"text": " and it's a little bit more showcase.",
				"tokens": [
					51603,
					293,
					309,
					311,
					257,
					707,
					857,
					544,
					20388,
					13,
					51720
				],
				"temperature": 0,
				"avg_logprob": -0.31998435,
				"compression_ratio": 1.6008065,
				"no_speech_prob": 1.5074405e-12
			},
			{
				"id": 319,
				"seek": 15032,
				"start": 907.6769,
				"end": 913.27686,
				"text": " I thought this was a fantastic thread here that really helped me understand kind of some of the",
				"tokens": [
					50365,
					286,
					1194,
					341,
					390,
					257,
					5456,
					7207,
					510,
					300,
					534,
					4254,
					385,
					1223,
					733,
					295,
					512,
					295,
					264,
					50645
				],
				"temperature": 0,
				"avg_logprob": -0.0846693,
				"compression_ratio": 1.6783217,
				"no_speech_prob": 1.1511856e-12
			},
			{
				"id": 320,
				"seek": 15032,
				"start": 913.27686,
				"end": 916.7969,
				"text": " implications of some of this stuff. So Prashant, who is one of the people that's been playing on",
				"tokens": [
					50645,
					16602,
					295,
					512,
					295,
					341,
					1507,
					13,
					407,
					2114,
					1299,
					394,
					11,
					567,
					307,
					472,
					295,
					264,
					561,
					300,
					311,
					668,
					2433,
					322,
					50821
				],
				"temperature": 0,
				"avg_logprob": -0.0846693,
				"compression_ratio": 1.6783217,
				"no_speech_prob": 1.1511856e-12
			},
			{
				"id": 321,
				"seek": 15032,
				"start": 916.7969,
				"end": 922.3369,
				"text": " both DSPy and BAML, learned something really interesting. What he did was he changed how DSPy",
				"tokens": [
					50821,
					1293,
					15816,
					47,
					88,
					293,
					363,
					2865,
					43,
					11,
					3264,
					746,
					534,
					1880,
					13,
					708,
					415,
					630,
					390,
					415,
					3105,
					577,
					15816,
					47,
					88,
					51098
				],
				"temperature": 0,
				"avg_logprob": -0.0846693,
				"compression_ratio": 1.6783217,
				"no_speech_prob": 1.1511856e-12
			},
			{
				"id": 322,
				"seek": 15032,
				"start": 922.3369,
				"end": 927.3569,
				"text": " represents their prompt format to be away from JSON schema and go more into like the BAML way of",
				"tokens": [
					51098,
					8855,
					641,
					12391,
					7877,
					281,
					312,
					1314,
					490,
					31828,
					34078,
					293,
					352,
					544,
					666,
					411,
					264,
					363,
					2865,
					43,
					636,
					295,
					51349
				],
				"temperature": 0,
				"avg_logprob": -0.0846693,
				"compression_ratio": 1.6783217,
				"no_speech_prob": 1.1511856e-12
			},
			{
				"id": 323,
				"seek": 15032,
				"start": 927.3569,
				"end": 931.4969,
				"text": " doing it. And what was really interesting is just a simple way of changing how you represent the",
				"tokens": [
					51349,
					884,
					309,
					13,
					400,
					437,
					390,
					534,
					1880,
					307,
					445,
					257,
					2199,
					636,
					295,
					4473,
					577,
					291,
					2906,
					264,
					51556
				],
				"temperature": 0,
				"avg_logprob": -0.0846693,
				"compression_ratio": 1.6783217,
				"no_speech_prob": 1.1511856e-12
			},
			{
				"id": 324,
				"seek": 17414,
				"start": 931.4969,
				"end": 936.3169,
				"text": " tool call for every single model by default was just better than JSON schema.",
				"tokens": [
					50365,
					2290,
					818,
					337,
					633,
					2167,
					2316,
					538,
					7576,
					390,
					445,
					1101,
					813,
					31828,
					34078,
					13,
					50606
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 325,
				"seek": 17414,
				"start": 937.4569,
				"end": 939.5969,
				"text": " But when you add the two together, it got even better.",
				"tokens": [
					50663,
					583,
					562,
					291,
					909,
					264,
					732,
					1214,
					11,
					309,
					658,
					754,
					1101,
					13,
					50770
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 326,
				"seek": 17414,
				"start": 940.1369,
				"end": 942.1369,
				"text": " And I think that goes into what Eugene is asking is like,",
				"tokens": [
					50797,
					400,
					286,
					519,
					300,
					1709,
					666,
					437,
					37059,
					307,
					3365,
					307,
					411,
					11,
					50897
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 327,
				"seek": 17414,
				"start": 942.1969,
				"end": 943.5969,
				"text": " what's the best way to represent a tool call?",
				"tokens": [
					50900,
					437,
					311,
					264,
					1151,
					636,
					281,
					2906,
					257,
					2290,
					818,
					30,
					50970
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 328,
				"seek": 17414,
				"start": 943.7969,
				"end": 945.1369,
				"text": " To be completely honest, I don't fucking know.",
				"tokens": [
					50980,
					1407,
					312,
					2584,
					3245,
					11,
					286,
					500,
					380,
					5546,
					458,
					13,
					51047
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 329,
				"seek": 17414,
				"start": 945.6369,
				"end": 949.39685,
				"text": " I have no opinions on this, but I think in general, simplicity is best.",
				"tokens": [
					51072,
					286,
					362,
					572,
					11819,
					322,
					341,
					11,
					457,
					286,
					519,
					294,
					2674,
					11,
					25632,
					307,
					1151,
					13,
					51260
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 330,
				"seek": 17414,
				"start": 949.9369,
				"end": 954.0769,
				"text": " So if you have a thing where you're generating a bunch of like Luma URLs,",
				"tokens": [
					51287,
					407,
					498,
					291,
					362,
					257,
					551,
					689,
					291,
					434,
					17746,
					257,
					3840,
					295,
					411,
					441,
					5544,
					43267,
					11,
					51494
				],
				"temperature": 0,
				"avg_logprob": -0.16501987,
				"compression_ratio": 1.56,
				"no_speech_prob": 1.1290369e-12
			},
			{
				"id": 331,
				"seek": 19672,
				"start": 954.0769,
				"end": 963.65686,
				"text": " don't put uuids into the uh don't put uuids into the uh into the prompt it's just going to hurt",
				"tokens": [
					50365,
					500,
					380,
					829,
					344,
					84,
					3742,
					666,
					264,
					2232,
					500,
					380,
					829,
					344,
					84,
					3742,
					666,
					264,
					2232,
					666,
					264,
					12391,
					309,
					311,
					445,
					516,
					281,
					4607,
					50844
				],
				"temperature": 0,
				"avg_logprob": -0.08523148,
				"compression_ratio": 1.7906977,
				"no_speech_prob": 9.212729e-13
			},
			{
				"id": 332,
				"seek": 19672,
				"start": 963.65686,
				"end": 968.1969,
				"text": " even if your tool call generates one like replace that out we have a thing in our chat bot uh that",
				"tokens": [
					50844,
					754,
					498,
					428,
					2290,
					818,
					23815,
					472,
					411,
					7406,
					300,
					484,
					321,
					362,
					257,
					551,
					294,
					527,
					5081,
					10592,
					2232,
					300,
					51071
				],
				"temperature": 0,
				"avg_logprob": -0.08523148,
				"compression_ratio": 1.7906977,
				"no_speech_prob": 9.212729e-13
			},
			{
				"id": 333,
				"seek": 19672,
				"start": 968.1969,
				"end": 972.65686,
				"text": " we made recently and i i thought it was kind of funny that and we talk about this all the time",
				"tokens": [
					51071,
					321,
					1027,
					3938,
					293,
					741,
					741,
					1194,
					309,
					390,
					733,
					295,
					4074,
					300,
					293,
					321,
					751,
					466,
					341,
					439,
					264,
					565,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.08523148,
				"compression_ratio": 1.7906977,
				"no_speech_prob": 9.212729e-13
			},
			{
				"id": 334,
				"seek": 19672,
				"start": 972.65686,
				"end": 982.6369,
				"text": " but like if you ask it like um how do i do this thing like how do i build a super complex agent",
				"tokens": [
					51294,
					457,
					411,
					498,
					291,
					1029,
					309,
					411,
					1105,
					577,
					360,
					741,
					360,
					341,
					551,
					411,
					577,
					360,
					741,
					1322,
					257,
					1687,
					3997,
					9461,
					51793
				],
				"temperature": 0,
				"avg_logprob": -0.08523148,
				"compression_ratio": 1.7906977,
				"no_speech_prob": 9.212729e-13
			},
			{
				"id": 335,
				"seek": 22528,
				"start": 982.6369,
				"end": 988.2169,
				"text": " what I found really funny in here is one of our uh someone on my team built this and I've tried my",
				"tokens": [
					50365,
					437,
					286,
					1352,
					534,
					4074,
					294,
					510,
					307,
					472,
					295,
					527,
					2232,
					1580,
					322,
					452,
					1469,
					3094,
					341,
					293,
					286,
					600,
					3031,
					452,
					50644
				],
				"temperature": 0,
				"avg_logprob": -0.09028646,
				"compression_ratio": 1.8022814,
				"no_speech_prob": 1.164587e-12
			},
			{
				"id": 336,
				"seek": 22528,
				"start": 988.2169,
				"end": 991.7169,
				"text": " best not to influence anyone on the team in any meaningful way I want them to discover their own",
				"tokens": [
					50644,
					1151,
					406,
					281,
					6503,
					2878,
					322,
					264,
					1469,
					294,
					604,
					10995,
					636,
					286,
					528,
					552,
					281,
					4411,
					641,
					1065,
					50819
				],
				"temperature": 0,
				"avg_logprob": -0.09028646,
				"compression_ratio": 1.8022814,
				"no_speech_prob": 1.164587e-12
			},
			{
				"id": 337,
				"seek": 22528,
				"start": 991.7169,
				"end": 998.9169,
				"text": " agent assigned decisions which is how do I get help how do I get help sorry which was when they",
				"tokens": [
					50819,
					9461,
					13279,
					5327,
					597,
					307,
					577,
					360,
					286,
					483,
					854,
					577,
					360,
					286,
					483,
					854,
					2597,
					597,
					390,
					562,
					436,
					51179
				],
				"temperature": 0,
				"avg_logprob": -0.09028646,
				"compression_ratio": 1.8022814,
				"no_speech_prob": 1.164587e-12
			},
			{
				"id": 338,
				"seek": 22528,
				"start": 998.9169,
				"end": 1004.7969,
				"text": " actually put our discord link in here they originally did this but this thing was inconsistent",
				"tokens": [
					51179,
					767,
					829,
					527,
					32989,
					2113,
					294,
					510,
					436,
					7993,
					630,
					341,
					457,
					341,
					551,
					390,
					36891,
					51473
				],
				"temperature": 0,
				"avg_logprob": -0.09028646,
				"compression_ratio": 1.8022814,
				"no_speech_prob": 1.164587e-12
			},
			{
				"id": 339,
				"seek": 22528,
				"start": 1004.7969,
				"end": 1009.4969,
				"text": " and they had to reprompt it to actually go ahead and actually change the boundaryml.com",
				"tokens": [
					51473,
					293,
					436,
					632,
					281,
					1085,
					4397,
					662,
					309,
					281,
					767,
					352,
					2286,
					293,
					767,
					1319,
					264,
					12866,
					15480,
					13,
					1112,
					51708
				],
				"temperature": 0,
				"avg_logprob": -0.09028646,
				"compression_ratio": 1.8022814,
				"no_speech_prob": 1.164587e-12
			},
			{
				"id": 340,
				"seek": 25214,
				"start": 1009.4969,
				"end": 1015.2569,
				"text": " slash discord. In this case, this one was fine because it actually pulled this page as context",
				"tokens": [
					50365,
					17330,
					32989,
					13,
					682,
					341,
					1389,
					11,
					341,
					472,
					390,
					2489,
					570,
					309,
					767,
					7373,
					341,
					3028,
					382,
					4319,
					50653
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 341,
				"seek": 25214,
				"start": 1015.2569,
				"end": 1020.0169,
				"text": " info and fed that into there. So it was able to do a regurgitation. But when you left on the system",
				"tokens": [
					50653,
					13614,
					293,
					4636,
					300,
					666,
					456,
					13,
					407,
					309,
					390,
					1075,
					281,
					360,
					257,
					1121,
					5476,
					4614,
					13,
					583,
					562,
					291,
					1411,
					322,
					264,
					1185,
					50891
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 342,
				"seek": 25214,
				"start": 1020.0169,
				"end": 1024.1969,
				"text": " prompt as the main way of getting help, it didn't work. So they actually changed.",
				"tokens": [
					50891,
					12391,
					382,
					264,
					2135,
					636,
					295,
					1242,
					854,
					11,
					309,
					994,
					380,
					589,
					13,
					407,
					436,
					767,
					3105,
					13,
					51100
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 343,
				"seek": 25214,
				"start": 1024.1969,
				"end": 1028.877,
				"text": " Sorry, this is the, you said the problem is because this is some weird long string token",
				"tokens": [
					51100,
					4919,
					11,
					341,
					307,
					264,
					11,
					291,
					848,
					264,
					1154,
					307,
					570,
					341,
					307,
					512,
					3657,
					938,
					6798,
					14862,
					51334
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 344,
				"seek": 25214,
				"start": 1028.877,
				"end": 1031.4769,
				"text": " that's hard for the model to remember. And in this case, what they're doing,",
				"tokens": [
					51334,
					300,
					311,
					1152,
					337,
					264,
					2316,
					281,
					1604,
					13,
					400,
					294,
					341,
					1389,
					11,
					437,
					436,
					434,
					884,
					11,
					51464
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 345,
				"seek": 25214,
				"start": 1031.5369,
				"end": 1035.8569,
				"text": " there's actually pruning it out and injecting this page in as the only contact information page.",
				"tokens": [
					51467,
					456,
					311,
					767,
					582,
					37726,
					309,
					484,
					293,
					10711,
					278,
					341,
					3028,
					294,
					382,
					264,
					787,
					3385,
					1589,
					3028,
					13,
					51683
				],
				"temperature": 0,
				"avg_logprob": -0.09412302,
				"compression_ratio": 1.7788779,
				"no_speech_prob": 2.20186e-12
			},
			{
				"id": 346,
				"seek": 27850,
				"start": 1035.8569,
				"end": 1037.5168,
				"text": " so it's really easy for it to regurgitate.",
				"tokens": [
					50365,
					370,
					309,
					311,
					534,
					1858,
					337,
					309,
					281,
					1121,
					5476,
					8086,
					13,
					50448
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 347,
				"seek": 27850,
				"start": 1037.7368,
				"end": 1038.637,
				"text": " It's a pretty beefy model.",
				"tokens": [
					50459,
					467,
					311,
					257,
					1238,
					9256,
					88,
					2316,
					13,
					50504
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 348,
				"seek": 27850,
				"start": 1039.3169,
				"end": 1041.9169,
				"text": " But what they found is we actually made a link",
				"tokens": [
					50538,
					583,
					437,
					436,
					1352,
					307,
					321,
					767,
					1027,
					257,
					2113,
					50668
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 349,
				"seek": 27850,
				"start": 1041.9169,
				"end": 1043.2769,
				"text": " called boundarymail.com slash discord",
				"tokens": [
					50668,
					1219,
					12866,
					11799,
					13,
					1112,
					17330,
					32989,
					50736
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 350,
				"seek": 27850,
				"start": 1043.2769,
				"end": 1045.0168,
				"text": " and putting this in the system prompt",
				"tokens": [
					50736,
					293,
					3372,
					341,
					294,
					264,
					1185,
					12391,
					50823
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 351,
				"seek": 27850,
				"start": 1045.0168,
				"end": 1046.617,
				"text": " works almost all the time.",
				"tokens": [
					50823,
					1985,
					1920,
					439,
					264,
					565,
					13,
					50903
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 352,
				"seek": 27850,
				"start": 1048.0369,
				"end": 1049.4369,
				"text": " All right, and it's the same context.",
				"tokens": [
					50974,
					1057,
					558,
					11,
					293,
					309,
					311,
					264,
					912,
					4319,
					13,
					51044
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 353,
				"seek": 27850,
				"start": 1049.5369,
				"end": 1050.4968,
				"text": " So it's like just thinking about",
				"tokens": [
					51049,
					407,
					309,
					311,
					411,
					445,
					1953,
					466,
					51097
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 354,
				"seek": 27850,
				"start": 1050.4968,
				"end": 1053.4569,
				"text": " how to make these systems work in terms of tool calls",
				"tokens": [
					51097,
					577,
					281,
					652,
					613,
					3652,
					589,
					294,
					2115,
					295,
					2290,
					5498,
					51245
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 355,
				"seek": 27850,
				"start": 1053.4569,
				"end": 1055.2169,
				"text": " is I don't really know what the best answer is,",
				"tokens": [
					51245,
					307,
					286,
					500,
					380,
					534,
					458,
					437,
					264,
					1151,
					1867,
					307,
					11,
					51333
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 356,
				"seek": 27850,
				"start": 1055.2969,
				"end": 1058.377,
				"text": " but the better you can do to get towards simplicity,",
				"tokens": [
					51337,
					457,
					264,
					1101,
					291,
					393,
					360,
					281,
					483,
					3030,
					25632,
					11,
					51491
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 357,
				"seek": 27850,
				"start": 1058.5569,
				"end": 1061.0969,
				"text": " in general, the way more throughput you will get.",
				"tokens": [
					51500,
					294,
					2674,
					11,
					264,
					636,
					544,
					44629,
					291,
					486,
					483,
					13,
					51627
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 358,
				"seek": 27850,
				"start": 1061.9169,
				"end": 1062.4169,
				"text": " Yeah, exactly.",
				"tokens": [
					51668,
					865,
					11,
					2293,
					13,
					51693
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 359,
				"seek": 27850,
				"start": 1062.5569,
				"end": 1063.6769,
				"text": " Random hashes are just not good.",
				"tokens": [
					51700,
					37603,
					575,
					8076,
					366,
					445,
					406,
					665,
					13,
					51756
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 360,
				"seek": 27850,
				"start": 1063.7769,
				"end": 1065.6569,
				"text": " Like the model is just never going to be good at that.",
				"tokens": [
					51761,
					1743,
					264,
					2316,
					307,
					445,
					1128,
					516,
					281,
					312,
					665,
					412,
					300,
					13,
					51855
				],
				"temperature": 0,
				"avg_logprob": -0.17732608,
				"compression_ratio": 1.6988636,
				"no_speech_prob": 2.1010277e-12
			},
			{
				"id": 361,
				"seek": 30850,
				"start": 1065.8569,
				"end": 1068.3969,
				"text": " And I think it's the same thing here.",
				"tokens": [
					50365,
					400,
					286,
					519,
					309,
					311,
					264,
					912,
					551,
					510,
					13,
					50492
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 362,
				"seek": 30850,
				"start": 1068.3969,
				"end": 1070.4769,
				"text": " Like I think in their scenario, it kind of works,",
				"tokens": [
					50492,
					1743,
					286,
					519,
					294,
					641,
					9005,
					11,
					309,
					733,
					295,
					1985,
					11,
					50596
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 363,
				"seek": 30850,
				"start": 1070.5369,
				"end": 1072.117,
				"text": " but I would just eval this for your own use case.",
				"tokens": [
					50599,
					457,
					286,
					576,
					445,
					1073,
					304,
					341,
					337,
					428,
					1065,
					764,
					1389,
					13,
					50678
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 364,
				"seek": 30850,
				"start": 1072.2769,
				"end": 1074.0369,
				"text": " I suspect if you have a simple task",
				"tokens": [
					50686,
					286,
					9091,
					498,
					291,
					362,
					257,
					2199,
					5633,
					50774
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 365,
				"seek": 30850,
				"start": 1074.0369,
				"end": 1075.2769,
				"text": " and it did it wrong twice,",
				"tokens": [
					50774,
					293,
					309,
					630,
					309,
					2085,
					6091,
					11,
					50836
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 366,
				"seek": 30850,
				"start": 1075.5168,
				"end": 1077.6569,
				"text": " you'll probably get better",
				"tokens": [
					50848,
					291,
					603,
					1391,
					483,
					1101,
					50955
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 367,
				"seek": 30850,
				"start": 1077.6569,
				"end": 1078.7769,
				"text": " by just giving it the right output.",
				"tokens": [
					50955,
					538,
					445,
					2902,
					309,
					264,
					558,
					5598,
					13,
					51011
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 368,
				"seek": 30850,
				"start": 1079.0369,
				"end": 1080.2969,
				"text": " If you're in a smaller model,",
				"tokens": [
					51024,
					759,
					291,
					434,
					294,
					257,
					4356,
					2316,
					11,
					51087
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 369,
				"seek": 30850,
				"start": 1080.5969,
				"end": 1082.0969,
				"text": " you probably don't want to give a smaller model",
				"tokens": [
					51102,
					291,
					1391,
					500,
					380,
					528,
					281,
					976,
					257,
					4356,
					2316,
					51177
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 370,
				"seek": 30850,
				"start": 1082.0969,
				"end": 1083.0969,
				"text": " the entire stack trace",
				"tokens": [
					51177,
					264,
					2302,
					8630,
					13508,
					51227
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 371,
				"seek": 30850,
				"start": 1083.0969,
				"end": 1086.3369,
				"text": " because a smaller model is just going to get lost in the sauce.",
				"tokens": [
					51227,
					570,
					257,
					4356,
					2316,
					307,
					445,
					516,
					281,
					483,
					2731,
					294,
					264,
					4880,
					13,
					51389
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 372,
				"seek": 30850,
				"start": 1086.877,
				"end": 1087.9369,
				"text": " If you have a bigger model,",
				"tokens": [
					51416,
					759,
					291,
					362,
					257,
					3801,
					2316,
					11,
					51469
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 373,
				"seek": 30850,
				"start": 1088.2568,
				"end": 1091.6969,
				"text": " it's more forgiving in general for all of this information.",
				"tokens": [
					51485,
					309,
					311,
					544,
					37701,
					294,
					2674,
					337,
					439,
					295,
					341,
					1589,
					13,
					51657
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 374,
				"seek": 30850,
				"start": 1092.4369,
				"end": 1094.1969,
				"text": " So trying to figure out exactly what you have to do",
				"tokens": [
					51694,
					407,
					1382,
					281,
					2573,
					484,
					2293,
					437,
					291,
					362,
					281,
					360,
					51782
				],
				"temperature": 0,
				"avg_logprob": -0.20065859,
				"compression_ratio": 1.8146964,
				"no_speech_prob": 1.7079798e-12
			},
			{
				"id": 375,
				"seek": 33684,
				"start": 1094.1969,
				"end": 1098.4369,
				"text": " both a parameterization of the problem space you're working in and the model you're using.",
				"tokens": [
					50365,
					1293,
					257,
					13075,
					2144,
					295,
					264,
					1154,
					1901,
					291,
					434,
					1364,
					294,
					293,
					264,
					2316,
					291,
					434,
					1228,
					13,
					50577
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 376,
				"seek": 33684,
				"start": 1098.4369,
				"end": 1103.7969,
				"text": " And it's like a fine art of figuring out what that is exactly. But the fact that this works",
				"tokens": [
					50577,
					400,
					309,
					311,
					411,
					257,
					2489,
					1523,
					295,
					15213,
					484,
					437,
					300,
					307,
					2293,
					13,
					583,
					264,
					1186,
					300,
					341,
					1985,
					50845
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 377,
				"seek": 33684,
				"start": 1103.7969,
				"end": 1107.3169,
				"text": " for them, like to me, this slightly changed my perspective of being completely rigid on only",
				"tokens": [
					50845,
					337,
					552,
					11,
					411,
					281,
					385,
					11,
					341,
					4748,
					3105,
					452,
					4585,
					295,
					885,
					2584,
					22195,
					322,
					787,
					51021
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 378,
				"seek": 33684,
				"start": 1107.3169,
				"end": 1112.9169,
				"text": " using observation 2b in the final prompt. Now what I do... Go ahead. Sorry, go ahead. No,",
				"tokens": [
					51021,
					1228,
					14816,
					568,
					65,
					294,
					264,
					2572,
					12391,
					13,
					823,
					437,
					286,
					360,
					485,
					1037,
					2286,
					13,
					4919,
					11,
					352,
					2286,
					13,
					883,
					11,
					51301
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 379,
				"seek": 33684,
				"start": 1112.9169,
				"end": 1116.2769,
				"text": " finish your thought. Well, what I was going to say is next time I have a really hard problem and",
				"tokens": [
					51301,
					2413,
					428,
					1194,
					13,
					1042,
					11,
					437,
					286,
					390,
					516,
					281,
					584,
					307,
					958,
					565,
					286,
					362,
					257,
					534,
					1152,
					1154,
					293,
					51469
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 380,
				"seek": 33684,
				"start": 1116.2769,
				"end": 1120.117,
				"text": " I use GPT-5, for example, I'll probably toss an entire stack trace and just see how well it does.",
				"tokens": [
					51469,
					286,
					764,
					26039,
					51,
					12,
					20,
					11,
					337,
					1365,
					11,
					286,
					603,
					1391,
					14432,
					364,
					2302,
					8630,
					13508,
					293,
					445,
					536,
					577,
					731,
					309,
					775,
					13,
					51661
				],
				"temperature": 0,
				"avg_logprob": -0.13935454,
				"compression_ratio": 1.6184971,
				"no_speech_prob": 2.362218e-12
			},
			{
				"id": 381,
				"seek": 36276,
				"start": 1120.117,
				"end": 1124.8969,
				"text": " because it's possible and models have changed last time i built this opinion was based off of",
				"tokens": [
					50365,
					570,
					309,
					311,
					1944,
					293,
					5245,
					362,
					3105,
					1036,
					565,
					741,
					3094,
					341,
					4800,
					390,
					2361,
					766,
					295,
					50604
				],
				"temperature": 0,
				"avg_logprob": -0.08061282,
				"compression_ratio": 1.6926606,
				"no_speech_prob": 1.973745e-12
			},
			{
				"id": 382,
				"seek": 36276,
				"start": 1124.8969,
				"end": 1130.2368,
				"text": " like the gpt-40 models and perhaps the models are different now and it's good for me to be aware of",
				"tokens": [
					50604,
					411,
					264,
					290,
					662,
					12,
					5254,
					5245,
					293,
					4317,
					264,
					5245,
					366,
					819,
					586,
					293,
					309,
					311,
					665,
					337,
					385,
					281,
					312,
					3650,
					295,
					50871
				],
				"temperature": 0,
				"avg_logprob": -0.08061282,
				"compression_ratio": 1.6926606,
				"no_speech_prob": 1.973745e-12
			},
			{
				"id": 383,
				"seek": 36276,
				"start": 1130.2368,
				"end": 1136.2769,
				"text": " that and like be open to changing my own perspectives interesting i mean so this is",
				"tokens": [
					50871,
					300,
					293,
					411,
					312,
					1269,
					281,
					4473,
					452,
					1065,
					16766,
					1880,
					741,
					914,
					370,
					341,
					307,
					51173
				],
				"temperature": 0,
				"avg_logprob": -0.08061282,
				"compression_ratio": 1.6926606,
				"no_speech_prob": 1.973745e-12
			},
			{
				"id": 384,
				"seek": 36276,
				"start": 1136.2769,
				"end": 1142.9769,
				"text": " interesting it doesn't say so it's like air recovery but i'm not it's not clear saying like",
				"tokens": [
					51173,
					1880,
					309,
					1177,
					380,
					584,
					370,
					309,
					311,
					411,
					1988,
					8597,
					457,
					741,
					478,
					406,
					309,
					311,
					406,
					1850,
					1566,
					411,
					51508
				],
				"temperature": 0,
				"avg_logprob": -0.08061282,
				"compression_ratio": 1.6926606,
				"no_speech_prob": 1.973745e-12
			},
			{
				"id": 385,
				"seek": 1149,
				"start": 1142.9769,
				"end": 1163.9553,
				"text": " um like this picture says hey like use the observation to get it to get it correctly right or use the thing to get it to get correctly the first time but it doesn seem to have an opinion on like okay if you have four errors and then you get it right that you should clear out those four errors before you proceed yeah yeah it how do i think the what they saying is typically",
				"tokens": [
					50365,
					1105,
					411,
					341,
					3036,
					1619,
					4177,
					411,
					764,
					264,
					14816,
					281,
					483,
					309,
					281,
					483,
					309,
					8944,
					558,
					50737,
					50737,
					420,
					764,
					264,
					551,
					281,
					483,
					309,
					281,
					483,
					8944,
					264,
					700,
					565,
					457,
					309,
					1177,
					380,
					1643,
					281,
					362,
					364,
					4800,
					50969,
					4800,
					322,
					411,
					1392,
					498,
					291,
					362,
					1451,
					13603,
					293,
					550,
					291,
					483,
					309,
					558,
					300,
					291,
					820,
					1850,
					484,
					50797,
					50797,
					729,
					1451,
					13603,
					949,
					291,
					8991,
					1338,
					1338,
					309,
					577,
					360,
					741,
					519,
					264,
					437,
					436,
					434,
					1566,
					307,
					5850,
					51063
				],
				"temperature": 0,
				"avg_logprob": -0.06284048,
				"compression_ratio": 1.8626198,
				"no_speech_prob": 2.3994493e-12
			},
			{
				"id": 386,
				"seek": 1149,
				"start": 1163.9553,
				"end": 1169.8153,
				"text": " many people never store this information i think what you're making a case for is hey consider",
				"tokens": [
					51063,
					867,
					561,
					1128,
					3531,
					341,
					1589,
					741,
					519,
					437,
					291,
					434,
					1455,
					257,
					1389,
					337,
					307,
					4177,
					1949,
					51356
				],
				"temperature": 0,
				"avg_logprob": -0.06284048,
				"compression_ratio": 1.8626198,
				"no_speech_prob": 2.3994493e-12
			},
			{
				"id": 387,
				"seek": 1149,
				"start": 1169.8153,
				"end": 1175.8153,
				"text": " storing the information sometimes it might help yeah this is uh factor nine of 12 factor agents",
				"tokens": [
					51356,
					26085,
					264,
					1589,
					2171,
					309,
					1062,
					854,
					1338,
					341,
					307,
					2232,
					5952,
					4949,
					295,
					2272,
					5952,
					12554,
					51656
				],
				"temperature": 0,
				"avg_logprob": -0.06284048,
				"compression_ratio": 1.8626198,
				"no_speech_prob": 2.3994493e-12
			},
			{
				"id": 388,
				"seek": 1149,
				"start": 1175.8153,
				"end": 1179.4553,
				"text": " is like be thoughtful about it but yeah tell the model what it did wrong because it will probably",
				"tokens": [
					51656,
					307,
					411,
					312,
					21566,
					466,
					309,
					457,
					1338,
					980,
					264,
					2316,
					437,
					309,
					630,
					2085,
					570,
					309,
					486,
					1391,
					51838
				],
				"temperature": 0,
				"avg_logprob": -0.06284048,
				"compression_ratio": 1.8626198,
				"no_speech_prob": 2.3994493e-12
			},
			{
				"id": 389,
				"seek": 4095,
				"start": 1179.4553,
				"end": 1186.4753,
				"text": " fix it. And the models are getting way better. So like the stack traces are likely more likely to",
				"tokens": [
					50365,
					3191,
					309,
					13,
					400,
					264,
					5245,
					366,
					1242,
					636,
					1101,
					13,
					407,
					411,
					264,
					8630,
					26076,
					366,
					3700,
					544,
					3700,
					281,
					50716
				],
				"temperature": 0,
				"avg_logprob": -0.08990526,
				"compression_ratio": 1.8505747,
				"no_speech_prob": 1.861248e-12
			},
			{
				"id": 390,
				"seek": 4095,
				"start": 1186.4753,
				"end": 1189.6354,
				"text": " help, especially as the stuff gets bigger and bigger in terms of context windows and how well",
				"tokens": [
					50716,
					854,
					11,
					2318,
					382,
					264,
					1507,
					2170,
					3801,
					293,
					3801,
					294,
					2115,
					295,
					4319,
					9309,
					293,
					577,
					731,
					50874
				],
				"temperature": 0,
				"avg_logprob": -0.08990526,
				"compression_ratio": 1.8505747,
				"no_speech_prob": 1.861248e-12
			},
			{
				"id": 391,
				"seek": 4095,
				"start": 1189.6354,
				"end": 1196.4954,
				"text": " models perform over long form context. That said, same thing we talked about here. Proximity to the",
				"tokens": [
					50874,
					5245,
					2042,
					670,
					938,
					1254,
					4319,
					13,
					663,
					848,
					11,
					912,
					551,
					321,
					2825,
					466,
					510,
					13,
					1705,
					3081,
					507,
					281,
					264,
					51217
				],
				"temperature": 0,
				"avg_logprob": -0.08990526,
				"compression_ratio": 1.8505747,
				"no_speech_prob": 1.861248e-12
			},
			{
				"id": 392,
				"seek": 4095,
				"start": 1196.4954,
				"end": 1201.3153,
				"text": " cache matters. Proximity to the observation, or where did it go? Proximity to the observations",
				"tokens": [
					51217,
					19459,
					7001,
					13,
					1705,
					3081,
					507,
					281,
					264,
					14816,
					11,
					420,
					689,
					630,
					309,
					352,
					30,
					1705,
					3081,
					507,
					281,
					264,
					18163,
					51458
				],
				"temperature": 0,
				"avg_logprob": -0.08990526,
				"compression_ratio": 1.8505747,
				"no_speech_prob": 1.861248e-12
			},
			{
				"id": 393,
				"seek": 4095,
				"start": 1201.3153,
				"end": 1206.0154,
				"text": " matter. I don't know where it went, that image, the repetition one. Proximity to the observation",
				"tokens": [
					51458,
					1871,
					13,
					286,
					500,
					380,
					458,
					689,
					309,
					1437,
					11,
					300,
					3256,
					11,
					264,
					30432,
					472,
					13,
					1705,
					3081,
					507,
					281,
					264,
					14816,
					51693
				],
				"temperature": 0,
				"avg_logprob": -0.08990526,
				"compression_ratio": 1.8505747,
				"no_speech_prob": 1.861248e-12
			},
			{
				"id": 394,
				"seek": 6751,
				"start": 1206.0154,
				"end": 1209.7954,
				"text": " matter so like if you make it too big you'll probably get loss of performance at some point",
				"tokens": [
					50365,
					1871,
					370,
					411,
					498,
					291,
					652,
					309,
					886,
					955,
					291,
					603,
					1391,
					483,
					4470,
					295,
					3389,
					412,
					512,
					935,
					50554
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 395,
				"seek": 6751,
				"start": 1209.7954,
				"end": 1214.4553,
				"text": " for your task um let's talk about few shot prompting i say this all the time don't do few",
				"tokens": [
					50554,
					337,
					428,
					5633,
					1105,
					718,
					311,
					751,
					466,
					1326,
					3347,
					12391,
					278,
					741,
					584,
					341,
					439,
					264,
					565,
					500,
					380,
					360,
					1326,
					50787
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 396,
				"seek": 6751,
				"start": 1214.4553,
				"end": 1218.8754,
				"text": " shot prompting the reason few shot prompting is bad is for the same reason that we say all the time",
				"tokens": [
					50787,
					3347,
					12391,
					278,
					264,
					1778,
					1326,
					3347,
					12391,
					278,
					307,
					1578,
					307,
					337,
					264,
					912,
					1778,
					300,
					321,
					584,
					439,
					264,
					565,
					51008
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 397,
				"seek": 6751,
				"start": 1218.8754,
				"end": 1223.2954,
				"text": " which is you're most people don't do few shot prompting correctly you're almost most likely",
				"tokens": [
					51008,
					597,
					307,
					291,
					434,
					881,
					561,
					500,
					380,
					360,
					1326,
					3347,
					12391,
					278,
					8944,
					291,
					434,
					1920,
					881,
					3700,
					51229
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 398,
				"seek": 6751,
				"start": 1223.2954,
				"end": 1228.2754,
				"text": " going to bias the model away towards your few shot example rather than helping understand what",
				"tokens": [
					51229,
					516,
					281,
					12577,
					264,
					2316,
					1314,
					3030,
					428,
					1326,
					3347,
					1365,
					2831,
					813,
					4315,
					1223,
					437,
					51478
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 399,
				"seek": 6751,
				"start": 1228.2754,
				"end": 1234.4154,
				"text": " it's actually meant to do well and literally building an agent is few shot prompting like",
				"tokens": [
					51478,
					309,
					311,
					767,
					4140,
					281,
					360,
					731,
					293,
					3736,
					2390,
					364,
					9461,
					307,
					1326,
					3347,
					12391,
					278,
					411,
					51785
				],
				"temperature": 0,
				"avg_logprob": -0.06974446,
				"compression_ratio": 2.029091,
				"no_speech_prob": 1.7081575e-12
			},
			{
				"id": 400,
				"seek": 9591,
				"start": 1234.4154,
				"end": 1240.5554,
				"text": " everything in your past context window becomes like influences the next step and so if the model",
				"tokens": [
					50365,
					1203,
					294,
					428,
					1791,
					4319,
					4910,
					3643,
					411,
					21222,
					264,
					958,
					1823,
					293,
					370,
					498,
					264,
					2316,
					50672
				],
				"temperature": 0,
				"avg_logprob": -0.035966303,
				"compression_ratio": 1.8931298,
				"no_speech_prob": 2.1257824e-12
			},
			{
				"id": 401,
				"seek": 9591,
				"start": 1240.5554,
				"end": 1246.8353,
				"text": " starts like starts a task and launches a sub-agent for the rest of that conversation the model will",
				"tokens": [
					50672,
					3719,
					411,
					3719,
					257,
					5633,
					293,
					31841,
					257,
					1422,
					12,
					559,
					317,
					337,
					264,
					1472,
					295,
					300,
					3761,
					264,
					2316,
					486,
					50986
				],
				"temperature": 0,
				"avg_logprob": -0.035966303,
				"compression_ratio": 1.8931298,
				"no_speech_prob": 2.1257824e-12
			},
			{
				"id": 402,
				"seek": 9591,
				"start": 1246.8353,
				"end": 1252.9353,
				"text": " be more likely to launch a sub-agent and so like you have to really like understand and this is why",
				"tokens": [
					50986,
					312,
					544,
					3700,
					281,
					4025,
					257,
					1422,
					12,
					559,
					317,
					293,
					370,
					411,
					291,
					362,
					281,
					534,
					411,
					1223,
					293,
					341,
					307,
					983,
					51291
				],
				"temperature": 0,
				"avg_logprob": -0.035966303,
				"compression_ratio": 1.8931298,
				"no_speech_prob": 2.1257824e-12
			},
			{
				"id": 403,
				"seek": 9591,
				"start": 1252.9353,
				"end": 1257.3754,
				"text": " we always say like use clear rather than re-steer if the model like starts doing something weird in",
				"tokens": [
					51291,
					321,
					1009,
					584,
					411,
					764,
					1850,
					2831,
					813,
					319,
					12,
					2941,
					260,
					498,
					264,
					2316,
					411,
					3719,
					884,
					746,
					3657,
					294,
					51513
				],
				"temperature": 0,
				"avg_logprob": -0.035966303,
				"compression_ratio": 1.8931298,
				"no_speech_prob": 2.1257824e-12
			},
			{
				"id": 404,
				"seek": 9591,
				"start": 1257.3754,
				"end": 1261.3153,
				"text": " your agentic chat whatever system it is obviously i care a lot about coding agents but like this is",
				"tokens": [
					51513,
					428,
					9461,
					299,
					5081,
					2035,
					1185,
					309,
					307,
					2745,
					741,
					1127,
					257,
					688,
					466,
					17720,
					12554,
					457,
					411,
					341,
					307,
					51710
				],
				"temperature": 0,
				"avg_logprob": -0.035966303,
				"compression_ratio": 1.8931298,
				"no_speech_prob": 2.1257824e-12
			},
			{
				"id": 405,
				"seek": 12281,
				"start": 1261.3153,
				"end": 1265.9353,
				"text": " true for everything is like the model starts going about it in the wrong way you are much better off",
				"tokens": [
					50365,
					2074,
					337,
					1203,
					307,
					411,
					264,
					2316,
					3719,
					516,
					466,
					309,
					294,
					264,
					2085,
					636,
					291,
					366,
					709,
					1101,
					766,
					50596
				],
				"temperature": 0,
				"avg_logprob": -0.03649186,
				"compression_ratio": 1.8725868,
				"no_speech_prob": 2.298589e-12
			},
			{
				"id": 406,
				"seek": 12281,
				"start": 1265.9353,
				"end": 1270.3353,
				"text": " starting with a fresh context and adding two sentences of steering or two words of steering",
				"tokens": [
					50596,
					2891,
					365,
					257,
					4451,
					4319,
					293,
					5127,
					732,
					16579,
					295,
					14823,
					420,
					732,
					2283,
					295,
					14823,
					50816
				],
				"temperature": 0,
				"avg_logprob": -0.03649186,
				"compression_ratio": 1.8725868,
				"no_speech_prob": 2.298589e-12
			},
			{
				"id": 407,
				"seek": 12281,
				"start": 1270.3353,
				"end": 1276.2153,
				"text": " by the way don't do this then you are to be like stop do it a different way and then because then",
				"tokens": [
					50816,
					538,
					264,
					636,
					500,
					380,
					360,
					341,
					550,
					291,
					366,
					281,
					312,
					411,
					1590,
					360,
					309,
					257,
					819,
					636,
					293,
					550,
					570,
					550,
					51110
				],
				"temperature": 0,
				"avg_logprob": -0.03649186,
				"compression_ratio": 1.8725868,
				"no_speech_prob": 2.298589e-12
			},
			{
				"id": 408,
				"seek": 12281,
				"start": 1276.2153,
				"end": 1282.0154,
				"text": " your prompt is like your context window says system prompt user message model tried this user",
				"tokens": [
					51110,
					428,
					12391,
					307,
					411,
					428,
					4319,
					4910,
					1619,
					1185,
					12391,
					4195,
					3636,
					2316,
					3031,
					341,
					4195,
					51400
				],
				"temperature": 0,
				"avg_logprob": -0.03649186,
				"compression_ratio": 1.8725868,
				"no_speech_prob": 2.298589e-12
			},
			{
				"id": 409,
				"seek": 12281,
				"start": 1282.0154,
				"end": 1286.9753,
				"text": " re-steered it and so you're more likely for the model to like expect that the conversation continues",
				"tokens": [
					51400,
					319,
					12,
					2941,
					4073,
					309,
					293,
					370,
					291,
					434,
					544,
					3700,
					337,
					264,
					2316,
					281,
					411,
					2066,
					300,
					264,
					3761,
					6515,
					51648
				],
				"temperature": 0,
				"avg_logprob": -0.03649186,
				"compression_ratio": 1.8725868,
				"no_speech_prob": 2.298589e-12
			},
			{
				"id": 410,
				"seek": 14847,
				"start": 1286.9753,
				"end": 1289.2754,
				"text": " who says model tries a bad thing, user re-steers.",
				"tokens": [
					50365,
					567,
					1619,
					2316,
					9898,
					257,
					1578,
					551,
					11,
					4195,
					319,
					12,
					2941,
					433,
					13,
					50480
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 411,
				"seek": 14847,
				"start": 1289.4154,
				"end": 1291.6753,
				"text": " Model tries a bad, like you're telling the model",
				"tokens": [
					50487,
					17105,
					9898,
					257,
					1578,
					11,
					411,
					291,
					434,
					3585,
					264,
					2316,
					50600
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 412,
				"seek": 14847,
				"start": 1291.6753,
				"end": 1294.6154,
				"text": " it's okay to make a mistake and then get corrected",
				"tokens": [
					50600,
					309,
					311,
					1392,
					281,
					652,
					257,
					6146,
					293,
					550,
					483,
					31687,
					50747
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 413,
				"seek": 14847,
				"start": 1294.6154,
				"end": 1297.2754,
				"text": " when what you really want is the model to think,",
				"tokens": [
					50747,
					562,
					437,
					291,
					534,
					528,
					307,
					264,
					2316,
					281,
					519,
					11,
					50880
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 414,
				"seek": 14847,
				"start": 1297.3754,
				"end": 1298.8754,
				"text": " okay, cool, I made the right decision.",
				"tokens": [
					50885,
					1392,
					11,
					1627,
					11,
					286,
					1027,
					264,
					558,
					3537,
					13,
					50960
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 415,
				"seek": 14847,
				"start": 1298.9353,
				"end": 1299.5953,
				"text": " I made the right decision.",
				"tokens": [
					50963,
					286,
					1027,
					264,
					558,
					3537,
					13,
					50996
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 416,
				"seek": 14847,
				"start": 1299.6554,
				"end": 1300.3553,
				"text": " I made the right decision.",
				"tokens": [
					50999,
					286,
					1027,
					264,
					558,
					3537,
					13,
					51034
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 417,
				"seek": 14847,
				"start": 1300.4553,
				"end": 1303.2754,
				"text": " That's how you craft like really good agentic context",
				"tokens": [
					51039,
					663,
					311,
					577,
					291,
					8448,
					411,
					534,
					665,
					9461,
					299,
					4319,
					51180
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 418,
				"seek": 14847,
				"start": 1303.2754,
				"end": 1303.8154,
				"text": " as you go.",
				"tokens": [
					51180,
					382,
					291,
					352,
					13,
					51207
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 419,
				"seek": 14847,
				"start": 1304.4154,
				"end": 1306.0154,
				"text": " Also contextually, just remember undo",
				"tokens": [
					51237,
					2743,
					4319,
					671,
					11,
					445,
					1604,
					23779,
					51317
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 420,
				"seek": 14847,
				"start": 1306.0154,
				"end": 1307.2354,
				"text": " is a pretty complex action.",
				"tokens": [
					51317,
					307,
					257,
					1238,
					3997,
					3069,
					13,
					51378
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 421,
				"seek": 14847,
				"start": 1307.5354,
				"end": 1309.5554,
				"text": " The fact that we can do command Z is nice on a computer,",
				"tokens": [
					51393,
					440,
					1186,
					300,
					321,
					393,
					360,
					5622,
					1176,
					307,
					1481,
					322,
					257,
					3820,
					11,
					51494
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 422,
				"seek": 14847,
				"start": 1309.8154,
				"end": 1312.4353,
				"text": " but it's actually really hard to undo sequences of actions.",
				"tokens": [
					51507,
					457,
					309,
					311,
					767,
					534,
					1152,
					281,
					23779,
					22978,
					295,
					5909,
					13,
					51638
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 423,
				"seek": 14847,
				"start": 1312.4353,
				"end": 1313.5953,
				"text": " And that's why many apps,",
				"tokens": [
					51638,
					400,
					300,
					311,
					983,
					867,
					7733,
					11,
					51696
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 424,
				"seek": 14847,
				"start": 1314.4154,
				"end": 1316.5554,
				"text": " most SaaS apps you use don't build command Z.",
				"tokens": [
					51737,
					881,
					49733,
					7733,
					291,
					764,
					500,
					380,
					1322,
					5622,
					1176,
					13,
					51844
				],
				"temperature": 0,
				"avg_logprob": -0.1571763,
				"compression_ratio": 1.8858025,
				"no_speech_prob": 3.894395e-12
			},
			{
				"id": 425,
				"seek": 17847,
				"start": 1316.9753,
				"end": 1319.9353,
				"text": " It's really freaking hard to command Z a lot of actions.",
				"tokens": [
					50365,
					467,
					311,
					534,
					14612,
					1152,
					281,
					5622,
					1176,
					257,
					688,
					295,
					5909,
					13,
					50513
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 426,
				"seek": 17847,
				"start": 1321.0354,
				"end": 1322.8154,
				"text": " And the model is just going to have a hard time too.",
				"tokens": [
					50568,
					400,
					264,
					2316,
					307,
					445,
					516,
					281,
					362,
					257,
					1152,
					565,
					886,
					13,
					50657
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 427,
				"seek": 17847,
				"start": 1324.3553,
				"end": 1326.1154,
				"text": " Photoshop, for example, will just eventually forget",
				"tokens": [
					50734,
					20821,
					11,
					337,
					1365,
					11,
					486,
					445,
					4728,
					2870,
					50822
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 428,
				"seek": 17847,
				"start": 1326.1154,
				"end": 1327.9353,
				"text": " that you're past a certain state.",
				"tokens": [
					50822,
					300,
					291,
					434,
					1791,
					257,
					1629,
					1785,
					13,
					50913
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 429,
				"seek": 17847,
				"start": 1327.9954,
				"end": 1329.3553,
				"text": " It'll just say you can't undo past this point.",
				"tokens": [
					50916,
					467,
					603,
					445,
					584,
					291,
					393,
					380,
					23779,
					1791,
					341,
					935,
					13,
					50984
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 430,
				"seek": 17847,
				"start": 1329.5354,
				"end": 1329.9553,
				"text": " Same thing.",
				"tokens": [
					50993,
					10635,
					551,
					13,
					51014
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 431,
				"seek": 17847,
				"start": 1330.8553,
				"end": 1331.9753,
				"text": " In terms of few shot examples,",
				"tokens": [
					51059,
					682,
					2115,
					295,
					1326,
					3347,
					5110,
					11,
					51115
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 432,
				"seek": 17847,
				"start": 1332.0753,
				"end": 1333.3154,
				"text": " there are ways to do it correctly.",
				"tokens": [
					51120,
					456,
					366,
					2098,
					281,
					360,
					309,
					8944,
					13,
					51182
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 433,
				"seek": 17847,
				"start": 1333.4954,
				"end": 1335.0953,
				"text": " It's just that you have to be really thoughtful about it.",
				"tokens": [
					51191,
					467,
					311,
					445,
					300,
					291,
					362,
					281,
					312,
					534,
					21566,
					466,
					309,
					13,
					51271
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 434,
				"seek": 17847,
				"start": 1335.1753,
				"end": 1336.4753,
				"text": " In this case, if I have a class object",
				"tokens": [
					51275,
					682,
					341,
					1389,
					11,
					498,
					286,
					362,
					257,
					1508,
					2657,
					51340
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 435,
				"seek": 17847,
				"start": 1336.4753,
				"end": 1339.4154,
				"text": " where I want product to mean only people",
				"tokens": [
					51340,
					689,
					286,
					528,
					1674,
					281,
					914,
					787,
					561,
					51487
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 436,
				"seek": 17847,
				"start": 1339.4154,
				"end": 1341.1753,
				"text": " that actually actively write code,",
				"tokens": [
					51487,
					300,
					767,
					13022,
					2464,
					3089,
					11,
					51575
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 437,
				"seek": 17847,
				"start": 1341.7354,
				"end": 1343.5554,
				"text": " then someone that's listed as director of an eng team,",
				"tokens": [
					51603,
					550,
					1580,
					300,
					311,
					10052,
					382,
					5391,
					295,
					364,
					1741,
					1469,
					11,
					51694
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 438,
				"seek": 17847,
				"start": 1344.0953,
				"end": 1346.4353,
				"text": " I can just write a small thing in here that says,",
				"tokens": [
					51721,
					286,
					393,
					445,
					2464,
					257,
					1359,
					551,
					294,
					510,
					300,
					1619,
					11,
					51838
				],
				"temperature": 0,
				"avg_logprob": -0.16065365,
				"compression_ratio": 1.7383721,
				"no_speech_prob": 1.1377841e-12
			},
			{
				"id": 439,
				"seek": 20793,
				"start": 1346.4353,
				"end": 1348.6953,
				"text": " because they don't code themselves category product.",
				"tokens": [
					50365,
					570,
					436,
					500,
					380,
					3089,
					2969,
					7719,
					1674,
					13,
					50478
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 440,
				"seek": 20793,
				"start": 1349.4553,
				"end": 1350.5753,
				"text": " And now the model can kind of,",
				"tokens": [
					50516,
					400,
					586,
					264,
					2316,
					393,
					733,
					295,
					11,
					50572
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 441,
				"seek": 20793,
				"start": 1351.2954,
				"end": 1353.0354,
				"text": " this is perfectly evident",
				"tokens": [
					50608,
					341,
					307,
					6239,
					16371,
					50695
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 442,
				"seek": 20793,
				"start": 1353.0354,
				"end": 1354.5953,
				"text": " that this is an example to the model.",
				"tokens": [
					50695,
					300,
					341,
					307,
					364,
					1365,
					281,
					264,
					2316,
					13,
					50773
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 443,
				"seek": 20793,
				"start": 1355.3754,
				"end": 1356.5354,
				"text": " I'm not trying to trick it.",
				"tokens": [
					50812,
					286,
					478,
					406,
					1382,
					281,
					4282,
					309,
					13,
					50870
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 444,
				"seek": 20793,
				"start": 1356.6154,
				"end": 1357.3553,
				"text": " I'm not trying to do anything.",
				"tokens": [
					50874,
					286,
					478,
					406,
					1382,
					281,
					360,
					1340,
					13,
					50911
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 445,
				"seek": 20793,
				"start": 1357.4154,
				"end": 1359.9553,
				"text": " It's just like this one is an example very clearly.",
				"tokens": [
					50914,
					467,
					311,
					445,
					411,
					341,
					472,
					307,
					364,
					1365,
					588,
					4448,
					13,
					51041
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 446,
				"seek": 20793,
				"start": 1360.4553,
				"end": 1362.1554,
				"text": " I use dot, dot, dot, simplify that.",
				"tokens": [
					51066,
					286,
					764,
					5893,
					11,
					5893,
					11,
					5893,
					11,
					20460,
					300,
					13,
					51151
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 447,
				"seek": 20793,
				"start": 1362.6154,
				"end": 1365.5953,
				"text": " I'm showing the schema without showing the full schema.",
				"tokens": [
					51174,
					286,
					478,
					4099,
					264,
					34078,
					1553,
					4099,
					264,
					1577,
					34078,
					13,
					51323
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 448,
				"seek": 20793,
				"start": 1365.9954,
				"end": 1368.4954,
				"text": " The model cannot possibly confuse Vaibhav Gupta",
				"tokens": [
					51343,
					440,
					2316,
					2644,
					6264,
					28584,
					16822,
					897,
					71,
					706,
					2694,
					47366,
					51468
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 449,
				"seek": 20793,
				"start": 1368.4954,
				"end": 1369.8353,
				"text": " as that person because,",
				"tokens": [
					51468,
					382,
					300,
					954,
					570,
					11,
					51535
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 450,
				"seek": 20793,
				"start": 1370.8353,
				"end": 1372.7954,
				"text": " I mean, the really, really dumb model can,",
				"tokens": [
					51585,
					286,
					914,
					11,
					264,
					534,
					11,
					534,
					10316,
					2316,
					393,
					11,
					51683
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 451,
				"seek": 20793,
				"start": 1372.8553,
				"end": 1374.6953,
				"text": " but modern models are just not going to have that problem.",
				"tokens": [
					51686,
					457,
					4363,
					5245,
					366,
					445,
					406,
					516,
					281,
					362,
					300,
					1154,
					13,
					51778
				],
				"temperature": 0,
				"avg_logprob": -0.19242069,
				"compression_ratio": 1.8385965,
				"no_speech_prob": 1.1333492e-12
			},
			{
				"id": 452,
				"seek": 23619,
				"start": 1375.3154,
				"end": 1377.3553,
				"text": " So if you're going to do few shot prompting, be clever about it.",
				"tokens": [
					50396,
					407,
					498,
					291,
					434,
					516,
					281,
					360,
					1326,
					3347,
					12391,
					278,
					11,
					312,
					13494,
					466,
					309,
					13,
					50498
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 453,
				"seek": 23619,
				"start": 1377.4553,
				"end": 1379.7354,
				"text": " Think about what you're really trying to have the few shot example",
				"tokens": [
					50503,
					6557,
					466,
					437,
					291,
					434,
					534,
					1382,
					281,
					362,
					264,
					1326,
					3347,
					1365,
					50617
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 454,
				"seek": 23619,
				"start": 1379.7354,
				"end": 1383.4154,
				"text": " explain to the model and only put the minimum number of tokens",
				"tokens": [
					50617,
					2903,
					281,
					264,
					2316,
					293,
					787,
					829,
					264,
					7285,
					1230,
					295,
					22667,
					50801
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 455,
				"seek": 23619,
				"start": 1383.4154,
				"end": 1385.3154,
				"text": " you need to explain that concept itself.",
				"tokens": [
					50801,
					291,
					643,
					281,
					2903,
					300,
					3410,
					2564,
					13,
					50896
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 456,
				"seek": 23619,
				"start": 1385.8553,
				"end": 1389.2354,
				"text": " And not just number of tokens, but like meaning of tokens, right?",
				"tokens": [
					50923,
					400,
					406,
					445,
					1230,
					295,
					22667,
					11,
					457,
					411,
					3620,
					295,
					22667,
					11,
					558,
					30,
					51092
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 457,
				"seek": 23619,
				"start": 1389.2354,
				"end": 1392.3754,
				"text": " Like that dot dot token means placeholder to the model.",
				"tokens": [
					51092,
					1743,
					300,
					5893,
					5893,
					14862,
					1355,
					1081,
					20480,
					281,
					264,
					2316,
					13,
					51249
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 458,
				"seek": 23619,
				"start": 1392.6554,
				"end": 1395.7754,
				"text": " Exactly. Yeah. And to almost anyone else reading it, it's the same thing.",
				"tokens": [
					51263,
					7587,
					13,
					865,
					13,
					400,
					281,
					1920,
					2878,
					1646,
					3760,
					309,
					11,
					309,
					311,
					264,
					912,
					551,
					13,
					51419
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 459,
				"seek": 23619,
				"start": 1395.8954,
				"end": 1397.3954,
				"text": " Same with dynamic few shot prompting.",
				"tokens": [
					51425,
					10635,
					365,
					8546,
					1326,
					3347,
					12391,
					278,
					13,
					51500
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 460,
				"seek": 23619,
				"start": 1397.7354,
				"end": 1400.7354,
				"text": " Sometimes you might want to put few shots that are actually similar to your example.",
				"tokens": [
					51517,
					4803,
					291,
					1062,
					528,
					281,
					829,
					1326,
					8305,
					300,
					366,
					767,
					2531,
					281,
					428,
					1365,
					13,
					51667
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 461,
				"seek": 23619,
				"start": 1401.3954,
				"end": 1403.6953,
				"text": " Sometimes you might want to put few shots because they're completely",
				"tokens": [
					51700,
					4803,
					291,
					1062,
					528,
					281,
					829,
					1326,
					8305,
					570,
					436,
					434,
					2584,
					51815
				],
				"temperature": 0,
				"avg_logprob": -0.16465674,
				"compression_ratio": 1.9347826,
				"no_speech_prob": 1.6621269e-12
			},
			{
				"id": 462,
				"seek": 26519,
				"start": 1403.6953,
				"end": 1407.6354,
				"text": " the opposite of your example. And sometimes you might want to pick a few shots that are unrelated",
				"tokens": [
					50365,
					264,
					6182,
					295,
					428,
					1365,
					13,
					400,
					2171,
					291,
					1062,
					528,
					281,
					1888,
					257,
					1326,
					8305,
					300,
					366,
					38967,
					50562
				],
				"temperature": 0,
				"avg_logprob": -0.06506421,
				"compression_ratio": 1.7309091,
				"no_speech_prob": 1.0565335e-12
			},
			{
				"id": 463,
				"seek": 26519,
				"start": 1407.6354,
				"end": 1411.5953,
				"text": " to your example. Let's say you're in a healthcare setting and you're processing doctor-patient",
				"tokens": [
					50562,
					281,
					428,
					1365,
					13,
					961,
					311,
					584,
					291,
					434,
					294,
					257,
					8884,
					3287,
					293,
					291,
					434,
					9007,
					4631,
					12,
					29752,
					50760
				],
				"temperature": 0,
				"avg_logprob": -0.06506421,
				"compression_ratio": 1.7309091,
				"no_speech_prob": 1.0565335e-12
			},
			{
				"id": 464,
				"seek": 26519,
				"start": 1411.5953,
				"end": 1420.0753,
				"text": " conversations. You probably don't want to talk about like, I don't know, like leg fractures",
				"tokens": [
					50760,
					7315,
					13,
					509,
					1391,
					500,
					380,
					528,
					281,
					751,
					466,
					411,
					11,
					286,
					500,
					380,
					458,
					11,
					411,
					1676,
					17948,
					1303,
					51184
				],
				"temperature": 0,
				"avg_logprob": -0.06506421,
				"compression_ratio": 1.7309091,
				"no_speech_prob": 1.0565335e-12
			},
			{
				"id": 465,
				"seek": 26519,
				"start": 1420.0753,
				"end": 1425.7754,
				"text": " if you're talking very commonly, if the person currently has a leg fracture. That's just going",
				"tokens": [
					51184,
					498,
					291,
					434,
					1417,
					588,
					12719,
					11,
					498,
					264,
					954,
					4362,
					575,
					257,
					1676,
					36877,
					13,
					663,
					311,
					445,
					516,
					51469
				],
				"temperature": 0,
				"avg_logprob": -0.06506421,
				"compression_ratio": 1.7309091,
				"no_speech_prob": 1.0565335e-12
			},
			{
				"id": 466,
				"seek": 26519,
				"start": 1425.7754,
				"end": 1430.3754,
				"text": " to mess up the accuracy of the system, no matter what you do. You probably don't want to put the",
				"tokens": [
					51469,
					281,
					2082,
					493,
					264,
					14170,
					295,
					264,
					1185,
					11,
					572,
					1871,
					437,
					291,
					360,
					13,
					509,
					1391,
					500,
					380,
					528,
					281,
					829,
					264,
					51699
				],
				"temperature": 0,
				"avg_logprob": -0.06506421,
				"compression_ratio": 1.7309091,
				"no_speech_prob": 1.0565335e-12
			},
			{
				"id": 467,
				"seek": 29187,
				"start": 1430.3754,
				"end": 1435.3353,
				"text": " person's name as the same name as your patient in your system. You probably don't want the doctor's",
				"tokens": [
					50365,
					954,
					311,
					1315,
					382,
					264,
					912,
					1315,
					382,
					428,
					4537,
					294,
					428,
					1185,
					13,
					509,
					1391,
					500,
					380,
					528,
					264,
					4631,
					311,
					50613
				],
				"temperature": 0,
				"avg_logprob": -0.10085523,
				"compression_ratio": 1.9,
				"no_speech_prob": 1.7014931e-12
			},
			{
				"id": 468,
				"seek": 29187,
				"start": 1435.3353,
				"end": 1439.2954,
				"text": " name to be the same. You probably don't want the hospital name to be the same. You probably don't",
				"tokens": [
					50613,
					1315,
					281,
					312,
					264,
					912,
					13,
					509,
					1391,
					500,
					380,
					528,
					264,
					4530,
					1315,
					281,
					312,
					264,
					912,
					13,
					509,
					1391,
					500,
					380,
					50811
				],
				"temperature": 0,
				"avg_logprob": -0.10085523,
				"compression_ratio": 1.9,
				"no_speech_prob": 1.7014931e-12
			},
			{
				"id": 469,
				"seek": 29187,
				"start": 1439.2954,
				"end": 1444.3353,
				"text": " want to confuse it with dates in case of continuity of any kind. So finding this sort of stuff out",
				"tokens": [
					50811,
					528,
					281,
					28584,
					309,
					365,
					11691,
					294,
					1389,
					295,
					23807,
					295,
					604,
					733,
					13,
					407,
					5006,
					341,
					1333,
					295,
					1507,
					484,
					51063
				],
				"temperature": 0,
				"avg_logprob": -0.10085523,
				"compression_ratio": 1.9,
				"no_speech_prob": 1.7014931e-12
			},
			{
				"id": 470,
				"seek": 29187,
				"start": 1444.3353,
				"end": 1449.9154,
				"text": " and picking the right FUTHOT example can be good, but as you're probably figuring this out,",
				"tokens": [
					51063,
					293,
					8867,
					264,
					558,
					479,
					8709,
					39,
					5068,
					1365,
					393,
					312,
					665,
					11,
					457,
					382,
					291,
					434,
					1391,
					15213,
					341,
					484,
					11,
					51342
				],
				"temperature": 0,
				"avg_logprob": -0.10085523,
				"compression_ratio": 1.9,
				"no_speech_prob": 1.7014931e-12
			},
			{
				"id": 471,
				"seek": 29187,
				"start": 1450.0753,
				"end": 1454.1154,
				"text": " it's a lot of work to be FUTHOT prompting correctly. So most people are better off not",
				"tokens": [
					51350,
					309,
					311,
					257,
					688,
					295,
					589,
					281,
					312,
					479,
					8709,
					39,
					5068,
					12391,
					278,
					8944,
					13,
					407,
					881,
					561,
					366,
					1101,
					766,
					406,
					51552
				],
				"temperature": 0,
				"avg_logprob": -0.10085523,
				"compression_ratio": 1.9,
				"no_speech_prob": 1.7014931e-12
			},
			{
				"id": 472,
				"seek": 31561,
				"start": 1454.1154,
				"end": 1455.2153,
				"text": " doing it rather than doing it.",
				"tokens": [
					50365,
					884,
					309,
					2831,
					813,
					884,
					309,
					13,
					50420
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 473,
				"seek": 31561,
				"start": 1457.3754,
				"end": 1458.6554,
				"text": " Let's go on to the next topic.",
				"tokens": [
					50528,
					961,
					311,
					352,
					322,
					281,
					264,
					958,
					4829,
					13,
					50592
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 474,
				"seek": 31561,
				"start": 1458.7954,
				"end": 1459.5554,
				"text": " Unless there's more questions.",
				"tokens": [
					50599,
					16581,
					456,
					311,
					544,
					1651,
					13,
					50637
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 475,
				"seek": 31561,
				"start": 1459.7354,
				"end": 1461.7354,
				"text": " Feel free to keep typing questions in the chat",
				"tokens": [
					50646,
					14113,
					1737,
					281,
					1066,
					18444,
					1651,
					294,
					264,
					5081,
					50746
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 476,
				"seek": 31561,
				"start": 1461.7354,
				"end": 1462.8954,
				"text": " if you have any.",
				"tokens": [
					50746,
					498,
					291,
					362,
					604,
					13,
					50804
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 477,
				"seek": 31561,
				"start": 1465.3154,
				"end": 1466.8553,
				"text": " We might have to do a follow-up session",
				"tokens": [
					50925,
					492,
					1062,
					362,
					281,
					360,
					257,
					1524,
					12,
					1010,
					5481,
					51002
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 478,
				"seek": 31561,
				"start": 1466.8553,
				"end": 1468.5753,
				"text": " and do the coding side",
				"tokens": [
					51002,
					293,
					360,
					264,
					17720,
					1252,
					51088
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 479,
				"seek": 31561,
				"start": 1468.5753,
				"end": 1469.6753,
				"text": " because I don't know if we're going to have time.",
				"tokens": [
					51088,
					570,
					286,
					500,
					380,
					458,
					498,
					321,
					434,
					516,
					281,
					362,
					565,
					13,
					51143
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 480,
				"seek": 31561,
				"start": 1470.6554,
				"end": 1471.0554,
				"text": " Yes.",
				"tokens": [
					51192,
					1079,
					13,
					51212
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 481,
				"seek": 31561,
				"start": 1471.3353,
				"end": 1473.5154,
				"text": " I really want to see an example",
				"tokens": [
					51226,
					286,
					534,
					528,
					281,
					536,
					364,
					1365,
					51335
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 482,
				"seek": 31561,
				"start": 1473.5154,
				"end": 1475.3553,
				"text": " of how to look at the responses.",
				"tokens": [
					51335,
					295,
					577,
					281,
					574,
					412,
					264,
					13019,
					13,
					51427
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 483,
				"seek": 31561,
				"start": 1476.6354,
				"end": 1477.2554,
				"text": " I'll check on the code.",
				"tokens": [
					51491,
					286,
					603,
					1520,
					322,
					264,
					3089,
					13,
					51522
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 484,
				"seek": 31561,
				"start": 1477.6953,
				"end": 1478.3954,
				"text": " Okay, cool.",
				"tokens": [
					51544,
					1033,
					11,
					1627,
					13,
					51579
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 485,
				"seek": 31561,
				"start": 1479.1354,
				"end": 1481.1554,
				"text": " I did want to talk about all these topics.",
				"tokens": [
					51616,
					286,
					630,
					528,
					281,
					751,
					466,
					439,
					613,
					8378,
					13,
					51717
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 486,
				"seek": 31561,
				"start": 1481.2754,
				"end": 1482.4154,
				"text": " I thought they were really, really interesting.",
				"tokens": [
					51723,
					286,
					1194,
					436,
					645,
					534,
					11,
					534,
					1880,
					13,
					51780
				],
				"temperature": 0,
				"avg_logprob": -0.28310034,
				"compression_ratio": 1.658363,
				"no_speech_prob": 3.4101782e-12
			},
			{
				"id": 487,
				"seek": 34391,
				"start": 1483.1354,
				"end": 1485.3954,
				"text": " I think the last one that I thought is worth...",
				"tokens": [
					50401,
					286,
					519,
					264,
					1036,
					472,
					300,
					286,
					1194,
					307,
					3163,
					485,
					50514
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 488,
				"seek": 34391,
				"start": 1485.3954,
				"end": 1486.4154,
				"text": " Actually, we'll talk about this one first",
				"tokens": [
					50514,
					5135,
					11,
					321,
					603,
					751,
					466,
					341,
					472,
					700,
					50565
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 489,
				"seek": 34391,
				"start": 1486.4154,
				"end": 1486.9954,
				"text": " because it's really easy.",
				"tokens": [
					50565,
					570,
					309,
					311,
					534,
					1858,
					13,
					50594
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 490,
				"seek": 34391,
				"start": 1488.1753,
				"end": 1490.9353,
				"text": " What I read is very much about context compression,",
				"tokens": [
					50653,
					708,
					286,
					1401,
					307,
					588,
					709,
					466,
					4319,
					19355,
					11,
					50791
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 491,
				"seek": 34391,
				"start": 1491.3154,
				"end": 1493.0154,
				"text": " at least the way I interpreted this one,",
				"tokens": [
					50810,
					412,
					1935,
					264,
					636,
					286,
					26749,
					341,
					472,
					11,
					50895
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 492,
				"seek": 34391,
				"start": 1493.3754,
				"end": 1495.5753,
				"text": " which wasn't about use the faucets in the context.",
				"tokens": [
					50913,
					597,
					2067,
					380,
					466,
					764,
					264,
					49567,
					1385,
					294,
					264,
					4319,
					13,
					51023
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 493,
				"seek": 34391,
				"start": 1495.7754,
				"end": 1496.5554,
				"text": " That's one way to do it.",
				"tokens": [
					51033,
					663,
					311,
					472,
					636,
					281,
					360,
					309,
					13,
					51072
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 494,
				"seek": 34391,
				"start": 1496.5953,
				"end": 1497.7554,
				"text": " But what I was hearing was,",
				"tokens": [
					51074,
					583,
					437,
					286,
					390,
					4763,
					390,
					11,
					51132
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 495,
				"seek": 34391,
				"start": 1498.1753,
				"end": 1500.8754,
				"text": " instead of putting in the entire chunk all the time",
				"tokens": [
					51153,
					2602,
					295,
					3372,
					294,
					264,
					2302,
					16635,
					439,
					264,
					565,
					51288
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 496,
				"seek": 34391,
				"start": 1500.8754,
				"end": 1501.9553,
				"text": " at all points,",
				"tokens": [
					51288,
					412,
					439,
					2793,
					11,
					51342
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 497,
				"seek": 34391,
				"start": 1503.0953,
				"end": 1504.6554,
				"text": " see if you can break down the system",
				"tokens": [
					51399,
					536,
					498,
					291,
					393,
					1821,
					760,
					264,
					1185,
					51477
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 498,
				"seek": 34391,
				"start": 1504.6554,
				"end": 1508.6154,
				"text": " and only put in the least amount of context that's relevant.",
				"tokens": [
					51477,
					293,
					787,
					829,
					294,
					264,
					1935,
					2372,
					295,
					4319,
					300,
					311,
					7340,
					13,
					51675
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 499,
				"seek": 34391,
				"start": 1509.1154,
				"end": 1509.8353,
				"text": " So for example,",
				"tokens": [
					51700,
					407,
					337,
					1365,
					11,
					51736
				],
				"temperature": 0,
				"avg_logprob": -0.18470179,
				"compression_ratio": 1.7058823,
				"no_speech_prob": 2.125281e-12
			},
			{
				"id": 500,
				"seek": 37133,
				"start": 1509.8353,
				"end": 1514.3954,
				"text": " if every uh and like i think that's the whole point everything is meant to be restorable the",
				"tokens": [
					50365,
					498,
					633,
					2232,
					293,
					411,
					741,
					519,
					300,
					311,
					264,
					1379,
					935,
					1203,
					307,
					4140,
					281,
					312,
					1472,
					15249,
					264,
					50593
				],
				"temperature": 0,
				"avg_logprob": -0.073492035,
				"compression_ratio": 2.0944207,
				"no_speech_prob": 1.5015713e-12
			},
			{
				"id": 501,
				"seek": 37133,
				"start": 1514.3954,
				"end": 1518.7754,
				"text": " idea is that if you're having a whole website here instead of actually putting the website over here",
				"tokens": [
					50593,
					1558,
					307,
					300,
					498,
					291,
					434,
					1419,
					257,
					1379,
					3144,
					510,
					2602,
					295,
					767,
					3372,
					264,
					3144,
					670,
					510,
					50812
				],
				"temperature": 0,
				"avg_logprob": -0.073492035,
				"compression_ratio": 2.0944207,
				"no_speech_prob": 1.5015713e-12
			},
			{
				"id": 502,
				"seek": 37133,
				"start": 1518.7754,
				"end": 1524.0554,
				"text": " just put the url over here and replace the original observation action two with just the url and say",
				"tokens": [
					50812,
					445,
					829,
					264,
					4038,
					75,
					670,
					510,
					293,
					7406,
					264,
					3380,
					14816,
					3069,
					732,
					365,
					445,
					264,
					4038,
					75,
					293,
					584,
					51076
				],
				"temperature": 0,
				"avg_logprob": -0.073492035,
				"compression_ratio": 2.0944207,
				"no_speech_prob": 1.5015713e-12
			},
			{
				"id": 503,
				"seek": 37133,
				"start": 1524.0554,
				"end": 1528.9954,
				"text": " from this url i got this action from this url i got this action from this url i got this action",
				"tokens": [
					51076,
					490,
					341,
					4038,
					75,
					741,
					658,
					341,
					3069,
					490,
					341,
					4038,
					75,
					741,
					658,
					341,
					3069,
					490,
					341,
					4038,
					75,
					741,
					658,
					341,
					3069,
					51323
				],
				"temperature": 0,
				"avg_logprob": -0.073492035,
				"compression_ratio": 2.0944207,
				"no_speech_prob": 1.5015713e-12
			},
			{
				"id": 504,
				"seek": 37133,
				"start": 1528.9954,
				"end": 1536.2954,
				"text": " and then oh i see so the model the model may even load the thing into context and then you choose",
				"tokens": [
					51323,
					293,
					550,
					1954,
					741,
					536,
					370,
					264,
					2316,
					264,
					2316,
					815,
					754,
					3677,
					264,
					551,
					666,
					4319,
					293,
					550,
					291,
					2826,
					51688
				],
				"temperature": 0,
				"avg_logprob": -0.073492035,
				"compression_ratio": 2.0944207,
				"no_speech_prob": 1.5015713e-12
			},
			{
				"id": 505,
				"seek": 39779,
				"start": 1536.2954,
				"end": 1538.3353,
				"text": " the next action and then you stitch the",
				"tokens": [
					50365,
					264,
					958,
					3069,
					293,
					550,
					291,
					5635,
					264,
					50467
				],
				"temperature": 0,
				"avg_logprob": -0.17261775,
				"compression_ratio": 1.0793651,
				"no_speech_prob": 6.307026e-13
			},
			{
				"id": 506,
				"seek": 39779,
				"start": 1538.3353,
				"end": 1539.9753,
				"text": " actual content back and pull",
				"tokens": [
					50467,
					3539,
					2701,
					646,
					293,
					2235,
					50549
				],
				"temperature": 0,
				"avg_logprob": -0.17261775,
				"compression_ratio": 1.0793651,
				"no_speech_prob": 6.307026e-13
			}
		],
		"x_groq": {
			"id": "req_01k7hvnnskfxqamwpyn558c1x3"
		}
	},
	{
		"task": "transcribe",
		"language": "English",
		"duration": 1405.35340125,
		"text": " pull it out of the context window so that like it can be restored later. It can be pulled back in, but it's not by default present. Exactly. And you can even tell this is a restorable tool, restorable data blob, restore with this key. And the model can then, if it needs to call the restore action. And yes, that increases the number of tools that you have to make to call your system, but it dramatically makes your context window way more efficient because most of the time, for example, for a coding agent, Imagine trying to keep every single file of the models that have to edit in the context and all the edits are made in the context. It's kind of useless. Like once it's made a file edit, I can kind of compress and say, I'm done with this file. The user is happy and it works. Here's a summary of what the file, what changes I made. But keeping the actual code that it generated is kind of useless for that kind of task. And then when I tell the model, hey, actually that file is wrong, then it can load the load actions and load the file again, reread it and go do the work. And there's a trade-off here between how many tool calls it takes versus how accurate the system is versus how big of a context window I have. But if you don't make this a possibility, then you kind of live in this world where you have to live in big context land. And then you're just hoping that GPT-26 has a 20 million token window. And maybe it will, but I really don't want to work in a world where every computer needs to have 50 terabytes of RAM or else it's useless. That's just a sad world to have to live in. And also like phones. I would like stuff to work on my phone without needing to go into this. A duck asked a question. How does it work with KV caching? Well, it really depends. If your system, I think an important thing to notice about KV caches and like people over-index on this sometimes. And I know I was trying to explain it so perhaps that was implying this, but look, KV caches don't, Caching and latency doesn't matter for small stuff. If most of your chat messages in your app or agentic loops are like one or two system calls long, don't do any of this crap. It is not worth it. Just make your system work. This stuff only matters for stuff that is going to run for a while. If your tasks are taking like a minute to run, yes, then KV cache matters. But in that case, if your task is taking a minute to run because it's calling like 50 different tool calls, you'll probably get way better KV cache optimization simply by changing the observations in this way as well, where like now I get a KV cache here and only if I load in a new observation do I break the KV cache because then I'm going to load observation one and then compress it into this format. So now this thing becomes compressed. But what I can do here is I can design an algorithm here that's pretty straightforward that goes ahead and says, hey, after about 15 observations, I always compress the oldest one. So yes, I broke my KV cache, but at any given time, all my previous context is always going to be similar, and I'll basically force the model to reload it. Because this, once it's compressed, is never reloaded ever again. So this basically stabilizes over time. Does that answer the question, Doc? Perfect. and then the debate of showing tool calls and what's out but yeah yeah I think you got it perfectly dug yeah exactly once it once it gets the next step and it knows what to do then it can do it and I think this is also like when people do there's like certain like compaction strategies that you can do like Anthropic talks about micro compaction which is like pulling at least like the tool calls out automatically but it's really hard to do this in a general purpose way which is what makes this paper so impressive is like they've done this for a general purpose agent. If you're building an agent for a very specific thing, you can optimize the heck out of it for that one use case. And you know exactly like I know for sure if a model reads this type of medical document, it's and once it makes the decision, it never needs to do it again. Or you can say, OK, once the model reads it and picks the next thing, I need to pass it to another model to summarize the document, put that in instead of the whole content and like do your own like deterministic compaction based on what types of things are being pulled into the context. Exactly. Now, I want to talk about the last part that I thought was super, super innovative and really changed my perspective a little bit on some things as well, which is around how tool calls work. So we talked about the KB cache. We talked about how having continuity and not breaking continuity matters. So we talked, I think one easy way to go fix this is you can take your tool calls and simply put them always guaranteed, put them at the end of your context window. And now you always have some amount of caching that you get for free. But what if you did put them at the top? Well, many times you're building an agent, you'll want to invalidate and change your tool calls dynamically. That is 100% of the time going to break your KV cache and you will just get slower latency all the way through and through. This was what the Swarm, like the original iteration of like the OpenAI Swarms framework or whatever, their way of doing multi-agent was to take the same context window, pass it to a different agent, which had a different system message and a different set of tools. Exactly. And like, it just doesn't really work. But what they end up doing here that I thought was really fascinating Instead of actually giving you these tools, they actually leave the tools in there. And what they do is they modify... Let me change. Let me delete. They modify a part of the system that I think most people don't think about, which is... Let me open up the OpenAI docs because I think that's going to show OpenAI responses API. Okay, cool. they modify the logits coming out of it. There you go And you can basically invalidate certain tokens as being valid out of the model So I don actually know how you modify the function calling tokens I can go look into that because it sounds like I don't know if OpenAI gives you those, but if you own the model, you can definitely do this. But the premise is, there we go. The premise is like, what the heck is function calling doing? And we need to go understand that briefly to really be able to understand and appreciate this technique. So what function calling does is function calling teaches a model about a special token called use tool. And the model outputs a token that says, I'm going to output a tool. And once it decides to do that token, the model providers then restrict the model to only pick tokens that match the tool specification that you have provided. Is this constrained decoding or is this still just the base like JSON mode? this is it so the base json mode does that which is it basically takes it basically forces the model to only output grammars that are valid uh json yeah yeah yeah constrained decoding constrained generation or decoding whatever you want to call it is basically the more general form of that where instead of taking the json grammar you can basically provide it the grammar of your choice any regex and that just zeros out all of the log probabilities for anything that doesn't match your regex right exactly so if i go back to the whiteboard what this is doing is this is basically saying like even though the model has a really high probability for it i'll vote this is zero because it doesn't match the regex of what's allowed so only the only the regex is the valid consideration and then i pick the best token from the valid regex now the problem that you will run into with this is sorry i have a lot of tabs open today yeah we talked about this a lot this concept in cracking the prompting interview, right? That was the one you talked about like how to get AI to write better code by allowing it to write things that might not necessarily be valid JSON, but are like closer to the way that the model has been trained to write code, which is by like reading code, not as snippets and JSON strings. The function calling system does basically this. It basically says, instead of having the model always follow the grammar, it says, hey, you will emit a special token that says you want to take an action and then I will force you to follow the grammar. So at that point, the model is basically deciding to some degree it can be taught when it should follow the grammar that it's given. And now OpenAID has a new thing, which I think is really, really good for everyone, which is they've allowed you to do function calling without a grammar or with a custom grammar. So now you can basically let the model say, I want to use an action and then let it freeform output whatever it wants. But what the Manus paper article talks about is that when it actually picks the use tool action at that point the model is now going to pick a special token it's going to pick a certain uh the first thing that comes out is the name of the function right exactly it's gonna have to spit out the name of the function instead of actually letting the model spit out the name of the function at that point you can effectively where'd it go you can effectively prevent the model from picking a certain tool at that point and say that by just zeroing the probability that the name of the function matches, like the basically like forcing, only allowing the log probs that actually match the tool set that you want to constrain to. Exactly. So this is actually a really, really good technique, but I'm going to show you, and I thought it's really, really clever, but I'm going to show you how naming your tools in an interesting way can dramatically change the accuracy of this technique. Cursor. And I'm going to do this with an example, I think, in code, because it's going to be a little bit better. And just while you're doing that, the question was like, when talking about tool calls, are we talking about explicit tool calling by model providers? Or are we also thinking about whatever structured output techniques we're using? I think tool calling, structured output, and function calling, while there are different flavors of each of them, I think those three words tend to refer to the same thing. Some people say like, oh, tool calling is only when you use JSON mode and structured output is, can, doesn't even need to use JSON mode. But again, like I use all three of those terms interchangeably, tool calling, structured output, function calling. They're all the same. Is that accurate? I agree. There's technical implementation differences that might have different trade-offs, but they're all the same. Fundamentally, what you're doing is trying to constrain what the model outputs to some degree in some manner or form. Well, and it's the constraint. It's not even about the constraining. I think for me, it's more so about we're going to create something that a deterministic program can consume. It's not for a human and it's not for a model. It's for Python code that I wrote. And so it has to have some expected structure that I can turn it into bytes in memory. Yeah. So let's say I have these three tools, right? And these are the tools I have. Call me, call mom, call texter. And let's say I'm operating the mode in like work mode. in work mode i want the tools call dexter and call me to be available in non-work mode so like uh i want the tools call mom and call me to be available i'll never call dexter non-work mode i don't like hanging out with it that much so well what we're going to do the model decide i'll say a statement like uh i need to talk to someone i'm going to go write the statement in this case call me is clearly not a good tool because I want to talk to someone else, not myself. So it should call mom or Dexter. If I give it all the tools as context, it could really pick any of them as random. In work mode, however, maybe there's another thread in here that says like, I'm at work. So the model and there some information I have in my program state that allows me to know that So I have some at work Boolean as true as true or false if it is i basically want to say like disable call mom and i want to go disable call mom at that point if i going to go do that. Sorry, I'm going to turn this off text. That is not correct. If I'm going to go do that, the model will start generating tokens. Now, the problem with this is the model may want to generate a token that starts, maybe the token vocabulary makes it so that the tokens are actually call and then mom. So the tokens end up being call and then mom. And over here, the tokens end up being call and then dexter if the model generates a token call even though i've invalidated this i've invalidated call mom the model may have actually thought it was really important to call mom and it may be actually accidentally forced to picking the call dexter tool forced to call me yeah because it doesn't have a choice because it produced the word call uh now with the intent to call mom with the intent to call mom because i'm at work mode and I've accidentally disabled the call mom tool. And now the model thinks it wants to call mom because all my tools are in the context window. So even if the model doesn't know, it can't actually pick call mom. As far as the model knows, it can pick call mom. So now it might even go do this. It might even start outputting call m because m was a valid token as well in my token vocabulary. And funnily enough, the model might not even end up calling dux or it might end up calling me. Because in the token vocabulary, we have call. We obviously have the letter M. We have the letter me. We have the word me. We have the letter mom. We probably have the words MO as well. Oh, okay. So the probability for mom might have been 99% and the probability for M might have been 1%. But because both of them are moving towards mom, those are the ones that got picked. Exactly. And MO might also be higher. So I've invalidated MO and mom because it doesn't meet the grammar of what's allowed based on what tools are valid, but I still will call M. So I'll call M. And now I'm going to call M. Well, what's the next best token? Well, there's only one valid token that's available here. I have to call it E. So now I'll end up calling the call me tool. And you can see how actually doing this can actually backfire in certain ways if you're not careful about how you're naming your tools. So if you're going to use this technique, it is important to be able to understand how these models work and build an intuition for this on your own. So then when something fails or doesn't fail, you can go iterate on this and make it better. Does that, any questions? Yeah, other questions about that? There was a lot of content in there. Otherwise, we're going to move to some of these questions. Call me versus call me variable name. No, that's totally arbitrary, I think. I want to show you like a really fun post by someone. Oh, that's a long thing. Let's not show my DMs. While you're doing that, Dex Horthy, with the understanding of tool calling can be either an explicit tool call or structured output, pick your flavor. In the work that you are doing, have you noticed any difference in agent performance when using model provider tool calling versus BAML structured output, basically home roll tool calling. I have no stake in this, but I believe there's a lot of agents out there that are built with native tool calling that would be better if they use the BAML structured output for some of the reasons we've kind of touched on here, but other things. But I need VibeAV to ship the benchmark again. When are we getting V2 of your tool calling benchmark? Sean showed a small benchmark already that showed. That's true. Quickly better. But I want to show everyone an accurate example of where this matters. So like in this case, the user was doing some sort of tool calling with Kimi K2 model. And they asked the model to output approach. And no matter what they did, like around like 2% of the time, Kimi K2 would literally just pull out a pro brace. And this is so wild that it would actually do this. And I started looking into this and eventually I was like, okay, well, there's prompt engineering techniques that you can do to go do this. So they did that. and eventually did they just add an alias to the field? well eventually what I did was I just wanted to go understand this I was like so what did I do? I dumped out the tokenizer I literally took the tokenizer for the K2 model and I dumped it out and lo and behold approach is two tokens so of course the model that's a dumber model is going to get this wrong because at this point it wants to do approach and then it's like it's just too dumb to actually do this so the solution this user had was they actually found a word that was a single token and now it works. That's so sick. And just like, but the point is like, if your tool calling is going to use similar technique, you have to understand that there are trade-offs to how your tokenizer vocabulary works. And the bigger the model, the shorter your context window, the less they matter. The smaller the model, the longer your context window, the more they matter. And knowing that is just key to getting success over here. Love it. Yeah. And remember, tokenizers are different for every model. So you have to like, this user could have spent forever trying to prompt engineer the way the heck out of this, trying to make it work, and they did. And I think Sam and our team tried this too. And I tried a couple techniques. And literally at some point it like the model is just too stupid what going on And I had to go and dig deep So don be afraid to go do that Just literally run the tokenizer code and just go see what happening Really, really easy hack to go make this work. Any model provider that doesn't give you the tokenizer, it's kind of annoying. Just have the model provider spit back out the word to you and count the tokens out of the HTTP request. That's often what I'll do with like anthropic models because they don't actually give me the tokenizer. So I'll say- Oh, you say, hey, just say the word approach to me. and then you look at the token counts in the raw JSON that comes back and say, how many tokens was that? Literally what I do. Okay. And it kind of works. Yeah, if you're curious about the benchmarks, I can share them afterwards on the email that we send out if you're interested or in our Discord, if you ask, we'll have Discord. Slava's question real quick. Yeah, we put all the streams in the GitHub repo. That's where you can find them. Yeah, do you want to pull it up? uh there it is so you can come here you can see every every recording we've done since march you can see the youtube you can see the code um you can sign up for the next one and if all you really know about dg if all you really want to do is just watch all the content we have a playlist somewhere um that you can find and i don't hear my voice again but i'll have everything on there and then i think we tried to make a interesting version of this where we actually do this with Dexter and you can actually scroll through here as well. Oh, man. Wait, this is sick. Wait, I want this on humanlayer.dev too. I'll send you the code. You can post it over. Amazing. Do you much, and then point the resource, try this out locally. The thing about we'll try and give you some sample code to go try this out locally, but honestly, for most of these problems, just take any of the problems that you have that you've already been working on and just like go look at the inspect the output elements that are coming out of these problems. So like for example, let's just write a really quick agent. Let's see if I have screen, if I don't have, let's see if I don't have screen sharing. Okay, cool. I'm going to disable screen sharing really fast while I copy the curl with my OpenAI key. and then I will bring it back. Copy. Clear. Okay. I'm going to screen share again. So if I run this API key, this request, I can just see that it literally cached nothing. And the reason it didn't cache anything is literally because I just have too small little context window. If I just increase this context window by make this a real resume example that is dense. And I'll eventually do stuff. Unlux all the stuff. Cool. Let's copy and paste this. And let's go run this again. it still didn't cache anything i have to make it even bigger but the whole point is right over here you need to go ahead and actually understand why this is happening or not happening and part of the reason here is just i know this is too smaller too uh too small of a text do you show the token count it's right here uh oh i see they won't even trigger caching here triple the resume. And just to be clear, you're just running the same request twice and seeing if it auto caches it based on the shared prefix. Exactly. And what I'm doing here is I'm just trying to have it triple the resume and that'll do some stuff probably. And hopefully I'll make it long enough. But the way that I can test this is just I can keep making test cases until I'm satisfied with the final output. And I'm going to see if it's actually long enough. Clear. Eventually, I'll put something. I'm going to get a lot slower. I'm still not long enough. But you're getting the point of how I would go do this. I know that OpenAI is probably caching out about 1024 tokens based on what they said in their docs. So in that case, I'm just going to make an example that has 1024 tokens. And I ran into this personally myself a few times when I was unit testing some of the code that we write because I was like, oh, why is caching not working? Well, it turned out I literally just wasn't implementing a long enough prompt and it would just break all the time. But once I actually implemented a big enough prompt, I could consistently see hits on caching pretty well. Other questions from anyone? Otherwise, I think we're 20 minutes over. I answered Vijay's question. It's about cloud code proxying. I'm actually, I'm in Austin this week with the Gauntlet AI squad. They have like a school here for learning AI engineers. And apparently one of the students here built a thing called CC Proxy that lets you just like strip all of the traces out of your cloud code running locally. That's cool. That's cool. I guess that's it for today's conversation in that case. Thank you guys for joining. Hopefully it was educational and hopefully everyone learned a few things. We'll have a new topic for next week that I think will be hopefully just as educational. It's going to be dope. Thank you all for coming. Thanks, ViBot, for running the session today. And thanks, everyone. Thanks, everyone here. I'll see you later.",
		"segments": [
			{
				"id": 0,
				"seek": 0,
				"start": 0,
				"end": 4.4,
				"text": " pull it out of the context window so that like it can be restored later.",
				"tokens": [
					50365,
					2235,
					309,
					484,
					295,
					264,
					4319,
					4910,
					370,
					300,
					411,
					309,
					393,
					312,
					23143,
					1780,
					13,
					50585
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 1,
				"seek": 0,
				"start": 4.4,
				"end": 7.24,
				"text": " It can be pulled back in, but it's not by default present.",
				"tokens": [
					50585,
					467,
					393,
					312,
					7373,
					646,
					294,
					11,
					457,
					309,
					311,
					406,
					538,
					7576,
					1974,
					13,
					50727
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 2,
				"seek": 0,
				"start": 7.66,
				"end": 11.16,
				"text": " Exactly. And you can even tell this is a restorable tool,",
				"tokens": [
					50748,
					7587,
					13,
					400,
					291,
					393,
					754,
					980,
					341,
					307,
					257,
					1472,
					15249,
					2290,
					11,
					50923
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 3,
				"seek": 0,
				"start": 11.52,
				"end": 14.12,
				"text": " restorable data blob, restore with this key.",
				"tokens": [
					50941,
					1472,
					15249,
					1412,
					46115,
					11,
					15227,
					365,
					341,
					2141,
					13,
					51071
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 4,
				"seek": 0,
				"start": 15.4,
				"end": 18.3,
				"text": " And the model can then, if it needs to call the restore action.",
				"tokens": [
					51135,
					400,
					264,
					2316,
					393,
					550,
					11,
					498,
					309,
					2203,
					281,
					818,
					264,
					15227,
					3069,
					13,
					51280
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 5,
				"seek": 0,
				"start": 18.4,
				"end": 18.8,
				"text": " And yes,",
				"tokens": [
					51285,
					400,
					2086,
					11,
					51305
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 6,
				"seek": 0,
				"start": 18.84,
				"end": 21.34,
				"text": " that increases the number of tools that you have to make to call your",
				"tokens": [
					51307,
					300,
					8637,
					264,
					1230,
					295,
					3873,
					300,
					291,
					362,
					281,
					652,
					281,
					818,
					428,
					51432
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 7,
				"seek": 0,
				"start": 21.34,
				"end": 21.68,
				"text": " system,",
				"tokens": [
					51432,
					1185,
					11,
					51449
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 8,
				"seek": 0,
				"start": 21.88,
				"end": 26.02,
				"text": " but it dramatically makes your context window way more efficient because",
				"tokens": [
					51459,
					457,
					309,
					17548,
					1669,
					428,
					4319,
					4910,
					636,
					544,
					7148,
					570,
					51666
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 9,
				"seek": 0,
				"start": 26.02,
				"end": 28.2,
				"text": " most of the time, for example, for a coding agent,",
				"tokens": [
					51666,
					881,
					295,
					264,
					565,
					11,
					337,
					1365,
					11,
					337,
					257,
					17720,
					9461,
					11,
					51775
				],
				"temperature": 0,
				"avg_logprob": -0.1629215,
				"compression_ratio": 1.8049645,
				"no_speech_prob": 2.9977316e-12
			},
			{
				"id": 10,
				"seek": 2820,
				"start": 28.72,
				"end": 31.06,
				"text": " Imagine trying to keep every single file of the models",
				"tokens": [
					50391,
					11739,
					1382,
					281,
					1066,
					633,
					2167,
					3991,
					295,
					264,
					5245,
					50508
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 11,
				"seek": 2820,
				"start": 31.06,
				"end": 32.22,
				"text": " that have to edit in the context",
				"tokens": [
					50508,
					300,
					362,
					281,
					8129,
					294,
					264,
					4319,
					50566
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 12,
				"seek": 2820,
				"start": 32.22,
				"end": 34.54,
				"text": " and all the edits are made in the context.",
				"tokens": [
					50566,
					293,
					439,
					264,
					41752,
					366,
					1027,
					294,
					264,
					4319,
					13,
					50682
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 13,
				"seek": 2820,
				"start": 34.66,
				"end": 35.46,
				"text": " It's kind of useless.",
				"tokens": [
					50688,
					467,
					311,
					733,
					295,
					14115,
					13,
					50728
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 14,
				"seek": 2820,
				"start": 36,
				"end": 37.46,
				"text": " Like once it's made a file edit,",
				"tokens": [
					50755,
					1743,
					1564,
					309,
					311,
					1027,
					257,
					3991,
					8129,
					11,
					50828
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 15,
				"seek": 2820,
				"start": 37.58,
				"end": 38.64,
				"text": " I can kind of compress and say,",
				"tokens": [
					50834,
					286,
					393,
					733,
					295,
					14778,
					293,
					584,
					11,
					50887
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 16,
				"seek": 2820,
				"start": 38.8,
				"end": 40,
				"text": " I'm done with this file.",
				"tokens": [
					50895,
					286,
					478,
					1096,
					365,
					341,
					3991,
					13,
					50955
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 17,
				"seek": 2820,
				"start": 40.08,
				"end": 41.3,
				"text": " The user is happy and it works.",
				"tokens": [
					50959,
					440,
					4195,
					307,
					2055,
					293,
					309,
					1985,
					13,
					51020
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 18,
				"seek": 2820,
				"start": 41.38,
				"end": 42.56,
				"text": " Here's a summary of what the file,",
				"tokens": [
					51024,
					1692,
					311,
					257,
					12691,
					295,
					437,
					264,
					3991,
					11,
					51083
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 19,
				"seek": 2820,
				"start": 42.82,
				"end": 43.68,
				"text": " what changes I made.",
				"tokens": [
					51096,
					437,
					2962,
					286,
					1027,
					13,
					51139
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 20,
				"seek": 2820,
				"start": 45.3,
				"end": 47.46,
				"text": " But keeping the actual code that it generated",
				"tokens": [
					51220,
					583,
					5145,
					264,
					3539,
					3089,
					300,
					309,
					10833,
					51328
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 21,
				"seek": 2820,
				"start": 47.46,
				"end": 49.24,
				"text": " is kind of useless for that kind of task.",
				"tokens": [
					51328,
					307,
					733,
					295,
					14115,
					337,
					300,
					733,
					295,
					5633,
					13,
					51417
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 22,
				"seek": 2820,
				"start": 49.88,
				"end": 51.12,
				"text": " And then when I tell the model,",
				"tokens": [
					51449,
					400,
					550,
					562,
					286,
					980,
					264,
					2316,
					11,
					51511
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 23,
				"seek": 2820,
				"start": 51.22,
				"end": 52.52,
				"text": " hey, actually that file is wrong,",
				"tokens": [
					51516,
					4177,
					11,
					767,
					300,
					3991,
					307,
					2085,
					11,
					51581
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 24,
				"seek": 2820,
				"start": 52.62,
				"end": 53.84,
				"text": " then it can load the load actions",
				"tokens": [
					51586,
					550,
					309,
					393,
					3677,
					264,
					3677,
					5909,
					51647
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 25,
				"seek": 2820,
				"start": 53.84,
				"end": 54.82,
				"text": " and load the file again,",
				"tokens": [
					51647,
					293,
					3677,
					264,
					3991,
					797,
					11,
					51696
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 26,
				"seek": 2820,
				"start": 55.14,
				"end": 56.26,
				"text": " reread it and go do the work.",
				"tokens": [
					51712,
					46453,
					345,
					309,
					293,
					352,
					360,
					264,
					589,
					13,
					51768
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 27,
				"seek": 2820,
				"start": 56.86,
				"end": 57.94,
				"text": " And there's a trade-off here",
				"tokens": [
					51798,
					400,
					456,
					311,
					257,
					4923,
					12,
					4506,
					510,
					51852
				],
				"temperature": 0,
				"avg_logprob": -0.16626675,
				"compression_ratio": 1.8580247,
				"no_speech_prob": 1.6816626e-12
			},
			{
				"id": 28,
				"seek": 5794,
				"start": 57.94,
				"end": 59.68,
				"text": " between how many tool calls it takes",
				"tokens": [
					50365,
					1296,
					577,
					867,
					2290,
					5498,
					309,
					2516,
					50452
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 29,
				"seek": 5794,
				"start": 59.68,
				"end": 61.32,
				"text": " versus how accurate the system is",
				"tokens": [
					50452,
					5717,
					577,
					8559,
					264,
					1185,
					307,
					50534
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 30,
				"seek": 5794,
				"start": 61.32,
				"end": 67.9,
				"text": " versus how big of a context window I have.",
				"tokens": [
					50534,
					5717,
					577,
					955,
					295,
					257,
					4319,
					4910,
					286,
					362,
					13,
					50863
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 31,
				"seek": 5794,
				"start": 69.12,
				"end": 72.58,
				"text": " But if you don't make this a possibility,",
				"tokens": [
					50924,
					583,
					498,
					291,
					500,
					380,
					652,
					341,
					257,
					7959,
					11,
					51097
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 32,
				"seek": 5794,
				"start": 72.78,
				"end": 73.96,
				"text": " then you kind of live in this world",
				"tokens": [
					51107,
					550,
					291,
					733,
					295,
					1621,
					294,
					341,
					1002,
					51166
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 33,
				"seek": 5794,
				"start": 73.96,
				"end": 75.62,
				"text": " where you have to live in big context land.",
				"tokens": [
					51166,
					689,
					291,
					362,
					281,
					1621,
					294,
					955,
					4319,
					2117,
					13,
					51249
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 34,
				"seek": 5794,
				"start": 75.9,
				"end": 77.62,
				"text": " And then you're just hoping that GPT-26",
				"tokens": [
					51263,
					400,
					550,
					291,
					434,
					445,
					7159,
					300,
					26039,
					51,
					12,
					10880,
					51349
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 35,
				"seek": 5794,
				"start": 77.62,
				"end": 79.46,
				"text": " has a 20 million token window.",
				"tokens": [
					51349,
					575,
					257,
					945,
					2459,
					14862,
					4910,
					13,
					51441
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 36,
				"seek": 5794,
				"start": 80.22,
				"end": 81.04,
				"text": " And maybe it will,",
				"tokens": [
					51479,
					400,
					1310,
					309,
					486,
					11,
					51520
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 37,
				"seek": 5794,
				"start": 81.14,
				"end": 84.88,
				"text": " but I really don't want to work in a world",
				"tokens": [
					51525,
					457,
					286,
					534,
					500,
					380,
					528,
					281,
					589,
					294,
					257,
					1002,
					51712
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 38,
				"seek": 5794,
				"start": 84.88,
				"end": 87.7,
				"text": " where every computer needs to have 50 terabytes of RAM",
				"tokens": [
					51712,
					689,
					633,
					3820,
					2203,
					281,
					362,
					2625,
					1796,
					24538,
					295,
					14561,
					51853
				],
				"temperature": 0,
				"avg_logprob": -0.1269733,
				"compression_ratio": 1.6145039,
				"no_speech_prob": 7.639787e-13
			},
			{
				"id": 39,
				"seek": 8770,
				"start": 87.7,
				"end": 88.66,
				"text": " or else it's useless.",
				"tokens": [
					50365,
					420,
					1646,
					309,
					311,
					14115,
					13,
					50413
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 40,
				"seek": 8770,
				"start": 89.94,
				"end": 91.82,
				"text": " That's just a sad world to have to live in.",
				"tokens": [
					50477,
					663,
					311,
					445,
					257,
					4227,
					1002,
					281,
					362,
					281,
					1621,
					294,
					13,
					50571
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 41,
				"seek": 8770,
				"start": 92.88,
				"end": 93.9,
				"text": " And also like phones.",
				"tokens": [
					50624,
					400,
					611,
					411,
					10216,
					13,
					50675
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 42,
				"seek": 8770,
				"start": 96.02,
				"end": 97.7,
				"text": " I would like stuff to work on my phone",
				"tokens": [
					50781,
					286,
					576,
					411,
					1507,
					281,
					589,
					322,
					452,
					2593,
					50865
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 43,
				"seek": 8770,
				"start": 97.7,
				"end": 99.88,
				"text": " without needing to go into this.",
				"tokens": [
					50865,
					1553,
					18006,
					281,
					352,
					666,
					341,
					13,
					50974
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 44,
				"seek": 8770,
				"start": 100.02,
				"end": 101.02,
				"text": " A duck asked a question.",
				"tokens": [
					50981,
					316,
					12482,
					2351,
					257,
					1168,
					13,
					51031
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 45,
				"seek": 8770,
				"start": 101.16,
				"end": 102.14,
				"text": " How does it work with KV caching?",
				"tokens": [
					51038,
					1012,
					775,
					309,
					589,
					365,
					591,
					53,
					269,
					2834,
					30,
					51087
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 46,
				"seek": 8770,
				"start": 102.28,
				"end": 104.48,
				"text": " Well, it really depends.",
				"tokens": [
					51094,
					1042,
					11,
					309,
					534,
					5946,
					13,
					51204
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 47,
				"seek": 8770,
				"start": 104.9,
				"end": 107.2,
				"text": " If your system, I think an important thing",
				"tokens": [
					51225,
					759,
					428,
					1185,
					11,
					286,
					519,
					364,
					1021,
					551,
					51340
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 48,
				"seek": 8770,
				"start": 107.2,
				"end": 108.18,
				"text": " to notice about KV caches",
				"tokens": [
					51340,
					281,
					3449,
					466,
					591,
					53,
					269,
					13272,
					51389
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 49,
				"seek": 8770,
				"start": 108.18,
				"end": 109.84,
				"text": " and like people over-index on this sometimes.",
				"tokens": [
					51389,
					293,
					411,
					561,
					670,
					12,
					471,
					3121,
					322,
					341,
					2171,
					13,
					51472
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 50,
				"seek": 8770,
				"start": 109.98,
				"end": 111.8,
				"text": " And I know I was trying to explain it",
				"tokens": [
					51479,
					400,
					286,
					458,
					286,
					390,
					1382,
					281,
					2903,
					309,
					51570
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 51,
				"seek": 8770,
				"start": 111.8,
				"end": 113.32,
				"text": " so perhaps that was implying this,",
				"tokens": [
					51570,
					370,
					4317,
					300,
					390,
					704,
					7310,
					341,
					11,
					51646
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 52,
				"seek": 8770,
				"start": 113.48,
				"end": 115.8,
				"text": " but look, KV caches don't,",
				"tokens": [
					51654,
					457,
					574,
					11,
					591,
					53,
					269,
					13272,
					500,
					380,
					11,
					51770
				],
				"temperature": 0,
				"avg_logprob": -0.20496632,
				"compression_ratio": 1.6183746,
				"no_speech_prob": 1.2065967e-12
			},
			{
				"id": 53,
				"seek": 11580,
				"start": 115.8,
				"end": 118.68,
				"text": " Caching and latency doesn't matter for small stuff.",
				"tokens": [
					50365,
					383,
					2834,
					293,
					27043,
					1177,
					380,
					1871,
					337,
					1359,
					1507,
					13,
					50509
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 54,
				"seek": 11580,
				"start": 118.96,
				"end": 122.24,
				"text": " If most of your chat messages in your app or agentic loops",
				"tokens": [
					50523,
					759,
					881,
					295,
					428,
					5081,
					7897,
					294,
					428,
					724,
					420,
					9461,
					299,
					16121,
					50687
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 55,
				"seek": 11580,
				"start": 122.24,
				"end": 124.36,
				"text": " are like one or two system calls long,",
				"tokens": [
					50687,
					366,
					411,
					472,
					420,
					732,
					1185,
					5498,
					938,
					11,
					50793
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 56,
				"seek": 11580,
				"start": 124.76,
				"end": 125.96,
				"text": " don't do any of this crap.",
				"tokens": [
					50813,
					500,
					380,
					360,
					604,
					295,
					341,
					12426,
					13,
					50873
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 57,
				"seek": 11580,
				"start": 126.38,
				"end": 127.34,
				"text": " It is not worth it.",
				"tokens": [
					50894,
					467,
					307,
					406,
					3163,
					309,
					13,
					50942
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 58,
				"seek": 11580,
				"start": 127.4,
				"end": 128.54,
				"text": " Just make your system work.",
				"tokens": [
					50945,
					1449,
					652,
					428,
					1185,
					589,
					13,
					51002
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 59,
				"seek": 11580,
				"start": 129.38,
				"end": 131.14,
				"text": " This stuff only matters for stuff",
				"tokens": [
					51044,
					639,
					1507,
					787,
					7001,
					337,
					1507,
					51132
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 60,
				"seek": 11580,
				"start": 131.14,
				"end": 132.54,
				"text": " that is going to run for a while.",
				"tokens": [
					51132,
					300,
					307,
					516,
					281,
					1190,
					337,
					257,
					1339,
					13,
					51202
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 61,
				"seek": 11580,
				"start": 133.14,
				"end": 135.32,
				"text": " If your tasks are taking like a minute to run,",
				"tokens": [
					51232,
					759,
					428,
					9608,
					366,
					1940,
					411,
					257,
					3456,
					281,
					1190,
					11,
					51341
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 62,
				"seek": 11580,
				"start": 136.12,
				"end": 138.1,
				"text": " yes, then KV cache matters.",
				"tokens": [
					51381,
					2086,
					11,
					550,
					591,
					53,
					19459,
					7001,
					13,
					51480
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 63,
				"seek": 11580,
				"start": 138.88,
				"end": 140.92,
				"text": " But in that case, if your task is taking a minute to run",
				"tokens": [
					51519,
					583,
					294,
					300,
					1389,
					11,
					498,
					428,
					5633,
					307,
					1940,
					257,
					3456,
					281,
					1190,
					51621
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 64,
				"seek": 11580,
				"start": 140.92,
				"end": 142.68,
				"text": " because it's calling like 50 different tool calls,",
				"tokens": [
					51621,
					570,
					309,
					311,
					5141,
					411,
					2625,
					819,
					2290,
					5498,
					11,
					51709
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 65,
				"seek": 11580,
				"start": 142.9,
				"end": 145.72,
				"text": " you'll probably get way better KV cache optimization",
				"tokens": [
					51720,
					291,
					603,
					1391,
					483,
					636,
					1101,
					591,
					53,
					19459,
					19618,
					51861
				],
				"temperature": 0,
				"avg_logprob": -0.15215306,
				"compression_ratio": 1.7541528,
				"no_speech_prob": 1.4497355e-12
			},
			{
				"id": 66,
				"seek": 14572,
				"start": 145.72,
				"end": 149.02,
				"text": " simply by changing the observations in this way as well,",
				"tokens": [
					50365,
					2935,
					538,
					4473,
					264,
					18163,
					294,
					341,
					636,
					382,
					731,
					11,
					50530
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 67,
				"seek": 14572,
				"start": 149.1,
				"end": 151.06,
				"text": " where like now I get a KV cache here",
				"tokens": [
					50534,
					689,
					411,
					586,
					286,
					483,
					257,
					591,
					53,
					19459,
					510,
					50632
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 68,
				"seek": 14572,
				"start": 151.06,
				"end": 152.98,
				"text": " and only if I load in a new observation",
				"tokens": [
					50632,
					293,
					787,
					498,
					286,
					3677,
					294,
					257,
					777,
					14816,
					50728
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 69,
				"seek": 14572,
				"start": 152.98,
				"end": 155.04,
				"text": " do I break the KV cache",
				"tokens": [
					50728,
					360,
					286,
					1821,
					264,
					591,
					53,
					19459,
					50831
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 70,
				"seek": 14572,
				"start": 155.04,
				"end": 157,
				"text": " because then I'm going to load observation one",
				"tokens": [
					50831,
					570,
					550,
					286,
					478,
					516,
					281,
					3677,
					14816,
					472,
					50929
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 71,
				"seek": 14572,
				"start": 157,
				"end": 158.54,
				"text": " and then compress it into this format.",
				"tokens": [
					50929,
					293,
					550,
					14778,
					309,
					666,
					341,
					7877,
					13,
					51006
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 72,
				"seek": 14572,
				"start": 159.02,
				"end": 160.74,
				"text": " So now this thing becomes compressed.",
				"tokens": [
					51030,
					407,
					586,
					341,
					551,
					3643,
					30353,
					13,
					51116
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 73,
				"seek": 14572,
				"start": 161.36,
				"end": 163.84,
				"text": " But what I can do here is I can design an algorithm here",
				"tokens": [
					51147,
					583,
					437,
					286,
					393,
					360,
					510,
					307,
					286,
					393,
					1715,
					364,
					9284,
					510,
					51271
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 74,
				"seek": 14572,
				"start": 163.84,
				"end": 164.72,
				"text": " that's pretty straightforward",
				"tokens": [
					51271,
					300,
					311,
					1238,
					15325,
					51315
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 75,
				"seek": 14572,
				"start": 164.72,
				"end": 168.56,
				"text": " that goes ahead and says,",
				"tokens": [
					51315,
					300,
					1709,
					2286,
					293,
					1619,
					11,
					51507
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 76,
				"seek": 14572,
				"start": 168.7,
				"end": 170.42,
				"text": " hey, after about 15 observations,",
				"tokens": [
					51514,
					4177,
					11,
					934,
					466,
					2119,
					18163,
					11,
					51600
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 77,
				"seek": 14572,
				"start": 170.42,
				"end": 172.14,
				"text": " I always compress the oldest one.",
				"tokens": [
					51600,
					286,
					1009,
					14778,
					264,
					14026,
					472,
					13,
					51686
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 78,
				"seek": 14572,
				"start": 173,
				"end": 174.74,
				"text": " So yes, I broke my KV cache,",
				"tokens": [
					51729,
					407,
					2086,
					11,
					286,
					6902,
					452,
					591,
					53,
					19459,
					11,
					51816
				],
				"temperature": 0,
				"avg_logprob": -0.16187935,
				"compression_ratio": 1.7725632,
				"no_speech_prob": 1.3996144e-12
			},
			{
				"id": 79,
				"seek": 17474,
				"start": 174.74,
				"end": 179.16,
				"text": " but at any given time, all my previous context is always going to be similar,",
				"tokens": [
					50365,
					457,
					412,
					604,
					2212,
					565,
					11,
					439,
					452,
					3894,
					4319,
					307,
					1009,
					516,
					281,
					312,
					2531,
					11,
					50586
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 80,
				"seek": 17474,
				"start": 179.54,
				"end": 181.8,
				"text": " and I'll basically force the model to reload it.",
				"tokens": [
					50605,
					293,
					286,
					603,
					1936,
					3464,
					264,
					2316,
					281,
					25628,
					309,
					13,
					50718
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 81,
				"seek": 17474,
				"start": 182,
				"end": 185.82,
				"text": " Because this, once it's compressed, is never reloaded ever again.",
				"tokens": [
					50728,
					1436,
					341,
					11,
					1564,
					309,
					311,
					30353,
					11,
					307,
					1128,
					25628,
					292,
					1562,
					797,
					13,
					50919
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 82,
				"seek": 17474,
				"start": 186.7,
				"end": 189.86,
				"text": " So this basically stabilizes over time.",
				"tokens": [
					50963,
					407,
					341,
					1936,
					11652,
					5660,
					670,
					565,
					13,
					51121
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 83,
				"seek": 17474,
				"start": 190.46,
				"end": 191.54,
				"text": " Does that answer the question, Doc?",
				"tokens": [
					51151,
					4402,
					300,
					1867,
					264,
					1168,
					11,
					16024,
					30,
					51205
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 84,
				"seek": 17474,
				"start": 198.68,
				"end": 199.24,
				"text": " Perfect.",
				"tokens": [
					51562,
					10246,
					13,
					51590
				],
				"temperature": 0,
				"avg_logprob": -0.31280336,
				"compression_ratio": 1.4427084,
				"no_speech_prob": 7.461867e-13
			},
			{
				"id": 85,
				"seek": 19924,
				"start": 199.24,
				"end": 207.06,
				"text": " and then the debate of showing tool calls and what's out but yeah yeah I think you got it",
				"tokens": [
					50365,
					293,
					550,
					264,
					7958,
					295,
					4099,
					2290,
					5498,
					293,
					437,
					311,
					484,
					457,
					1338,
					1338,
					286,
					519,
					291,
					658,
					309,
					50756
				],
				"temperature": 0,
				"avg_logprob": -0.060127337,
				"compression_ratio": 1.8571428,
				"no_speech_prob": 2.1675207e-12
			},
			{
				"id": 86,
				"seek": 19924,
				"start": 207.06,
				"end": 211.42,
				"text": " perfectly dug yeah exactly once it once it gets the next step and it knows what to do then it can",
				"tokens": [
					50756,
					6239,
					22954,
					1338,
					2293,
					1564,
					309,
					1564,
					309,
					2170,
					264,
					958,
					1823,
					293,
					309,
					3255,
					437,
					281,
					360,
					550,
					309,
					393,
					50974
				],
				"temperature": 0,
				"avg_logprob": -0.060127337,
				"compression_ratio": 1.8571428,
				"no_speech_prob": 2.1675207e-12
			},
			{
				"id": 87,
				"seek": 19924,
				"start": 211.42,
				"end": 216.62,
				"text": " do it and I think this is also like when people do there's like certain like compaction strategies",
				"tokens": [
					50974,
					360,
					309,
					293,
					286,
					519,
					341,
					307,
					611,
					411,
					562,
					561,
					360,
					456,
					311,
					411,
					1629,
					411,
					715,
					2894,
					9029,
					51234
				],
				"temperature": 0,
				"avg_logprob": -0.060127337,
				"compression_ratio": 1.8571428,
				"no_speech_prob": 2.1675207e-12
			},
			{
				"id": 88,
				"seek": 19924,
				"start": 216.62,
				"end": 220.56,
				"text": " that you can do like Anthropic talks about micro compaction which is like pulling at least like",
				"tokens": [
					51234,
					300,
					291,
					393,
					360,
					411,
					12727,
					39173,
					6686,
					466,
					4532,
					715,
					2894,
					597,
					307,
					411,
					8407,
					412,
					1935,
					411,
					51431
				],
				"temperature": 0,
				"avg_logprob": -0.060127337,
				"compression_ratio": 1.8571428,
				"no_speech_prob": 2.1675207e-12
			},
			{
				"id": 89,
				"seek": 19924,
				"start": 220.56,
				"end": 226,
				"text": " the tool calls out automatically but it's really hard to do this in a general purpose way which is",
				"tokens": [
					51431,
					264,
					2290,
					5498,
					484,
					6772,
					457,
					309,
					311,
					534,
					1152,
					281,
					360,
					341,
					294,
					257,
					2674,
					4334,
					636,
					597,
					307,
					51703
				],
				"temperature": 0,
				"avg_logprob": -0.060127337,
				"compression_ratio": 1.8571428,
				"no_speech_prob": 2.1675207e-12
			},
			{
				"id": 90,
				"seek": 22600,
				"start": 226,
				"end": 229.18,
				"text": " what makes this paper so impressive is like they've done this for a general purpose agent.",
				"tokens": [
					50365,
					437,
					1669,
					341,
					3035,
					370,
					8992,
					307,
					411,
					436,
					600,
					1096,
					341,
					337,
					257,
					2674,
					4334,
					9461,
					13,
					50524
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 91,
				"seek": 22600,
				"start": 229.26,
				"end": 232.84,
				"text": " If you're building an agent for a very specific thing, you can optimize the heck out of it for",
				"tokens": [
					50528,
					759,
					291,
					434,
					2390,
					364,
					9461,
					337,
					257,
					588,
					2685,
					551,
					11,
					291,
					393,
					19719,
					264,
					12872,
					484,
					295,
					309,
					337,
					50707
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 92,
				"seek": 22600,
				"start": 232.84,
				"end": 238.08,
				"text": " that one use case. And you know exactly like I know for sure if a model reads this type of medical",
				"tokens": [
					50707,
					300,
					472,
					764,
					1389,
					13,
					400,
					291,
					458,
					2293,
					411,
					286,
					458,
					337,
					988,
					498,
					257,
					2316,
					15700,
					341,
					2010,
					295,
					4625,
					50969
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 93,
				"seek": 22600,
				"start": 238.08,
				"end": 242.7,
				"text": " document, it's and once it makes the decision, it never needs to do it again. Or you can say,",
				"tokens": [
					50969,
					4166,
					11,
					309,
					311,
					293,
					1564,
					309,
					1669,
					264,
					3537,
					11,
					309,
					1128,
					2203,
					281,
					360,
					309,
					797,
					13,
					1610,
					291,
					393,
					584,
					11,
					51200
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 94,
				"seek": 22600,
				"start": 242.76,
				"end": 246.04,
				"text": " OK, once the model reads it and picks the next thing, I need to pass it to another model to",
				"tokens": [
					51203,
					2264,
					11,
					1564,
					264,
					2316,
					15700,
					309,
					293,
					16137,
					264,
					958,
					551,
					11,
					286,
					643,
					281,
					1320,
					309,
					281,
					1071,
					2316,
					281,
					51367
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 95,
				"seek": 22600,
				"start": 246.04,
				"end": 250.24,
				"text": " summarize the document, put that in instead of the whole content and like do your own like",
				"tokens": [
					51367,
					20858,
					264,
					4166,
					11,
					829,
					300,
					294,
					2602,
					295,
					264,
					1379,
					2701,
					293,
					411,
					360,
					428,
					1065,
					411,
					51577
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 96,
				"seek": 22600,
				"start": 250.24,
				"end": 254.34,
				"text": " deterministic compaction based on what types of things are being pulled into the context.",
				"tokens": [
					51577,
					15957,
					3142,
					715,
					2894,
					2361,
					322,
					437,
					3467,
					295,
					721,
					366,
					885,
					7373,
					666,
					264,
					4319,
					13,
					51782
				],
				"temperature": 0,
				"avg_logprob": -0.10509038,
				"compression_ratio": 1.8033241,
				"no_speech_prob": 3.0447047e-12
			},
			{
				"id": 97,
				"seek": 25434,
				"start": 254.8,
				"end": 255.16,
				"text": " Exactly.",
				"tokens": [
					50388,
					7587,
					13,
					50406
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 98,
				"seek": 25434,
				"start": 255.72,
				"end": 259.64,
				"text": " Now, I want to talk about the last part that I thought was super, super innovative and",
				"tokens": [
					50434,
					823,
					11,
					286,
					528,
					281,
					751,
					466,
					264,
					1036,
					644,
					300,
					286,
					1194,
					390,
					1687,
					11,
					1687,
					12999,
					293,
					50630
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 99,
				"seek": 25434,
				"start": 259.64,
				"end": 262.82,
				"text": " really changed my perspective a little bit on some things as well, which is around how",
				"tokens": [
					50630,
					534,
					3105,
					452,
					4585,
					257,
					707,
					857,
					322,
					512,
					721,
					382,
					731,
					11,
					597,
					307,
					926,
					577,
					50789
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 100,
				"seek": 25434,
				"start": 262.82,
				"end": 263.42,
				"text": " tool calls work.",
				"tokens": [
					50789,
					2290,
					5498,
					589,
					13,
					50819
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 101,
				"seek": 25434,
				"start": 263.86,
				"end": 265.16,
				"text": " So we talked about the KB cache.",
				"tokens": [
					50841,
					407,
					321,
					2825,
					466,
					264,
					591,
					33,
					19459,
					13,
					50906
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 102,
				"seek": 25434,
				"start": 265.22,
				"end": 268.46,
				"text": " We talked about how having continuity and not breaking continuity matters.",
				"tokens": [
					50909,
					492,
					2825,
					466,
					577,
					1419,
					23807,
					293,
					406,
					7697,
					23807,
					7001,
					13,
					51071
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 103,
				"seek": 25434,
				"start": 269.12,
				"end": 273.78,
				"text": " So we talked, I think one easy way to go fix this is you can take your tool calls and simply",
				"tokens": [
					51104,
					407,
					321,
					2825,
					11,
					286,
					519,
					472,
					1858,
					636,
					281,
					352,
					3191,
					341,
					307,
					291,
					393,
					747,
					428,
					2290,
					5498,
					293,
					2935,
					51337
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 104,
				"seek": 25434,
				"start": 273.78,
				"end": 276.68,
				"text": " put them always guaranteed, put them at the end of your context window.",
				"tokens": [
					51337,
					829,
					552,
					1009,
					18031,
					11,
					829,
					552,
					412,
					264,
					917,
					295,
					428,
					4319,
					4910,
					13,
					51482
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 105,
				"seek": 25434,
				"start": 276.68,
				"end": 279.98,
				"text": " And now you always have some amount of caching that you get for free.",
				"tokens": [
					51482,
					400,
					586,
					291,
					1009,
					362,
					512,
					2372,
					295,
					269,
					2834,
					300,
					291,
					483,
					337,
					1737,
					13,
					51647
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 106,
				"seek": 25434,
				"start": 280.58,
				"end": 282.22,
				"text": " But what if you did put them at the top?",
				"tokens": [
					51677,
					583,
					437,
					498,
					291,
					630,
					829,
					552,
					412,
					264,
					1192,
					30,
					51759
				],
				"temperature": 0,
				"avg_logprob": -0.13355477,
				"compression_ratio": 1.810559,
				"no_speech_prob": 1.898134e-12
			},
			{
				"id": 107,
				"seek": 28222,
				"start": 282.64,
				"end": 284.04,
				"text": " Well, many times you're building an agent,",
				"tokens": [
					50386,
					1042,
					11,
					867,
					1413,
					291,
					434,
					2390,
					364,
					9461,
					11,
					50456
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 108,
				"seek": 28222,
				"start": 284.12,
				"end": 285.02,
				"text": " you'll want to invalidate",
				"tokens": [
					50460,
					291,
					603,
					528,
					281,
					34702,
					473,
					50505
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 109,
				"seek": 28222,
				"start": 285.02,
				"end": 286.36,
				"text": " and change your tool calls dynamically.",
				"tokens": [
					50505,
					293,
					1319,
					428,
					2290,
					5498,
					43492,
					13,
					50572
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 110,
				"seek": 28222,
				"start": 287.8,
				"end": 290.1,
				"text": " That is 100% of the time",
				"tokens": [
					50644,
					663,
					307,
					2319,
					4,
					295,
					264,
					565,
					50759
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 111,
				"seek": 28222,
				"start": 290.1,
				"end": 291.16,
				"text": " going to break your KV cache",
				"tokens": [
					50759,
					516,
					281,
					1821,
					428,
					591,
					53,
					19459,
					50812
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 112,
				"seek": 28222,
				"start": 291.16,
				"end": 292.38,
				"text": " and you will just get slower latency",
				"tokens": [
					50812,
					293,
					291,
					486,
					445,
					483,
					14009,
					27043,
					50873
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 113,
				"seek": 28222,
				"start": 292.38,
				"end": 293.34,
				"text": " all the way through and through.",
				"tokens": [
					50873,
					439,
					264,
					636,
					807,
					293,
					807,
					13,
					50921
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 114,
				"seek": 28222,
				"start": 293.34,
				"end": 295.18,
				"text": " This was what the Swarm,",
				"tokens": [
					50921,
					639,
					390,
					437,
					264,
					3926,
					4452,
					11,
					51013
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 115,
				"seek": 28222,
				"start": 295.28,
				"end": 296.62,
				"text": " like the original iteration",
				"tokens": [
					51018,
					411,
					264,
					3380,
					24784,
					51085
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 116,
				"seek": 28222,
				"start": 296.62,
				"end": 299.22,
				"text": " of like the OpenAI Swarms framework or whatever,",
				"tokens": [
					51085,
					295,
					411,
					264,
					7238,
					48698,
					3926,
					19537,
					8388,
					420,
					2035,
					11,
					51215
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 117,
				"seek": 28222,
				"start": 299.38,
				"end": 300.66,
				"text": " their way of doing multi-agent",
				"tokens": [
					51223,
					641,
					636,
					295,
					884,
					4825,
					12,
					559,
					317,
					51287
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 118,
				"seek": 28222,
				"start": 300.66,
				"end": 302,
				"text": " was to take the same context window,",
				"tokens": [
					51287,
					390,
					281,
					747,
					264,
					912,
					4319,
					4910,
					11,
					51354
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 119,
				"seek": 28222,
				"start": 302.4,
				"end": 303.66,
				"text": " pass it to a different agent,",
				"tokens": [
					51374,
					1320,
					309,
					281,
					257,
					819,
					9461,
					11,
					51437
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 120,
				"seek": 28222,
				"start": 303.66,
				"end": 304.94,
				"text": " which had a different system message",
				"tokens": [
					51437,
					597,
					632,
					257,
					819,
					1185,
					3636,
					51501
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 121,
				"seek": 28222,
				"start": 304.94,
				"end": 306,
				"text": " and a different set of tools.",
				"tokens": [
					51501,
					293,
					257,
					819,
					992,
					295,
					3873,
					13,
					51554
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 122,
				"seek": 28222,
				"start": 306.38,
				"end": 306.7,
				"text": " Exactly.",
				"tokens": [
					51573,
					7587,
					13,
					51589
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 123,
				"seek": 28222,
				"start": 306.92,
				"end": 308.6,
				"text": " And like, it just doesn't really work.",
				"tokens": [
					51600,
					400,
					411,
					11,
					309,
					445,
					1177,
					380,
					534,
					589,
					13,
					51684
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 124,
				"seek": 28222,
				"start": 308.92,
				"end": 310.22,
				"text": " But what they end up doing here",
				"tokens": [
					51700,
					583,
					437,
					436,
					917,
					493,
					884,
					510,
					51765
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 125,
				"seek": 28222,
				"start": 310.22,
				"end": 311.9,
				"text": " that I thought was really fascinating",
				"tokens": [
					51765,
					300,
					286,
					1194,
					390,
					534,
					10343,
					51849
				],
				"temperature": 0,
				"avg_logprob": -0.15825652,
				"compression_ratio": 1.7331461,
				"no_speech_prob": 1.4051066e-12
			},
			{
				"id": 126,
				"seek": 31190,
				"start": 311.9,
				"end": 315.34,
				"text": " Instead of actually giving you these tools,",
				"tokens": [
					50365,
					7156,
					295,
					767,
					2902,
					291,
					613,
					3873,
					11,
					50537
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 127,
				"seek": 31190,
				"start": 315.84,
				"end": 317.4,
				"text": " they actually leave the tools in there.",
				"tokens": [
					50562,
					436,
					767,
					1856,
					264,
					3873,
					294,
					456,
					13,
					50640
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 128,
				"seek": 31190,
				"start": 317.96,
				"end": 320.9,
				"text": " And what they do is they modify...",
				"tokens": [
					50668,
					400,
					437,
					436,
					360,
					307,
					436,
					16927,
					485,
					50815
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 129,
				"seek": 31190,
				"start": 321.78,
				"end": 323.36,
				"text": " Let me change. Let me delete.",
				"tokens": [
					50859,
					961,
					385,
					1319,
					13,
					961,
					385,
					12097,
					13,
					50938
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 130,
				"seek": 31190,
				"start": 324,
				"end": 326.04,
				"text": " They modify a part of the system",
				"tokens": [
					50970,
					814,
					16927,
					257,
					644,
					295,
					264,
					1185,
					51072
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 131,
				"seek": 31190,
				"start": 326.04,
				"end": 327.44,
				"text": " that I think most people don't think about,",
				"tokens": [
					51072,
					300,
					286,
					519,
					881,
					561,
					500,
					380,
					519,
					466,
					11,
					51142
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 132,
				"seek": 31190,
				"start": 328.48,
				"end": 329.9,
				"text": " which is...",
				"tokens": [
					51194,
					597,
					307,
					485,
					51265
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 133,
				"seek": 31190,
				"start": 329.9,
				"end": 331.62,
				"text": " Let me open up the OpenAI docs",
				"tokens": [
					51265,
					961,
					385,
					1269,
					493,
					264,
					7238,
					48698,
					45623,
					51351
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 134,
				"seek": 31190,
				"start": 331.62,
				"end": 333.8,
				"text": " because I think that's going to show",
				"tokens": [
					51351,
					570,
					286,
					519,
					300,
					311,
					516,
					281,
					855,
					51460
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 135,
				"seek": 31190,
				"start": 333.8,
				"end": 339.58,
				"text": " OpenAI responses API.",
				"tokens": [
					51460,
					7238,
					48698,
					13019,
					9362,
					13,
					51749
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 136,
				"seek": 31190,
				"start": 340.4,
				"end": 341.18,
				"text": " Okay, cool.",
				"tokens": [
					51790,
					1033,
					11,
					1627,
					13,
					51829
				],
				"temperature": 0,
				"avg_logprob": -0.30586156,
				"compression_ratio": 1.6142857,
				"no_speech_prob": 8.6545544e-13
			},
			{
				"id": 137,
				"seek": 34190,
				"start": 341.9,
				"end": 344.06,
				"text": " they modify the logits coming out of it.",
				"tokens": [
					50365,
					436,
					16927,
					264,
					3565,
					1208,
					1348,
					484,
					295,
					309,
					13,
					50473
				],
				"temperature": 0,
				"avg_logprob": -0.1661331,
				"compression_ratio": 1.4709678,
				"no_speech_prob": 5.674558e-13
			},
			{
				"id": 138,
				"seek": 346,
				"start": 346.64,
				"end": 356.90112,
				"text": " There you go And you can basically invalidate certain tokens as being valid out of the model So I don actually know how you modify the function calling tokens I can go look into that because it sounds like",
				"tokens": [
					50602,
					821,
					291,
					352,
					13,
					50645,
					50645,
					400,
					291,
					393,
					1936,
					34702,
					473,
					1629,
					22667,
					50796,
					50796,
					382,
					885,
					7363,
					484,
					295,
					264,
					2316,
					13,
					50904,
					50678,
					407,
					286,
					500,
					380,
					767,
					458,
					577,
					291,
					16927,
					50743,
					50743,
					264,
					2445,
					5141,
					22667,
					13,
					50803,
					50809,
					286,
					393,
					352,
					574,
					666,
					300,
					50856,
					50856,
					570,
					309,
					3263,
					411,
					11,
					50893
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 139,
				"seek": 346,
				"start": 357.08112,
				"end": 358.4211,
				"text": " I don't know if OpenAI gives you those,",
				"tokens": [
					50902,
					286,
					500,
					380,
					458,
					498,
					7238,
					48698,
					2709,
					291,
					729,
					11,
					50969
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 140,
				"seek": 346,
				"start": 358.48114,
				"end": 359.26114,
				"text": " but if you own the model,",
				"tokens": [
					50972,
					457,
					498,
					291,
					1065,
					264,
					2316,
					11,
					51011
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 141,
				"seek": 346,
				"start": 359.32114,
				"end": 360.16113,
				"text": " you can definitely do this.",
				"tokens": [
					51014,
					291,
					393,
					2138,
					360,
					341,
					13,
					51056
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 142,
				"seek": 346,
				"start": 360.66113,
				"end": 362.30112,
				"text": " But the premise is,",
				"tokens": [
					51081,
					583,
					264,
					22045,
					307,
					11,
					51163
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 143,
				"seek": 346,
				"start": 363.50113,
				"end": 366.32114,
				"text": " there we go.",
				"tokens": [
					51223,
					456,
					321,
					352,
					13,
					51364
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 144,
				"seek": 346,
				"start": 366.62112,
				"end": 367.34113,
				"text": " The premise is like,",
				"tokens": [
					51379,
					440,
					22045,
					307,
					411,
					11,
					51415
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 145,
				"seek": 346,
				"start": 367.40112,
				"end": 368.90112,
				"text": " what the heck is function calling doing?",
				"tokens": [
					51418,
					437,
					264,
					12872,
					307,
					2445,
					5141,
					884,
					30,
					51493
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 146,
				"seek": 346,
				"start": 369.00113,
				"end": 370.90112,
				"text": " And we need to go understand that briefly",
				"tokens": [
					51498,
					400,
					321,
					643,
					281,
					352,
					1223,
					300,
					10515,
					51593
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 147,
				"seek": 346,
				"start": 370.90112,
				"end": 372.4211,
				"text": " to really be able to understand",
				"tokens": [
					51593,
					281,
					534,
					312,
					1075,
					281,
					1223,
					51669
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 148,
				"seek": 346,
				"start": 372.4211,
				"end": 373.8611,
				"text": " and appreciate this technique.",
				"tokens": [
					51669,
					293,
					4449,
					341,
					6532,
					13,
					51741
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 149,
				"seek": 346,
				"start": 374.72113,
				"end": 376.1411,
				"text": " So what function calling does",
				"tokens": [
					51784,
					407,
					437,
					2445,
					5141,
					775,
					51855
				],
				"temperature": 0,
				"avg_logprob": -0.21400167,
				"compression_ratio": 1.8135593,
				"no_speech_prob": 9.287571e-13
			},
			{
				"id": 150,
				"seek": 3326,
				"start": 376.1411,
				"end": 379.12112,
				"text": " is function calling teaches a model",
				"tokens": [
					50365,
					307,
					2445,
					5141,
					16876,
					257,
					2316,
					50514
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 151,
				"seek": 3326,
				"start": 379.12112,
				"end": 381.16113,
				"text": " about a special token called use tool.",
				"tokens": [
					50514,
					466,
					257,
					2121,
					14862,
					1219,
					764,
					2290,
					13,
					50616
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 152,
				"seek": 3326,
				"start": 381.74112,
				"end": 384.48114,
				"text": " And the model outputs a token that says,",
				"tokens": [
					50645,
					400,
					264,
					2316,
					23930,
					257,
					14862,
					300,
					1619,
					11,
					50782
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 153,
				"seek": 3326,
				"start": 384.60114,
				"end": 385.54114,
				"text": " I'm going to output a tool.",
				"tokens": [
					50788,
					286,
					478,
					516,
					281,
					5598,
					257,
					2290,
					13,
					50835
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 154,
				"seek": 3326,
				"start": 385.96112,
				"end": 388.54114,
				"text": " And once it decides to do that token,",
				"tokens": [
					50856,
					400,
					1564,
					309,
					14898,
					281,
					360,
					300,
					14862,
					11,
					50985
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 155,
				"seek": 3326,
				"start": 388.62112,
				"end": 391.40112,
				"text": " the model providers then restrict the model",
				"tokens": [
					50989,
					264,
					2316,
					11330,
					550,
					7694,
					264,
					2316,
					51128
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 156,
				"seek": 3326,
				"start": 391.40112,
				"end": 395.52112,
				"text": " to only pick tokens that match the tool specification",
				"tokens": [
					51128,
					281,
					787,
					1888,
					22667,
					300,
					2995,
					264,
					2290,
					31256,
					51334
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 157,
				"seek": 3326,
				"start": 395.52112,
				"end": 396.66113,
				"text": " that you have provided.",
				"tokens": [
					51334,
					300,
					291,
					362,
					5649,
					13,
					51391
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 158,
				"seek": 3326,
				"start": 397.72113,
				"end": 400.00113,
				"text": " Is this constrained decoding",
				"tokens": [
					51444,
					1119,
					341,
					38901,
					979,
					8616,
					51558
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 159,
				"seek": 3326,
				"start": 400.00113,
				"end": 402.48114,
				"text": " or is this still just the base like JSON mode?",
				"tokens": [
					51558,
					420,
					307,
					341,
					920,
					445,
					264,
					3096,
					411,
					31828,
					4391,
					30,
					51682
				],
				"temperature": 0,
				"avg_logprob": -0.20347996,
				"compression_ratio": 1.7149321,
				"no_speech_prob": 7.0104827e-13
			},
			{
				"id": 160,
				"seek": 5960,
				"start": 402.48114,
				"end": 409.96112,
				"text": " this is it so the base json mode does that which is it basically takes it basically forces the",
				"tokens": [
					50365,
					341,
					307,
					309,
					370,
					264,
					3096,
					361,
					3015,
					4391,
					775,
					300,
					597,
					307,
					309,
					1936,
					2516,
					309,
					1936,
					5874,
					264,
					50739
				],
				"temperature": 0,
				"avg_logprob": -0.054153435,
				"compression_ratio": 1.9419087,
				"no_speech_prob": 1.3775846e-12
			},
			{
				"id": 161,
				"seek": 5960,
				"start": 409.96112,
				"end": 415.58112,
				"text": " model to only output grammars that are valid uh json yeah yeah yeah constrained decoding",
				"tokens": [
					50739,
					2316,
					281,
					787,
					5598,
					17570,
					685,
					300,
					366,
					7363,
					2232,
					361,
					3015,
					1338,
					1338,
					1338,
					38901,
					979,
					8616,
					51020
				],
				"temperature": 0,
				"avg_logprob": -0.054153435,
				"compression_ratio": 1.9419087,
				"no_speech_prob": 1.3775846e-12
			},
			{
				"id": 162,
				"seek": 5960,
				"start": 415.58112,
				"end": 419.34113,
				"text": " constrained generation or decoding whatever you want to call it is basically the more general",
				"tokens": [
					51020,
					38901,
					5125,
					420,
					979,
					8616,
					2035,
					291,
					528,
					281,
					818,
					309,
					307,
					1936,
					264,
					544,
					2674,
					51208
				],
				"temperature": 0,
				"avg_logprob": -0.054153435,
				"compression_ratio": 1.9419087,
				"no_speech_prob": 1.3775846e-12
			},
			{
				"id": 163,
				"seek": 5960,
				"start": 419.34113,
				"end": 423.54114,
				"text": " form of that where instead of taking the json grammar you can basically provide it the grammar",
				"tokens": [
					51208,
					1254,
					295,
					300,
					689,
					2602,
					295,
					1940,
					264,
					361,
					3015,
					22317,
					291,
					393,
					1936,
					2893,
					309,
					264,
					22317,
					51418
				],
				"temperature": 0,
				"avg_logprob": -0.054153435,
				"compression_ratio": 1.9419087,
				"no_speech_prob": 1.3775846e-12
			},
			{
				"id": 164,
				"seek": 5960,
				"start": 423.54114,
				"end": 428.84113,
				"text": " of your choice any regex and that just zeros out all of the log probabilities for anything that",
				"tokens": [
					51418,
					295,
					428,
					3922,
					604,
					319,
					432,
					87,
					293,
					300,
					445,
					35193,
					484,
					439,
					295,
					264,
					3565,
					33783,
					337,
					1340,
					300,
					51683
				],
				"temperature": 0,
				"avg_logprob": -0.054153435,
				"compression_ratio": 1.9419087,
				"no_speech_prob": 1.3775846e-12
			},
			{
				"id": 165,
				"seek": 8596,
				"start": 428.84113,
				"end": 435.3611,
				"text": " doesn't match your regex right exactly so if i go back to the whiteboard what this is doing is this",
				"tokens": [
					50365,
					1177,
					380,
					2995,
					428,
					319,
					432,
					87,
					558,
					2293,
					370,
					498,
					741,
					352,
					646,
					281,
					264,
					2418,
					3787,
					437,
					341,
					307,
					884,
					307,
					341,
					50691
				],
				"temperature": 0,
				"avg_logprob": -0.03303501,
				"compression_ratio": 1.7890909,
				"no_speech_prob": 1.6815114e-12
			},
			{
				"id": 166,
				"seek": 8596,
				"start": 435.3611,
				"end": 438.8611,
				"text": " is basically saying like even though the model has a really high probability for it i'll vote this",
				"tokens": [
					50691,
					307,
					1936,
					1566,
					411,
					754,
					1673,
					264,
					2316,
					575,
					257,
					534,
					1090,
					8482,
					337,
					309,
					741,
					603,
					4740,
					341,
					50866
				],
				"temperature": 0,
				"avg_logprob": -0.03303501,
				"compression_ratio": 1.7890909,
				"no_speech_prob": 1.6815114e-12
			},
			{
				"id": 167,
				"seek": 8596,
				"start": 438.8611,
				"end": 443.68112,
				"text": " is zero because it doesn't match the regex of what's allowed so only the only the regex is the",
				"tokens": [
					50866,
					307,
					4018,
					570,
					309,
					1177,
					380,
					2995,
					264,
					319,
					432,
					87,
					295,
					437,
					311,
					4350,
					370,
					787,
					264,
					787,
					264,
					319,
					432,
					87,
					307,
					264,
					51107
				],
				"temperature": 0,
				"avg_logprob": -0.03303501,
				"compression_ratio": 1.7890909,
				"no_speech_prob": 1.6815114e-12
			},
			{
				"id": 168,
				"seek": 8596,
				"start": 443.68112,
				"end": 449.42114,
				"text": " valid consideration and then i pick the best token from the valid regex now the problem that you will",
				"tokens": [
					51107,
					7363,
					12381,
					293,
					550,
					741,
					1888,
					264,
					1151,
					14862,
					490,
					264,
					7363,
					319,
					432,
					87,
					586,
					264,
					1154,
					300,
					291,
					486,
					51394
				],
				"temperature": 0,
				"avg_logprob": -0.03303501,
				"compression_ratio": 1.7890909,
				"no_speech_prob": 1.6815114e-12
			},
			{
				"id": 169,
				"seek": 8596,
				"start": 449.42114,
				"end": 457.44113,
				"text": " run into with this is sorry i have a lot of tabs open today yeah we talked about this a lot this",
				"tokens": [
					51394,
					1190,
					666,
					365,
					341,
					307,
					2597,
					741,
					362,
					257,
					688,
					295,
					20743,
					1269,
					965,
					1338,
					321,
					2825,
					466,
					341,
					257,
					688,
					341,
					51795
				],
				"temperature": 0,
				"avg_logprob": -0.03303501,
				"compression_ratio": 1.7890909,
				"no_speech_prob": 1.6815114e-12
			},
			{
				"id": 170,
				"seek": 11456,
				"start": 457.44113,
				"end": 461.98114,
				"text": " concept in cracking the prompting interview, right? That was the one you talked about like",
				"tokens": [
					50365,
					3410,
					294,
					25229,
					264,
					12391,
					278,
					4049,
					11,
					558,
					30,
					663,
					390,
					264,
					472,
					291,
					2825,
					466,
					411,
					50592
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 171,
				"seek": 11456,
				"start": 461.98114,
				"end": 467.40112,
				"text": " how to get AI to write better code by allowing it to write things that might not necessarily be",
				"tokens": [
					50592,
					577,
					281,
					483,
					7318,
					281,
					2464,
					1101,
					3089,
					538,
					8293,
					309,
					281,
					2464,
					721,
					300,
					1062,
					406,
					4725,
					312,
					50863
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 172,
				"seek": 11456,
				"start": 467.40112,
				"end": 471.40112,
				"text": " valid JSON, but are like closer to the way that the model has been trained to write code,",
				"tokens": [
					50863,
					7363,
					31828,
					11,
					457,
					366,
					411,
					4966,
					281,
					264,
					636,
					300,
					264,
					2316,
					575,
					668,
					8895,
					281,
					2464,
					3089,
					11,
					51063
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 173,
				"seek": 11456,
				"start": 471.40112,
				"end": 475.30115,
				"text": " which is by like reading code, not as snippets and JSON strings.",
				"tokens": [
					51063,
					597,
					307,
					538,
					411,
					3760,
					3089,
					11,
					406,
					382,
					35623,
					1385,
					293,
					31828,
					13985,
					13,
					51258
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 174,
				"seek": 11456,
				"start": 475.50113,
				"end": 479.84113,
				"text": " The function calling system does basically this. It basically says, instead of having the model",
				"tokens": [
					51268,
					440,
					2445,
					5141,
					1185,
					775,
					1936,
					341,
					13,
					467,
					1936,
					1619,
					11,
					2602,
					295,
					1419,
					264,
					2316,
					51485
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 175,
				"seek": 11456,
				"start": 479.84113,
				"end": 485.08112,
				"text": " always follow the grammar, it says, hey, you will emit a special token that says you want to take",
				"tokens": [
					51485,
					1009,
					1524,
					264,
					22317,
					11,
					309,
					1619,
					11,
					4177,
					11,
					291,
					486,
					32084,
					257,
					2121,
					14862,
					300,
					1619,
					291,
					528,
					281,
					747,
					51747
				],
				"temperature": 0,
				"avg_logprob": -0.08508742,
				"compression_ratio": 1.7258065,
				"no_speech_prob": 2.4657153e-12
			},
			{
				"id": 176,
				"seek": 14220,
				"start": 485.08112,
				"end": 489.56113,
				"text": " an action and then I will force you to follow the grammar. So at that point, the model is basically",
				"tokens": [
					50365,
					364,
					3069,
					293,
					550,
					286,
					486,
					3464,
					291,
					281,
					1524,
					264,
					22317,
					13,
					407,
					412,
					300,
					935,
					11,
					264,
					2316,
					307,
					1936,
					50589
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 177,
				"seek": 14220,
				"start": 489.56113,
				"end": 493.80115,
				"text": " deciding to some degree it can be taught when it should follow the grammar that it's given.",
				"tokens": [
					50589,
					17990,
					281,
					512,
					4314,
					309,
					393,
					312,
					5928,
					562,
					309,
					820,
					1524,
					264,
					22317,
					300,
					309,
					311,
					2212,
					13,
					50801
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 178,
				"seek": 14220,
				"start": 494.22113,
				"end": 498.86115,
				"text": " And now OpenAID has a new thing, which I think is really, really good for everyone, which is they've",
				"tokens": [
					50822,
					400,
					586,
					7238,
					32,
					2777,
					575,
					257,
					777,
					551,
					11,
					597,
					286,
					519,
					307,
					534,
					11,
					534,
					665,
					337,
					1518,
					11,
					597,
					307,
					436,
					600,
					51054
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 179,
				"seek": 14220,
				"start": 498.86115,
				"end": 503.04114,
				"text": " allowed you to do function calling without a grammar or with a custom grammar. So now you can",
				"tokens": [
					51054,
					4350,
					291,
					281,
					360,
					2445,
					5141,
					1553,
					257,
					22317,
					420,
					365,
					257,
					2375,
					22317,
					13,
					407,
					586,
					291,
					393,
					51263
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 180,
				"seek": 14220,
				"start": 503.04114,
				"end": 506.78113,
				"text": " basically let the model say, I want to use an action and then let it freeform output whatever",
				"tokens": [
					51263,
					1936,
					718,
					264,
					2316,
					584,
					11,
					286,
					528,
					281,
					764,
					364,
					3069,
					293,
					550,
					718,
					309,
					1737,
					837,
					5598,
					2035,
					51450
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 181,
				"seek": 14220,
				"start": 506.78113,
				"end": 513.48114,
				"text": " it wants. But what the Manus paper article talks about is that when it actually picks the use tool",
				"tokens": [
					51450,
					309,
					2738,
					13,
					583,
					437,
					264,
					2458,
					301,
					3035,
					7222,
					6686,
					466,
					307,
					300,
					562,
					309,
					767,
					16137,
					264,
					764,
					2290,
					51785
				],
				"temperature": 0,
				"avg_logprob": -0.09821318,
				"compression_ratio": 1.8380952,
				"no_speech_prob": 1.6743206e-12
			},
			{
				"id": 182,
				"seek": 17060,
				"start": 513.48114,
				"end": 519.04114,
				"text": " action at that point the model is now going to pick a special token it's going to pick a certain",
				"tokens": [
					50365,
					3069,
					412,
					300,
					935,
					264,
					2316,
					307,
					586,
					516,
					281,
					1888,
					257,
					2121,
					14862,
					309,
					311,
					516,
					281,
					1888,
					257,
					1629,
					50643
				],
				"temperature": 0,
				"avg_logprob": -0.05257973,
				"compression_ratio": 2.1936936,
				"no_speech_prob": 1.9204924e-12
			},
			{
				"id": 183,
				"seek": 17060,
				"start": 519.04114,
				"end": 524.54114,
				"text": " uh the first thing that comes out is the name of the function right exactly it's gonna have to spit",
				"tokens": [
					50643,
					2232,
					264,
					700,
					551,
					300,
					1487,
					484,
					307,
					264,
					1315,
					295,
					264,
					2445,
					558,
					2293,
					309,
					311,
					799,
					362,
					281,
					22127,
					50918
				],
				"temperature": 0,
				"avg_logprob": -0.05257973,
				"compression_ratio": 2.1936936,
				"no_speech_prob": 1.9204924e-12
			},
			{
				"id": 184,
				"seek": 17060,
				"start": 524.54114,
				"end": 527.92114,
				"text": " out the name of the function instead of actually letting the model spit out the name of the",
				"tokens": [
					50918,
					484,
					264,
					1315,
					295,
					264,
					2445,
					2602,
					295,
					767,
					8295,
					264,
					2316,
					22127,
					484,
					264,
					1315,
					295,
					264,
					51087
				],
				"temperature": 0,
				"avg_logprob": -0.05257973,
				"compression_ratio": 2.1936936,
				"no_speech_prob": 1.9204924e-12
			},
			{
				"id": 185,
				"seek": 17060,
				"start": 527.92114,
				"end": 535.66113,
				"text": " function at that point you can effectively where'd it go you can effectively prevent the model from",
				"tokens": [
					51087,
					2445,
					412,
					300,
					935,
					291,
					393,
					8659,
					689,
					1116,
					309,
					352,
					291,
					393,
					8659,
					4871,
					264,
					2316,
					490,
					51474
				],
				"temperature": 0,
				"avg_logprob": -0.05257973,
				"compression_ratio": 2.1936936,
				"no_speech_prob": 1.9204924e-12
			},
			{
				"id": 186,
				"seek": 17060,
				"start": 535.66113,
				"end": 541.92114,
				"text": " picking a certain tool at that point and say that by just zeroing the probability that the name of",
				"tokens": [
					51474,
					8867,
					257,
					1629,
					2290,
					412,
					300,
					935,
					293,
					584,
					300,
					538,
					445,
					4018,
					278,
					264,
					8482,
					300,
					264,
					1315,
					295,
					51787
				],
				"temperature": 0,
				"avg_logprob": -0.05257973,
				"compression_ratio": 2.1936936,
				"no_speech_prob": 1.9204924e-12
			},
			{
				"id": 187,
				"seek": 19904,
				"start": 541.92114,
				"end": 547.10114,
				"text": " the function matches, like the basically like forcing, only allowing the log probs that",
				"tokens": [
					50365,
					264,
					2445,
					10676,
					11,
					411,
					264,
					1936,
					411,
					19030,
					11,
					787,
					8293,
					264,
					3565,
					1239,
					82,
					300,
					50624
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 188,
				"seek": 19904,
				"start": 547.10114,
				"end": 549.16113,
				"text": " actually match the tool set that you want to constrain to.",
				"tokens": [
					50624,
					767,
					2995,
					264,
					2290,
					992,
					300,
					291,
					528,
					281,
					1817,
					7146,
					281,
					13,
					50727
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 189,
				"seek": 19904,
				"start": 549.80115,
				"end": 550.2011,
				"text": " Exactly.",
				"tokens": [
					50759,
					7587,
					13,
					50779
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 190,
				"seek": 19904,
				"start": 551.2811,
				"end": 556.3211,
				"text": " So this is actually a really, really good technique, but I'm going to show you, and I",
				"tokens": [
					50833,
					407,
					341,
					307,
					767,
					257,
					534,
					11,
					534,
					665,
					6532,
					11,
					457,
					286,
					478,
					516,
					281,
					855,
					291,
					11,
					293,
					286,
					51085
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 191,
				"seek": 19904,
				"start": 556.3211,
				"end": 560.7611,
				"text": " thought it's really, really clever, but I'm going to show you how naming your tools in",
				"tokens": [
					51085,
					1194,
					309,
					311,
					534,
					11,
					534,
					13494,
					11,
					457,
					286,
					478,
					516,
					281,
					855,
					291,
					577,
					25290,
					428,
					3873,
					294,
					51307
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 192,
				"seek": 19904,
				"start": 560.7611,
				"end": 565.86115,
				"text": " an interesting way can dramatically change the accuracy of this technique.",
				"tokens": [
					51307,
					364,
					1880,
					636,
					393,
					17548,
					1319,
					264,
					14170,
					295,
					341,
					6532,
					13,
					51562
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 193,
				"seek": 19904,
				"start": 567.5011,
				"end": 567.94116,
				"text": " Cursor.",
				"tokens": [
					51644,
					383,
					2156,
					284,
					13,
					51666
				],
				"temperature": 0,
				"avg_logprob": -0.18101996,
				"compression_ratio": 1.7715517,
				"no_speech_prob": 1.8182775e-12
			},
			{
				"id": 194,
				"seek": 22506,
				"start": 567.94116,
				"end": 572.1411,
				"text": " And I'm going to do this with an example, I think, in code, because it's going to be a little bit better.",
				"tokens": [
					50365,
					400,
					286,
					478,
					516,
					281,
					360,
					341,
					365,
					364,
					1365,
					11,
					286,
					519,
					11,
					294,
					3089,
					11,
					570,
					309,
					311,
					516,
					281,
					312,
					257,
					707,
					857,
					1101,
					13,
					50575
				],
				"temperature": 0,
				"avg_logprob": -0.13230492,
				"compression_ratio": 1.7957747,
				"no_speech_prob": 2.0282636e-12
			},
			{
				"id": 195,
				"seek": 22506,
				"start": 574.2011,
				"end": 579.8411,
				"text": " And just while you're doing that, the question was like, when talking about tool calls, are we talking about explicit tool calling by model providers?",
				"tokens": [
					50678,
					400,
					445,
					1339,
					291,
					434,
					884,
					300,
					11,
					264,
					1168,
					390,
					411,
					11,
					562,
					1417,
					466,
					2290,
					5498,
					11,
					366,
					321,
					1417,
					466,
					13691,
					2290,
					5141,
					538,
					2316,
					11330,
					30,
					50960
				],
				"temperature": 0,
				"avg_logprob": -0.13230492,
				"compression_ratio": 1.7957747,
				"no_speech_prob": 2.0282636e-12
			},
			{
				"id": 196,
				"seek": 22506,
				"start": 579.9011,
				"end": 582.5011,
				"text": " Or are we also thinking about whatever structured output techniques we're using?",
				"tokens": [
					50963,
					1610,
					366,
					321,
					611,
					1953,
					466,
					2035,
					18519,
					5598,
					7512,
					321,
					434,
					1228,
					30,
					51093
				],
				"temperature": 0,
				"avg_logprob": -0.13230492,
				"compression_ratio": 1.7957747,
				"no_speech_prob": 2.0282636e-12
			},
			{
				"id": 197,
				"seek": 22506,
				"start": 582.68115,
				"end": 593.3211,
				"text": " I think tool calling, structured output, and function calling, while there are different flavors of each of them, I think those three words tend to refer to the same thing.",
				"tokens": [
					51102,
					286,
					519,
					2290,
					5141,
					11,
					18519,
					5598,
					11,
					293,
					2445,
					5141,
					11,
					1339,
					456,
					366,
					819,
					16303,
					295,
					1184,
					295,
					552,
					11,
					286,
					519,
					729,
					1045,
					2283,
					3928,
					281,
					2864,
					281,
					264,
					912,
					551,
					13,
					51634
				],
				"temperature": 0,
				"avg_logprob": -0.13230492,
				"compression_ratio": 1.7957747,
				"no_speech_prob": 2.0282636e-12
			},
			{
				"id": 198,
				"seek": 25044,
				"start": 593.3211,
				"end": 600.16113,
				"text": " Some people say like, oh, tool calling is only when you use JSON mode and structured output is, can, doesn't even need to use JSON mode.",
				"tokens": [
					50365,
					2188,
					561,
					584,
					411,
					11,
					1954,
					11,
					2290,
					5141,
					307,
					787,
					562,
					291,
					764,
					31828,
					4391,
					293,
					18519,
					5598,
					307,
					11,
					393,
					11,
					1177,
					380,
					754,
					643,
					281,
					764,
					31828,
					4391,
					13,
					50707
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 199,
				"seek": 25044,
				"start": 600.2411,
				"end": 606.3411,
				"text": " But again, like I use all three of those terms interchangeably, tool calling, structured output, function calling.",
				"tokens": [
					50711,
					583,
					797,
					11,
					411,
					286,
					764,
					439,
					1045,
					295,
					729,
					2115,
					30358,
					1188,
					11,
					2290,
					5141,
					11,
					18519,
					5598,
					11,
					2445,
					5141,
					13,
					51016
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 200,
				"seek": 25044,
				"start": 606.7011,
				"end": 607.42114,
				"text": " They're all the same.",
				"tokens": [
					51034,
					814,
					434,
					439,
					264,
					912,
					13,
					51070
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 201,
				"seek": 25044,
				"start": 607.44116,
				"end": 607.92114,
				"text": " Is that accurate?",
				"tokens": [
					51071,
					1119,
					300,
					8559,
					30,
					51095
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 202,
				"seek": 25044,
				"start": 608.48114,
				"end": 608.8611,
				"text": " I agree.",
				"tokens": [
					51123,
					286,
					3986,
					13,
					51142
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 203,
				"seek": 25044,
				"start": 609.18115,
				"end": 612.8611,
				"text": " There's technical implementation differences that might have different trade-offs, but they're all the same.",
				"tokens": [
					51158,
					821,
					311,
					6191,
					11420,
					7300,
					300,
					1062,
					362,
					819,
					4923,
					12,
					19231,
					11,
					457,
					436,
					434,
					439,
					264,
					912,
					13,
					51342
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 204,
				"seek": 25044,
				"start": 613.4611,
				"end": 617.5811,
				"text": " Fundamentally, what you're doing is trying to constrain what the model outputs to some degree in some manner or form.",
				"tokens": [
					51372,
					13493,
					2466,
					379,
					11,
					437,
					291,
					434,
					884,
					307,
					1382,
					281,
					1817,
					7146,
					437,
					264,
					2316,
					23930,
					281,
					512,
					4314,
					294,
					512,
					9060,
					420,
					1254,
					13,
					51578
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 205,
				"seek": 25044,
				"start": 618.48114,
				"end": 619.98114,
				"text": " Well, and it's the constraint.",
				"tokens": [
					51623,
					1042,
					11,
					293,
					309,
					311,
					264,
					25534,
					13,
					51698
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 206,
				"seek": 25044,
				"start": 620.04114,
				"end": 621.04114,
				"text": " It's not even about the constraining.",
				"tokens": [
					51701,
					467,
					311,
					406,
					754,
					466,
					264,
					11525,
					1760,
					13,
					51751
				],
				"temperature": 0,
				"avg_logprob": -0.14615956,
				"compression_ratio": 1.8282208,
				"no_speech_prob": 2.0285139e-12
			},
			{
				"id": 207,
				"seek": 27816,
				"start": 621.04114,
				"end": 628.5011,
				"text": " I think for me, it's more so about we're going to create something that a deterministic program can consume.",
				"tokens": [
					50365,
					286,
					519,
					337,
					385,
					11,
					309,
					311,
					544,
					370,
					466,
					321,
					434,
					516,
					281,
					1884,
					746,
					300,
					257,
					15957,
					3142,
					1461,
					393,
					14732,
					13,
					50738
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 208,
				"seek": 27816,
				"start": 628.68115,
				"end": 630.3611,
				"text": " It's not for a human and it's not for a model.",
				"tokens": [
					50747,
					467,
					311,
					406,
					337,
					257,
					1952,
					293,
					309,
					311,
					406,
					337,
					257,
					2316,
					13,
					50831
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 209,
				"seek": 27816,
				"start": 630.5811,
				"end": 632.32117,
				"text": " It's for Python code that I wrote.",
				"tokens": [
					50842,
					467,
					311,
					337,
					15329,
					3089,
					300,
					286,
					4114,
					13,
					50929
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 210,
				"seek": 27816,
				"start": 632.7211,
				"end": 637.04114,
				"text": " And so it has to have some expected structure that I can turn it into bytes in memory.",
				"tokens": [
					50949,
					400,
					370,
					309,
					575,
					281,
					362,
					512,
					5176,
					3877,
					300,
					286,
					393,
					1261,
					309,
					666,
					36088,
					294,
					4675,
					13,
					51165
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 211,
				"seek": 27816,
				"start": 637.8611,
				"end": 637.94116,
				"text": " Yeah.",
				"tokens": [
					51206,
					865,
					13,
					51210
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 212,
				"seek": 27816,
				"start": 638.4611,
				"end": 642.06116,
				"text": " So let's say I have these three tools, right?",
				"tokens": [
					51236,
					407,
					718,
					311,
					584,
					286,
					362,
					613,
					1045,
					3873,
					11,
					558,
					30,
					51416
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 213,
				"seek": 27816,
				"start": 642.80115,
				"end": 644.10114,
				"text": " And these are the tools I have.",
				"tokens": [
					51453,
					400,
					613,
					366,
					264,
					3873,
					286,
					362,
					13,
					51518
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 214,
				"seek": 27816,
				"start": 644.2211,
				"end": 645.80115,
				"text": " Call me, call mom, call texter.",
				"tokens": [
					51524,
					7807,
					385,
					11,
					818,
					1225,
					11,
					818,
					535,
					36671,
					13,
					51603
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 215,
				"seek": 27816,
				"start": 646.68115,
				"end": 649.68115,
				"text": " And let's say I'm operating the mode in like work mode.",
				"tokens": [
					51647,
					400,
					718,
					311,
					584,
					286,
					478,
					7447,
					264,
					4391,
					294,
					411,
					589,
					4391,
					13,
					51797
				],
				"temperature": 0,
				"avg_logprob": -0.16641861,
				"compression_ratio": 1.7335907,
				"no_speech_prob": 1.7970937e-12
			},
			{
				"id": 216,
				"seek": 30680,
				"start": 649.68115,
				"end": 657.48114,
				"text": " in work mode i want the tools call dexter and call me to be available in non-work mode so like uh",
				"tokens": [
					50365,
					294,
					589,
					4391,
					741,
					528,
					264,
					3873,
					818,
					368,
					36671,
					293,
					818,
					385,
					281,
					312,
					2435,
					294,
					2107,
					12,
					1902,
					4391,
					370,
					411,
					2232,
					50755
				],
				"temperature": 0,
				"avg_logprob": -0.11828975,
				"compression_ratio": 2.037234,
				"no_speech_prob": 9.001864e-13
			},
			{
				"id": 217,
				"seek": 30680,
				"start": 657.48114,
				"end": 662.3611,
				"text": " i want the tools call mom and call me to be available i'll never call dexter non-work mode",
				"tokens": [
					50755,
					741,
					528,
					264,
					3873,
					818,
					1225,
					293,
					818,
					385,
					281,
					312,
					2435,
					741,
					603,
					1128,
					818,
					368,
					36671,
					2107,
					12,
					1902,
					4391,
					50999
				],
				"temperature": 0,
				"avg_logprob": -0.11828975,
				"compression_ratio": 2.037234,
				"no_speech_prob": 9.001864e-13
			},
			{
				"id": 218,
				"seek": 30680,
				"start": 662.3611,
				"end": 668.9611,
				"text": " i don't like hanging out with it that much so well what we're going to do the model decide i'll",
				"tokens": [
					50999,
					741,
					500,
					380,
					411,
					8345,
					484,
					365,
					309,
					300,
					709,
					370,
					731,
					437,
					321,
					434,
					516,
					281,
					360,
					264,
					2316,
					4536,
					741,
					603,
					51329
				],
				"temperature": 0,
				"avg_logprob": -0.11828975,
				"compression_ratio": 2.037234,
				"no_speech_prob": 9.001864e-13
			},
			{
				"id": 219,
				"seek": 30680,
				"start": 668.9611,
				"end": 675.7411,
				"text": " say a statement like uh i need to talk to someone i'm going to go write the statement in this case",
				"tokens": [
					51329,
					584,
					257,
					5629,
					411,
					2232,
					741,
					643,
					281,
					751,
					281,
					1580,
					741,
					478,
					516,
					281,
					352,
					2464,
					264,
					5629,
					294,
					341,
					1389,
					51668
				],
				"temperature": 0,
				"avg_logprob": -0.11828975,
				"compression_ratio": 2.037234,
				"no_speech_prob": 9.001864e-13
			},
			{
				"id": 220,
				"seek": 33286,
				"start": 675.7411,
				"end": 680.8411,
				"text": " call me is clearly not a good tool because I want to talk to someone else, not myself. So it should",
				"tokens": [
					50365,
					818,
					385,
					307,
					4448,
					406,
					257,
					665,
					2290,
					570,
					286,
					528,
					281,
					751,
					281,
					1580,
					1646,
					11,
					406,
					2059,
					13,
					407,
					309,
					820,
					50620
				],
				"temperature": 0,
				"avg_logprob": -0.112853006,
				"compression_ratio": 1.6049383,
				"no_speech_prob": 1.5252074e-12
			},
			{
				"id": 221,
				"seek": 33286,
				"start": 680.8411,
				"end": 687.04114,
				"text": " call mom or Dexter. If I give it all the tools as context, it could really pick any of them as",
				"tokens": [
					50620,
					818,
					1225,
					420,
					1346,
					36671,
					13,
					759,
					286,
					976,
					309,
					439,
					264,
					3873,
					382,
					4319,
					11,
					309,
					727,
					534,
					1888,
					604,
					295,
					552,
					382,
					50930
				],
				"temperature": 0,
				"avg_logprob": -0.112853006,
				"compression_ratio": 1.6049383,
				"no_speech_prob": 1.5252074e-12
			},
			{
				"id": 222,
				"seek": 33286,
				"start": 687.04114,
				"end": 693.2211,
				"text": " random. In work mode, however, maybe there's another thread in here that says like, I'm at work.",
				"tokens": [
					50930,
					4974,
					13,
					682,
					589,
					4391,
					11,
					4461,
					11,
					1310,
					456,
					311,
					1071,
					7207,
					294,
					510,
					300,
					1619,
					411,
					11,
					286,
					478,
					412,
					589,
					13,
					51239
				],
				"temperature": 0,
				"avg_logprob": -0.112853006,
				"compression_ratio": 1.6049383,
				"no_speech_prob": 1.5252074e-12
			},
			{
				"id": 223,
				"seek": 697,
				"start": 694.2811,
				"end": 717.60223,
				"text": " So the model and there some information I have in my program state that allows me to know that So I have some at work Boolean as true as true or false if it is i basically want to say like disable call mom and i want to go disable call mom at that point if i going to go",
				"tokens": [
					51292,
					407,
					264,
					2316,
					11,
					293,
					456,
					311,
					512,
					1589,
					286,
					362,
					294,
					452,
					1461,
					1785,
					300,
					4045,
					385,
					281,
					458,
					300,
					13,
					51502,
					50365,
					407,
					286,
					362,
					512,
					412,
					589,
					23351,
					28499,
					382,
					2074,
					420,
					382,
					2074,
					420,
					7908,
					498,
					309,
					307,
					741,
					1936,
					528,
					50837,
					50837,
					281,
					584,
					411,
					28362,
					818,
					1225,
					293,
					741,
					528,
					281,
					352,
					28362,
					818,
					1225,
					412,
					300,
					935,
					498,
					741,
					478,
					516,
					281,
					352,
					51361
				],
				"temperature": 0,
				"avg_logprob": -0.075280435,
				"compression_ratio": 1.5819672,
				"no_speech_prob": 1.2351556e-12
			},
			{
				"id": 224,
				"seek": 2689,
				"start": 717.60223,
				"end": 726.94226,
				"text": " do that. Sorry, I'm going to turn this off text. That is not correct. If I'm going to go do that,",
				"tokens": [
					50365,
					360,
					300,
					13,
					4919,
					11,
					286,
					478,
					516,
					281,
					1261,
					341,
					766,
					2487,
					13,
					663,
					307,
					406,
					3006,
					13,
					759,
					286,
					478,
					516,
					281,
					352,
					360,
					300,
					11,
					50832
				],
				"temperature": 0,
				"avg_logprob": -0.11486241,
				"compression_ratio": 1.7729468,
				"no_speech_prob": 1.3829497e-12
			},
			{
				"id": 225,
				"seek": 2689,
				"start": 727.2822,
				"end": 732.7023,
				"text": " the model will start generating tokens. Now, the problem with this is the model may want to",
				"tokens": [
					50849,
					264,
					2316,
					486,
					722,
					17746,
					22667,
					13,
					823,
					11,
					264,
					1154,
					365,
					341,
					307,
					264,
					2316,
					815,
					528,
					281,
					51120
				],
				"temperature": 0,
				"avg_logprob": -0.11486241,
				"compression_ratio": 1.7729468,
				"no_speech_prob": 1.3829497e-12
			},
			{
				"id": 226,
				"seek": 2689,
				"start": 732.7023,
				"end": 736.86224,
				"text": " generate a token that starts, maybe the token vocabulary makes it so that the tokens are",
				"tokens": [
					51120,
					8460,
					257,
					14862,
					300,
					3719,
					11,
					1310,
					264,
					14862,
					19864,
					1669,
					309,
					370,
					300,
					264,
					22667,
					366,
					51328
				],
				"temperature": 0,
				"avg_logprob": -0.11486241,
				"compression_ratio": 1.7729468,
				"no_speech_prob": 1.3829497e-12
			},
			{
				"id": 227,
				"seek": 2689,
				"start": 736.86224,
				"end": 743.26227,
				"text": " actually call and then mom. So the tokens end up being call and then mom. And over here,",
				"tokens": [
					51328,
					767,
					818,
					293,
					550,
					1225,
					13,
					407,
					264,
					22667,
					917,
					493,
					885,
					818,
					293,
					550,
					1225,
					13,
					400,
					670,
					510,
					11,
					51648
				],
				"temperature": 0,
				"avg_logprob": -0.11486241,
				"compression_ratio": 1.7729468,
				"no_speech_prob": 1.3829497e-12
			},
			{
				"id": 228,
				"seek": 5255,
				"start": 743.26227,
				"end": 750.4623,
				"text": " the tokens end up being call and then dexter if the model generates a token call",
				"tokens": [
					50365,
					264,
					22667,
					917,
					493,
					885,
					818,
					293,
					550,
					368,
					36671,
					498,
					264,
					2316,
					23815,
					257,
					14862,
					818,
					50725
				],
				"temperature": 0,
				"avg_logprob": -0.05734848,
				"compression_ratio": 2.0852017,
				"no_speech_prob": 1.0900725e-12
			},
			{
				"id": 229,
				"seek": 5255,
				"start": 750.4623,
				"end": 756.00226,
				"text": " even though i've invalidated this i've invalidated call mom the model may have actually thought it",
				"tokens": [
					50725,
					754,
					1673,
					741,
					600,
					34702,
					770,
					341,
					741,
					600,
					34702,
					770,
					818,
					1225,
					264,
					2316,
					815,
					362,
					767,
					1194,
					309,
					51002
				],
				"temperature": 0,
				"avg_logprob": -0.05734848,
				"compression_ratio": 2.0852017,
				"no_speech_prob": 1.0900725e-12
			},
			{
				"id": 230,
				"seek": 5255,
				"start": 756.00226,
				"end": 760.9623,
				"text": " was really important to call mom and it may be actually accidentally forced to picking the call",
				"tokens": [
					51002,
					390,
					534,
					1021,
					281,
					818,
					1225,
					293,
					309,
					815,
					312,
					767,
					15715,
					7579,
					281,
					8867,
					264,
					818,
					51250
				],
				"temperature": 0,
				"avg_logprob": -0.05734848,
				"compression_ratio": 2.0852017,
				"no_speech_prob": 1.0900725e-12
			},
			{
				"id": 231,
				"seek": 5255,
				"start": 760.9623,
				"end": 765.94226,
				"text": " dexter tool forced to call me yeah because it doesn't have a choice because it produced the",
				"tokens": [
					51250,
					368,
					36671,
					2290,
					7579,
					281,
					818,
					385,
					1338,
					570,
					309,
					1177,
					380,
					362,
					257,
					3922,
					570,
					309,
					7126,
					264,
					51499
				],
				"temperature": 0,
				"avg_logprob": -0.05734848,
				"compression_ratio": 2.0852017,
				"no_speech_prob": 1.0900725e-12
			},
			{
				"id": 232,
				"seek": 5255,
				"start": 765.94226,
				"end": 772.98224,
				"text": " word call uh now with the intent to call mom with the intent to call mom because i'm at work mode",
				"tokens": [
					51499,
					1349,
					818,
					2232,
					586,
					365,
					264,
					8446,
					281,
					818,
					1225,
					365,
					264,
					8446,
					281,
					818,
					1225,
					570,
					741,
					478,
					412,
					589,
					4391,
					51851
				],
				"temperature": 0,
				"avg_logprob": -0.05734848,
				"compression_ratio": 2.0852017,
				"no_speech_prob": 1.0900725e-12
			},
			{
				"id": 233,
				"seek": 8227,
				"start": 772.98224,
				"end": 775.00226,
				"text": " and I've accidentally disabled the call mom tool.",
				"tokens": [
					50365,
					293,
					286,
					600,
					15715,
					15191,
					264,
					818,
					1225,
					2290,
					13,
					50466
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 234,
				"seek": 8227,
				"start": 775.76227,
				"end": 778.50226,
				"text": " And now the model thinks it wants to call mom",
				"tokens": [
					50504,
					400,
					586,
					264,
					2316,
					7309,
					309,
					2738,
					281,
					818,
					1225,
					50641
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 235,
				"seek": 8227,
				"start": 778.50226,
				"end": 781.10223,
				"text": " because all my tools are in the context window.",
				"tokens": [
					50641,
					570,
					439,
					452,
					3873,
					366,
					294,
					264,
					4319,
					4910,
					13,
					50771
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 236,
				"seek": 8227,
				"start": 781.24225,
				"end": 783.6423,
				"text": " So even if the model doesn't know,",
				"tokens": [
					50778,
					407,
					754,
					498,
					264,
					2316,
					1177,
					380,
					458,
					11,
					50898
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 237,
				"seek": 8227,
				"start": 783.68225,
				"end": 785.2023,
				"text": " it can't actually pick call mom.",
				"tokens": [
					50900,
					309,
					393,
					380,
					767,
					1888,
					818,
					1225,
					13,
					50976
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 238,
				"seek": 8227,
				"start": 785.62225,
				"end": 788.2023,
				"text": " As far as the model knows, it can pick call mom.",
				"tokens": [
					50997,
					1018,
					1400,
					382,
					264,
					2316,
					3255,
					11,
					309,
					393,
					1888,
					818,
					1225,
					13,
					51126
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 239,
				"seek": 8227,
				"start": 789.30225,
				"end": 790.8423,
				"text": " So now it might even go do this.",
				"tokens": [
					51181,
					407,
					586,
					309,
					1062,
					754,
					352,
					360,
					341,
					13,
					51258
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 240,
				"seek": 8227,
				"start": 790.94226,
				"end": 792.7222,
				"text": " It might even start outputting call m",
				"tokens": [
					51263,
					467,
					1062,
					754,
					722,
					5598,
					783,
					818,
					275,
					51352
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 241,
				"seek": 8227,
				"start": 792.7222,
				"end": 795.88226,
				"text": " because m was a valid token as well in my token vocabulary.",
				"tokens": [
					51352,
					570,
					275,
					390,
					257,
					7363,
					14862,
					382,
					731,
					294,
					452,
					14862,
					19864,
					13,
					51510
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 242,
				"seek": 8227,
				"start": 797.26227,
				"end": 798.4022,
				"text": " And funnily enough,",
				"tokens": [
					51579,
					400,
					1019,
					77,
					953,
					1547,
					11,
					51636
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 243,
				"seek": 8227,
				"start": 798.54224,
				"end": 800.36224,
				"text": " the model might not even end up calling dux",
				"tokens": [
					51643,
					264,
					2316,
					1062,
					406,
					754,
					917,
					493,
					5141,
					1581,
					87,
					51734
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 244,
				"seek": 8227,
				"start": 800.36224,
				"end": 802.12225,
				"text": " or it might end up calling me.",
				"tokens": [
					51734,
					420,
					309,
					1062,
					917,
					493,
					5141,
					385,
					13,
					51822
				],
				"temperature": 0,
				"avg_logprob": -0.14802493,
				"compression_ratio": 1.9209486,
				"no_speech_prob": 7.010442e-13
			},
			{
				"id": 245,
				"seek": 11227,
				"start": 802.98224,
				"end": 805.30225,
				"text": " Because in the token vocabulary, we have call.",
				"tokens": [
					50365,
					1436,
					294,
					264,
					14862,
					19864,
					11,
					321,
					362,
					818,
					13,
					50481
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 246,
				"seek": 11227,
				"start": 805.8423,
				"end": 808.0222,
				"text": " We obviously have the letter M.",
				"tokens": [
					50508,
					492,
					2745,
					362,
					264,
					5063,
					376,
					13,
					50617
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 247,
				"seek": 11227,
				"start": 808.2023,
				"end": 809.10223,
				"text": " We have the letter me.",
				"tokens": [
					50626,
					492,
					362,
					264,
					5063,
					385,
					13,
					50671
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 248,
				"seek": 11227,
				"start": 809.32227,
				"end": 809.94226,
				"text": " We have the word me.",
				"tokens": [
					50682,
					492,
					362,
					264,
					1349,
					385,
					13,
					50713
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 249,
				"seek": 11227,
				"start": 810.06226,
				"end": 810.9623,
				"text": " We have the letter mom.",
				"tokens": [
					50719,
					492,
					362,
					264,
					5063,
					1225,
					13,
					50764
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 250,
				"seek": 11227,
				"start": 811.16223,
				"end": 812.94226,
				"text": " We probably have the words MO as well.",
				"tokens": [
					50774,
					492,
					1391,
					362,
					264,
					2283,
					19290,
					382,
					731,
					13,
					50863
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 251,
				"seek": 11227,
				"start": 813.9022,
				"end": 814.56226,
				"text": " Oh, okay.",
				"tokens": [
					50911,
					876,
					11,
					1392,
					13,
					50944
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 252,
				"seek": 11227,
				"start": 814.60223,
				"end": 817.36224,
				"text": " So the probability for mom might have been 99%",
				"tokens": [
					50946,
					407,
					264,
					8482,
					337,
					1225,
					1062,
					362,
					668,
					11803,
					4,
					51084
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 253,
				"seek": 11227,
				"start": 817.36224,
				"end": 820.2023,
				"text": " and the probability for M might have been 1%.",
				"tokens": [
					51084,
					293,
					264,
					8482,
					337,
					376,
					1062,
					362,
					668,
					502,
					6856,
					51226
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 254,
				"seek": 11227,
				"start": 820.2023,
				"end": 824.26227,
				"text": " But because both of them are moving towards mom,",
				"tokens": [
					51226,
					583,
					570,
					1293,
					295,
					552,
					366,
					2684,
					3030,
					1225,
					11,
					51429
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 255,
				"seek": 11227,
				"start": 824.36224,
				"end": 825.5222,
				"text": " those are the ones that got picked.",
				"tokens": [
					51434,
					729,
					366,
					264,
					2306,
					300,
					658,
					6183,
					13,
					51492
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 256,
				"seek": 11227,
				"start": 825.76227,
				"end": 826.1423,
				"text": " Exactly.",
				"tokens": [
					51504,
					7587,
					13,
					51523
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 257,
				"seek": 11227,
				"start": 826.30225,
				"end": 828.0222,
				"text": " And MO might also be higher.",
				"tokens": [
					51531,
					400,
					19290,
					1062,
					611,
					312,
					2946,
					13,
					51617
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 258,
				"seek": 11227,
				"start": 828.0222,
				"end": 829.9022,
				"text": " So I've invalidated MO and mom",
				"tokens": [
					51617,
					407,
					286,
					600,
					34702,
					770,
					19290,
					293,
					1225,
					51711
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 259,
				"seek": 11227,
				"start": 829.9022,
				"end": 831.9022,
				"text": " because it doesn't meet the grammar of what's allowed",
				"tokens": [
					51711,
					570,
					309,
					1177,
					380,
					1677,
					264,
					22317,
					295,
					437,
					311,
					4350,
					51811
				],
				"temperature": 0,
				"avg_logprob": -0.19151723,
				"compression_ratio": 1.8168498,
				"no_speech_prob": 1.1334522e-12
			},
			{
				"id": 260,
				"seek": 14119,
				"start": 831.9022,
				"end": 834.06226,
				"text": " based on what tools are valid,",
				"tokens": [
					50365,
					2361,
					322,
					437,
					3873,
					366,
					7363,
					11,
					50473
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 261,
				"seek": 14119,
				"start": 834.2822,
				"end": 835.48224,
				"text": " but I still will call M.",
				"tokens": [
					50484,
					457,
					286,
					920,
					486,
					818,
					376,
					13,
					50544
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 262,
				"seek": 14119,
				"start": 835.80225,
				"end": 836.38226,
				"text": " So I'll call M.",
				"tokens": [
					50560,
					407,
					286,
					603,
					818,
					376,
					13,
					50589
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 263,
				"seek": 14119,
				"start": 836.56226,
				"end": 837.4022,
				"text": " And now I'm going to call M.",
				"tokens": [
					50598,
					400,
					586,
					286,
					478,
					516,
					281,
					818,
					376,
					13,
					50640
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 264,
				"seek": 14119,
				"start": 837.50226,
				"end": 838.42224,
				"text": " Well, what's the next best token?",
				"tokens": [
					50645,
					1042,
					11,
					437,
					311,
					264,
					958,
					1151,
					14862,
					30,
					50691
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 265,
				"seek": 14119,
				"start": 838.8423,
				"end": 840.62225,
				"text": " Well, there's only one valid token that's available here.",
				"tokens": [
					50712,
					1042,
					11,
					456,
					311,
					787,
					472,
					7363,
					14862,
					300,
					311,
					2435,
					510,
					13,
					50801
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 266,
				"seek": 14119,
				"start": 840.6423,
				"end": 841.4022,
				"text": " I have to call it E.",
				"tokens": [
					50802,
					286,
					362,
					281,
					818,
					309,
					462,
					13,
					50840
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 267,
				"seek": 14119,
				"start": 842.38226,
				"end": 844.26227,
				"text": " So now I'll end up calling the call me tool.",
				"tokens": [
					50889,
					407,
					586,
					286,
					603,
					917,
					493,
					5141,
					264,
					818,
					385,
					2290,
					13,
					50983
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 268,
				"seek": 14119,
				"start": 845.48224,
				"end": 847.74225,
				"text": " And you can see how actually doing this",
				"tokens": [
					51044,
					400,
					291,
					393,
					536,
					577,
					767,
					884,
					341,
					51157
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 269,
				"seek": 14119,
				"start": 847.74225,
				"end": 849.24225,
				"text": " can actually backfire in certain ways",
				"tokens": [
					51157,
					393,
					767,
					646,
					12037,
					294,
					1629,
					2098,
					51232
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 270,
				"seek": 14119,
				"start": 849.24225,
				"end": 851.9623,
				"text": " if you're not careful about how you're naming your tools.",
				"tokens": [
					51232,
					498,
					291,
					434,
					406,
					5026,
					466,
					577,
					291,
					434,
					25290,
					428,
					3873,
					13,
					51368
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 271,
				"seek": 14119,
				"start": 853.7023,
				"end": 855.98224,
				"text": " So if you're going to use this technique,",
				"tokens": [
					51455,
					407,
					498,
					291,
					434,
					516,
					281,
					764,
					341,
					6532,
					11,
					51569
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 272,
				"seek": 14119,
				"start": 856.2023,
				"end": 857.74225,
				"text": " it is important to be able to understand",
				"tokens": [
					51580,
					309,
					307,
					1021,
					281,
					312,
					1075,
					281,
					1223,
					51657
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 273,
				"seek": 14119,
				"start": 857.74225,
				"end": 858.9022,
				"text": " how these models work",
				"tokens": [
					51657,
					577,
					613,
					5245,
					589,
					51715
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 274,
				"seek": 14119,
				"start": 858.9022,
				"end": 861.18225,
				"text": " and build an intuition for this on your own.",
				"tokens": [
					51715,
					293,
					1322,
					364,
					24002,
					337,
					341,
					322,
					428,
					1065,
					13,
					51829
				],
				"temperature": 0,
				"avg_logprob": -0.1598515,
				"compression_ratio": 1.8013245,
				"no_speech_prob": 1.1202892e-12
			},
			{
				"id": 275,
				"seek": 17047,
				"start": 861.18225,
				"end": 865.86224,
				"text": " So then when something fails or doesn't fail, you can go iterate on this and make it better.",
				"tokens": [
					50365,
					407,
					550,
					562,
					746,
					18199,
					420,
					1177,
					380,
					3061,
					11,
					291,
					393,
					352,
					44497,
					322,
					341,
					293,
					652,
					309,
					1101,
					13,
					50599
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 276,
				"seek": 17047,
				"start": 872.32227,
				"end": 874.54224,
				"text": " Does that, any questions?",
				"tokens": [
					50922,
					4402,
					300,
					11,
					604,
					1651,
					30,
					51033
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 277,
				"seek": 17047,
				"start": 874.56226,
				"end": 875.60223,
				"text": " Yeah, other questions about that?",
				"tokens": [
					51034,
					865,
					11,
					661,
					1651,
					466,
					300,
					30,
					51086
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 278,
				"seek": 17047,
				"start": 875.7023,
				"end": 877.24225,
				"text": " There was a lot of content in there.",
				"tokens": [
					51091,
					821,
					390,
					257,
					688,
					295,
					2701,
					294,
					456,
					13,
					51168
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 279,
				"seek": 17047,
				"start": 878.00226,
				"end": 879.54224,
				"text": " Otherwise, we're going to move to some of these questions.",
				"tokens": [
					51206,
					10328,
					11,
					321,
					434,
					516,
					281,
					1286,
					281,
					512,
					295,
					613,
					1651,
					13,
					51283
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 280,
				"seek": 17047,
				"start": 882.5222,
				"end": 885.30225,
				"text": " Call me versus call me variable name.",
				"tokens": [
					51432,
					7807,
					385,
					5717,
					818,
					385,
					7006,
					1315,
					13,
					51571
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 281,
				"seek": 17047,
				"start": 885.4022,
				"end": 887.10223,
				"text": " No, that's totally arbitrary, I think.",
				"tokens": [
					51576,
					883,
					11,
					300,
					311,
					3879,
					23211,
					11,
					286,
					519,
					13,
					51661
				],
				"temperature": 0,
				"avg_logprob": -0.22629511,
				"compression_ratio": 1.5330188,
				"no_speech_prob": 1.4840577e-12
			},
			{
				"id": 282,
				"seek": 19639,
				"start": 887.10223,
				"end": 894.2222,
				"text": " I want to show you like a really fun post by someone.",
				"tokens": [
					50365,
					286,
					528,
					281,
					855,
					291,
					411,
					257,
					534,
					1019,
					2183,
					538,
					1580,
					13,
					50721
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 283,
				"seek": 19639,
				"start": 897.56226,
				"end": 899.2822,
				"text": " Oh, that's a long thing.",
				"tokens": [
					50888,
					876,
					11,
					300,
					311,
					257,
					938,
					551,
					13,
					50974
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 284,
				"seek": 19639,
				"start": 899.44226,
				"end": 900.7023,
				"text": " Let's not show my DMs.",
				"tokens": [
					50982,
					961,
					311,
					406,
					855,
					452,
					15322,
					82,
					13,
					51045
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 285,
				"seek": 19639,
				"start": 903.18225,
				"end": 907.24225,
				"text": " While you're doing that, Dex Horthy, with the understanding of tool calling can be either",
				"tokens": [
					51169,
					3987,
					291,
					434,
					884,
					300,
					11,
					1346,
					87,
					389,
					2652,
					88,
					11,
					365,
					264,
					3701,
					295,
					2290,
					5141,
					393,
					312,
					2139,
					51372
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 286,
				"seek": 19639,
				"start": 907.24225,
				"end": 909.7222,
				"text": " an explicit tool call or structured output, pick your flavor.",
				"tokens": [
					51372,
					364,
					13691,
					2290,
					818,
					420,
					18519,
					5598,
					11,
					1888,
					428,
					6813,
					13,
					51496
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 287,
				"seek": 19639,
				"start": 910.2023,
				"end": 914.7822,
				"text": " In the work that you are doing, have you noticed any difference in agent performance when using",
				"tokens": [
					51520,
					682,
					264,
					589,
					300,
					291,
					366,
					884,
					11,
					362,
					291,
					5694,
					604,
					2649,
					294,
					9461,
					3389,
					562,
					1228,
					51749
				],
				"temperature": 0,
				"avg_logprob": -0.21148911,
				"compression_ratio": 1.5173913,
				"no_speech_prob": 1.748614e-12
			},
			{
				"id": 288,
				"seek": 22407,
				"start": 914.7822,
				"end": 919.16223,
				"text": " model provider tool calling versus BAML structured output, basically home roll tool calling.",
				"tokens": [
					50365,
					2316,
					12398,
					2290,
					5141,
					5717,
					363,
					2865,
					43,
					18519,
					5598,
					11,
					1936,
					1280,
					3373,
					2290,
					5141,
					13,
					50584
				],
				"temperature": 0,
				"avg_logprob": -0.15182608,
				"compression_ratio": 1.71875,
				"no_speech_prob": 2.0684463e-12
			},
			{
				"id": 289,
				"seek": 22407,
				"start": 921.06226,
				"end": 927.7222,
				"text": " I have no stake in this, but I believe there's a lot of agents out there that are built with",
				"tokens": [
					50679,
					286,
					362,
					572,
					10407,
					294,
					341,
					11,
					457,
					286,
					1697,
					456,
					311,
					257,
					688,
					295,
					12554,
					484,
					456,
					300,
					366,
					3094,
					365,
					51012
				],
				"temperature": 0,
				"avg_logprob": -0.15182608,
				"compression_ratio": 1.71875,
				"no_speech_prob": 2.0684463e-12
			},
			{
				"id": 290,
				"seek": 22407,
				"start": 927.7222,
				"end": 933.66223,
				"text": " native tool calling that would be better if they use the BAML structured output for some of the",
				"tokens": [
					51012,
					8470,
					2290,
					5141,
					300,
					576,
					312,
					1101,
					498,
					436,
					764,
					264,
					363,
					2865,
					43,
					18519,
					5598,
					337,
					512,
					295,
					264,
					51309
				],
				"temperature": 0,
				"avg_logprob": -0.15182608,
				"compression_ratio": 1.71875,
				"no_speech_prob": 2.0684463e-12
			},
			{
				"id": 291,
				"seek": 22407,
				"start": 933.66223,
				"end": 939.7222,
				"text": " reasons we've kind of touched on here, but other things. But I need VibeAV to ship the benchmark",
				"tokens": [
					51309,
					4112,
					321,
					600,
					733,
					295,
					9828,
					322,
					510,
					11,
					457,
					661,
					721,
					13,
					583,
					286,
					643,
					6626,
					650,
					32,
					53,
					281,
					5374,
					264,
					18927,
					51612
				],
				"temperature": 0,
				"avg_logprob": -0.15182608,
				"compression_ratio": 1.71875,
				"no_speech_prob": 2.0684463e-12
			},
			{
				"id": 292,
				"seek": 22407,
				"start": 939.7222,
				"end": 943.10223,
				"text": " again. When are we getting V2 of your tool calling benchmark?",
				"tokens": [
					51612,
					797,
					13,
					1133,
					366,
					321,
					1242,
					691,
					17,
					295,
					428,
					2290,
					5141,
					18927,
					30,
					51781
				],
				"temperature": 0,
				"avg_logprob": -0.15182608,
				"compression_ratio": 1.71875,
				"no_speech_prob": 2.0684463e-12
			},
			{
				"id": 293,
				"seek": 25239,
				"start": 943.10223,
				"end": 945.94226,
				"text": " Sean showed a small benchmark already that showed.",
				"tokens": [
					50365,
					14839,
					4712,
					257,
					1359,
					18927,
					1217,
					300,
					4712,
					13,
					50507
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 294,
				"seek": 25239,
				"start": 946.06226,
				"end": 946.4022,
				"text": " That's true.",
				"tokens": [
					50513,
					663,
					311,
					2074,
					13,
					50530
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 295,
				"seek": 25239,
				"start": 946.82227,
				"end": 947.36224,
				"text": " Quickly better.",
				"tokens": [
					50551,
					31800,
					1101,
					13,
					50578
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 296,
				"seek": 25239,
				"start": 947.9623,
				"end": 950.4022,
				"text": " But I want to show everyone an accurate example of where this matters.",
				"tokens": [
					50608,
					583,
					286,
					528,
					281,
					855,
					1518,
					364,
					8559,
					1365,
					295,
					689,
					341,
					7001,
					13,
					50730
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 297,
				"seek": 25239,
				"start": 950.50226,
				"end": 953.86224,
				"text": " So like in this case, the user was doing some sort of tool calling with Kimi K2 model.",
				"tokens": [
					50735,
					407,
					411,
					294,
					341,
					1389,
					11,
					264,
					4195,
					390,
					884,
					512,
					1333,
					295,
					2290,
					5141,
					365,
					5652,
					72,
					591,
					17,
					2316,
					13,
					50903
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 298,
				"seek": 25239,
				"start": 954.32227,
				"end": 956.04224,
				"text": " And they asked the model to output approach.",
				"tokens": [
					50926,
					400,
					436,
					2351,
					264,
					2316,
					281,
					5598,
					3109,
					13,
					51012
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 299,
				"seek": 25239,
				"start": 956.5222,
				"end": 961.80225,
				"text": " And no matter what they did, like around like 2% of the time, Kimi K2 would literally just pull out a pro brace.",
				"tokens": [
					51036,
					400,
					572,
					1871,
					437,
					436,
					630,
					11,
					411,
					926,
					411,
					568,
					4,
					295,
					264,
					565,
					11,
					5652,
					72,
					591,
					17,
					576,
					3736,
					445,
					2235,
					484,
					257,
					447,
					38458,
					13,
					51300
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 300,
				"seek": 25239,
				"start": 963.54224,
				"end": 966.2822,
				"text": " And this is so wild that it would actually do this.",
				"tokens": [
					51387,
					400,
					341,
					307,
					370,
					4868,
					300,
					309,
					576,
					767,
					360,
					341,
					13,
					51524
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 301,
				"seek": 25239,
				"start": 966.4623,
				"end": 971.7223,
				"text": " And I started looking into this and eventually I was like, okay, well, there's prompt engineering techniques that you can do to go do this.",
				"tokens": [
					51533,
					400,
					286,
					1409,
					1237,
					666,
					341,
					293,
					4728,
					286,
					390,
					411,
					11,
					1392,
					11,
					731,
					11,
					456,
					311,
					12391,
					7043,
					7512,
					300,
					291,
					393,
					360,
					281,
					352,
					360,
					341,
					13,
					51796
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 302,
				"seek": 25239,
				"start": 972.00226,
				"end": 972.88226,
				"text": " So they did that.",
				"tokens": [
					51810,
					407,
					436,
					630,
					300,
					13,
					51854
				],
				"temperature": 0,
				"avg_logprob": -0.2149388,
				"compression_ratio": 1.7385057,
				"no_speech_prob": 1.4609413e-12
			},
			{
				"id": 303,
				"seek": 28239,
				"start": 973.1023,
				"end": 974.5823,
				"text": " and eventually",
				"tokens": [
					50365,
					293,
					4728,
					50439
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 304,
				"seek": 28239,
				"start": 974.5823,
				"end": 976.2223,
				"text": " did they just add an alias to the field?",
				"tokens": [
					50439,
					630,
					436,
					445,
					909,
					364,
					419,
					4609,
					281,
					264,
					2519,
					30,
					50521
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 305,
				"seek": 28239,
				"start": 976.88226,
				"end": 978.4623,
				"text": " well eventually what I did was",
				"tokens": [
					50554,
					731,
					4728,
					437,
					286,
					630,
					390,
					50633
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 306,
				"seek": 28239,
				"start": 978.4623,
				"end": 980.24225,
				"text": " I just wanted to go understand this",
				"tokens": [
					50633,
					286,
					445,
					1415,
					281,
					352,
					1223,
					341,
					50722
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 307,
				"seek": 28239,
				"start": 980.24225,
				"end": 982.2023,
				"text": " I was like so what did I do? I dumped out the tokenizer",
				"tokens": [
					50722,
					286,
					390,
					411,
					370,
					437,
					630,
					286,
					360,
					30,
					286,
					32131,
					484,
					264,
					14862,
					6545,
					50820
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 308,
				"seek": 28239,
				"start": 982.2023,
				"end": 984.80225,
				"text": " I literally took the tokenizer for the K2 model",
				"tokens": [
					50820,
					286,
					3736,
					1890,
					264,
					14862,
					6545,
					337,
					264,
					591,
					17,
					2316,
					50950
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 309,
				"seek": 28239,
				"start": 984.80225,
				"end": 985.5823,
				"text": " and I dumped it out",
				"tokens": [
					50950,
					293,
					286,
					32131,
					309,
					484,
					50989
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 310,
				"seek": 28239,
				"start": 985.5823,
				"end": 989.2223,
				"text": " and lo and behold approach is two tokens",
				"tokens": [
					50989,
					293,
					450,
					293,
					27234,
					3109,
					307,
					732,
					22667,
					51171
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 311,
				"seek": 28239,
				"start": 989.2223,
				"end": 992.7622,
				"text": " so of course the model that's a dumber model",
				"tokens": [
					51171,
					370,
					295,
					1164,
					264,
					2316,
					300,
					311,
					257,
					274,
					4182,
					2316,
					51348
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 312,
				"seek": 28239,
				"start": 992.7622,
				"end": 993.7622,
				"text": " is going to get this wrong",
				"tokens": [
					51348,
					307,
					516,
					281,
					483,
					341,
					2085,
					51398
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 313,
				"seek": 28239,
				"start": 993.7622,
				"end": 996.66223,
				"text": " because at this point it wants to do approach",
				"tokens": [
					51398,
					570,
					412,
					341,
					935,
					309,
					2738,
					281,
					360,
					3109,
					51543
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 314,
				"seek": 28239,
				"start": 996.66223,
				"end": 999.2622,
				"text": " and then it's like it's just too dumb to actually do this",
				"tokens": [
					51543,
					293,
					550,
					309,
					311,
					411,
					309,
					311,
					445,
					886,
					10316,
					281,
					767,
					360,
					341,
					51673
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 315,
				"seek": 28239,
				"start": 999.2622,
				"end": 1001.9022,
				"text": " so the solution this user had was",
				"tokens": [
					51673,
					370,
					264,
					3827,
					341,
					4195,
					632,
					390,
					51805
				],
				"temperature": 0,
				"avg_logprob": -0.13491684,
				"compression_ratio": 1.8897338,
				"no_speech_prob": 1.5250804e-12
			},
			{
				"id": 316,
				"seek": 31119,
				"start": 1001.9022,
				"end": 1004.62225,
				"text": " they actually found a word that was a single token and now it works.",
				"tokens": [
					50365,
					436,
					767,
					1352,
					257,
					1349,
					300,
					390,
					257,
					2167,
					14862,
					293,
					586,
					309,
					1985,
					13,
					50501
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 317,
				"seek": 31119,
				"start": 1006.04224,
				"end": 1007.0222,
				"text": " That's so sick.",
				"tokens": [
					50572,
					663,
					311,
					370,
					4998,
					13,
					50621
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 318,
				"seek": 31119,
				"start": 1007.7223,
				"end": 1009.42224,
				"text": " And just like, but the point is like,",
				"tokens": [
					50656,
					400,
					445,
					411,
					11,
					457,
					264,
					935,
					307,
					411,
					11,
					50741
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 319,
				"seek": 31119,
				"start": 1009.44226,
				"end": 1011.36224,
				"text": " if your tool calling is going to use similar technique,",
				"tokens": [
					50742,
					498,
					428,
					2290,
					5141,
					307,
					516,
					281,
					764,
					2531,
					6532,
					11,
					50838
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 320,
				"seek": 31119,
				"start": 1011.5222,
				"end": 1015.56226,
				"text": " you have to understand that there are trade-offs to how your tokenizer",
				"tokens": [
					50846,
					291,
					362,
					281,
					1223,
					300,
					456,
					366,
					4923,
					12,
					19231,
					281,
					577,
					428,
					14862,
					6545,
					51048
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 321,
				"seek": 31119,
				"start": 1015.56226,
				"end": 1018.38226,
				"text": " vocabulary works. And the bigger the model,",
				"tokens": [
					51048,
					19864,
					1985,
					13,
					400,
					264,
					3801,
					264,
					2316,
					11,
					51189
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 322,
				"seek": 31119,
				"start": 1018.5222,
				"end": 1020.6422,
				"text": " the shorter your context window, the less they matter.",
				"tokens": [
					51196,
					264,
					11639,
					428,
					4319,
					4910,
					11,
					264,
					1570,
					436,
					1871,
					13,
					51302
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 323,
				"seek": 31119,
				"start": 1021.12225,
				"end": 1023.48224,
				"text": " The smaller the model, the longer your context window,",
				"tokens": [
					51326,
					440,
					4356,
					264,
					2316,
					11,
					264,
					2854,
					428,
					4319,
					4910,
					11,
					51444
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 324,
				"seek": 31119,
				"start": 1023.62225,
				"end": 1024.3823,
				"text": " the more they matter.",
				"tokens": [
					51451,
					264,
					544,
					436,
					1871,
					13,
					51489
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 325,
				"seek": 31119,
				"start": 1024.8423,
				"end": 1027.6023,
				"text": " And knowing that is just key to getting success over here.",
				"tokens": [
					51512,
					400,
					5276,
					300,
					307,
					445,
					2141,
					281,
					1242,
					2245,
					670,
					510,
					13,
					51650
				],
				"temperature": 0,
				"avg_logprob": -0.16896418,
				"compression_ratio": 1.7859778,
				"no_speech_prob": 1.6814916e-12
			},
			{
				"id": 326,
				"seek": 34119,
				"start": 1031.9022,
				"end": 1034.0623,
				"text": " Love it.",
				"tokens": [
					50365,
					5956,
					309,
					13,
					50473
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 327,
				"seek": 34119,
				"start": 1034.0623,
				"end": 1034.8423,
				"text": " Yeah.",
				"tokens": [
					50473,
					865,
					13,
					50512
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 328,
				"seek": 34119,
				"start": 1034.8423,
				"end": 1037.5823,
				"text": " And remember, tokenizers are different for every model.",
				"tokens": [
					50512,
					400,
					1604,
					11,
					14862,
					22525,
					366,
					819,
					337,
					633,
					2316,
					13,
					50649
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 329,
				"seek": 34119,
				"start": 1037.5823,
				"end": 1041.2223,
				"text": " So you have to like, this user could have spent forever",
				"tokens": [
					50649,
					407,
					291,
					362,
					281,
					411,
					11,
					341,
					4195,
					727,
					362,
					4418,
					5680,
					50831
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 330,
				"seek": 34119,
				"start": 1041.2223,
				"end": 1043.2822,
				"text": " trying to prompt engineer the way the heck out of this,",
				"tokens": [
					50831,
					1382,
					281,
					12391,
					11403,
					264,
					636,
					264,
					12872,
					484,
					295,
					341,
					11,
					50934
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 331,
				"seek": 34119,
				"start": 1043.2822,
				"end": 1045.4423,
				"text": " trying to make it work, and they did.",
				"tokens": [
					50934,
					1382,
					281,
					652,
					309,
					589,
					11,
					293,
					436,
					630,
					13,
					51042
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 332,
				"seek": 34119,
				"start": 1045.4423,
				"end": 1046.9822,
				"text": " And I think Sam and our team tried this too.",
				"tokens": [
					51042,
					400,
					286,
					519,
					4832,
					293,
					527,
					1469,
					3031,
					341,
					886,
					13,
					51119
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 333,
				"seek": 34119,
				"start": 1046.9822,
				"end": 1048.3223,
				"text": " And I tried a couple techniques.",
				"tokens": [
					51119,
					400,
					286,
					3031,
					257,
					1916,
					7512,
					13,
					51186
				],
				"temperature": 0,
				"avg_logprob": -0.2369961,
				"compression_ratio": 1.7391304,
				"no_speech_prob": 1.2398583e-12
			},
			{
				"id": 334,
				"seek": 1049,
				"start": 1048.3223,
				"end": 1057.3035,
				"text": " And literally at some point it like the model is just too stupid what going on And I had to go and dig deep So don be afraid to go do that Just literally run the tokenizer code and just go see what happening",
				"tokens": [
					51186,
					400,
					3736,
					11,
					412,
					512,
					935,
					11,
					309,
					311,
					411,
					264,
					2316,
					51254,
					51254,
					307,
					445,
					886,
					6631,
					437,
					311,
					516,
					322,
					13,
					51339,
					51339,
					400,
					286,
					632,
					281,
					352,
					293,
					2528,
					2452,
					13,
					51424,
					50561,
					407,
					500,
					380,
					312,
					4638,
					281,
					352,
					360,
					300,
					13,
					50637,
					50640,
					1449,
					3736,
					1190,
					264,
					14862,
					6545,
					3089,
					50731,
					50731,
					293,
					445,
					352,
					536,
					437,
					311,
					2737,
					13,
					50779
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 335,
				"seek": 1049,
				"start": 1059.0835,
				"end": 1061.0835,
				"text": " Really, really easy hack to go make this work.",
				"tokens": [
					50868,
					4083,
					11,
					534,
					1858,
					10339,
					281,
					352,
					652,
					341,
					589,
					13,
					50968
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 336,
				"seek": 1049,
				"start": 1061.2035,
				"end": 1062.9634,
				"text": " Any model provider that doesn't give you the tokenizer,",
				"tokens": [
					50974,
					2639,
					2316,
					12398,
					300,
					1177,
					380,
					976,
					291,
					264,
					14862,
					6545,
					11,
					51062
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 337,
				"seek": 1049,
				"start": 1063.0635,
				"end": 1063.6434,
				"text": " it's kind of annoying.",
				"tokens": [
					51067,
					309,
					311,
					733,
					295,
					11304,
					13,
					51096
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 338,
				"seek": 1049,
				"start": 1064.8834,
				"end": 1067.6234,
				"text": " Just have the model provider spit back out the word to you",
				"tokens": [
					51158,
					1449,
					362,
					264,
					2316,
					12398,
					22127,
					646,
					484,
					264,
					1349,
					281,
					291,
					51295
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 339,
				"seek": 1049,
				"start": 1067.6234,
				"end": 1069.5835,
				"text": " and count the tokens out of the HTTP request.",
				"tokens": [
					51295,
					293,
					1207,
					264,
					22667,
					484,
					295,
					264,
					33283,
					5308,
					13,
					51393
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 340,
				"seek": 1049,
				"start": 1070.0435,
				"end": 1072.0435,
				"text": " That's often what I'll do with like anthropic models",
				"tokens": [
					51416,
					663,
					311,
					2049,
					437,
					286,
					603,
					360,
					365,
					411,
					22727,
					299,
					5245,
					51516
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 341,
				"seek": 1049,
				"start": 1072.0435,
				"end": 1073.3434,
				"text": " because they don't actually give me the tokenizer.",
				"tokens": [
					51516,
					570,
					436,
					500,
					380,
					767,
					976,
					385,
					264,
					14862,
					6545,
					13,
					51581
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 342,
				"seek": 1049,
				"start": 1073.7434,
				"end": 1074.1835,
				"text": " So I'll say-",
				"tokens": [
					51601,
					407,
					286,
					603,
					584,
					12,
					51623
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 343,
				"seek": 1049,
				"start": 1074.1835,
				"end": 1076.6034,
				"text": " Oh, you say, hey, just say the word approach to me.",
				"tokens": [
					51623,
					876,
					11,
					291,
					584,
					11,
					4177,
					11,
					445,
					584,
					264,
					1349,
					3109,
					281,
					385,
					13,
					51744
				],
				"temperature": 0,
				"avg_logprob": -0.13890877,
				"compression_ratio": 1.7784431,
				"no_speech_prob": 3.0566543e-12
			},
			{
				"id": 344,
				"seek": 3807,
				"start": 1076.6034,
				"end": 1078.0635,
				"text": " and then you look at the token counts",
				"tokens": [
					50365,
					293,
					550,
					291,
					574,
					412,
					264,
					14862,
					14893,
					50438
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 345,
				"seek": 3807,
				"start": 1078.0635,
				"end": 1079.7234,
				"text": " in the raw JSON that comes back",
				"tokens": [
					50438,
					294,
					264,
					8936,
					31828,
					300,
					1487,
					646,
					50521
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 346,
				"seek": 3807,
				"start": 1079.7234,
				"end": 1080.9634,
				"text": " and say, how many tokens was that?",
				"tokens": [
					50521,
					293,
					584,
					11,
					577,
					867,
					22667,
					390,
					300,
					30,
					50583
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 347,
				"seek": 3807,
				"start": 1081.2834,
				"end": 1082.2035,
				"text": " Literally what I do.",
				"tokens": [
					50599,
					23768,
					437,
					286,
					360,
					13,
					50645
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 348,
				"seek": 3807,
				"start": 1082.9435,
				"end": 1083.3434,
				"text": " Okay.",
				"tokens": [
					50682,
					1033,
					13,
					50702
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 349,
				"seek": 3807,
				"start": 1084.0234,
				"end": 1085.0635,
				"text": " And it kind of works.",
				"tokens": [
					50736,
					400,
					309,
					733,
					295,
					1985,
					13,
					50788
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 350,
				"seek": 3807,
				"start": 1088.7434,
				"end": 1090.6434,
				"text": " Yeah, if you're curious about the benchmarks,",
				"tokens": [
					50972,
					865,
					11,
					498,
					291,
					434,
					6369,
					466,
					264,
					43751,
					11,
					51067
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 351,
				"seek": 3807,
				"start": 1090.9235,
				"end": 1092.6835,
				"text": " I can share them afterwards",
				"tokens": [
					51081,
					286,
					393,
					2073,
					552,
					10543,
					51169
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 352,
				"seek": 3807,
				"start": 1092.6835,
				"end": 1094.2634,
				"text": " on the email that we send out",
				"tokens": [
					51169,
					322,
					264,
					3796,
					300,
					321,
					2845,
					484,
					51248
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 353,
				"seek": 3807,
				"start": 1094.2634,
				"end": 1095.6034,
				"text": " if you're interested",
				"tokens": [
					51248,
					498,
					291,
					434,
					3102,
					51315
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 354,
				"seek": 3807,
				"start": 1095.6034,
				"end": 1096.7634,
				"text": " or in our Discord,",
				"tokens": [
					51315,
					420,
					294,
					527,
					32623,
					11,
					51373
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 355,
				"seek": 3807,
				"start": 1097.1434,
				"end": 1098.1835,
				"text": " if you ask, we'll have Discord.",
				"tokens": [
					51392,
					498,
					291,
					1029,
					11,
					321,
					603,
					362,
					32623,
					13,
					51444
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 356,
				"seek": 3807,
				"start": 1099.2434,
				"end": 1100.9034,
				"text": " Slava's question real quick.",
				"tokens": [
					51497,
					6187,
					4061,
					311,
					1168,
					957,
					1702,
					13,
					51580
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 357,
				"seek": 3807,
				"start": 1101.0234,
				"end": 1103.2035,
				"text": " Yeah, we put all the streams in the GitHub repo.",
				"tokens": [
					51586,
					865,
					11,
					321,
					829,
					439,
					264,
					15842,
					294,
					264,
					23331,
					49040,
					13,
					51695
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 358,
				"seek": 3807,
				"start": 1103.3634,
				"end": 1104.5234,
				"text": " That's where you can find them.",
				"tokens": [
					51703,
					663,
					311,
					689,
					291,
					393,
					915,
					552,
					13,
					51761
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 359,
				"seek": 3807,
				"start": 1104.9235,
				"end": 1105.6835,
				"text": " Yeah, do you want to pull it up?",
				"tokens": [
					51781,
					865,
					11,
					360,
					291,
					528,
					281,
					2235,
					309,
					493,
					30,
					51819
				],
				"temperature": 0,
				"avg_logprob": -0.2219148,
				"compression_ratio": 1.6445993,
				"no_speech_prob": 1.6816863e-12
			},
			{
				"id": 360,
				"seek": 6807,
				"start": 1106.6034,
				"end": 1115.0034,
				"text": " uh there it is so you can come here you can see every every recording we've done since march",
				"tokens": [
					50365,
					2232,
					456,
					309,
					307,
					370,
					291,
					393,
					808,
					510,
					291,
					393,
					536,
					633,
					633,
					6613,
					321,
					600,
					1096,
					1670,
					8368,
					50785
				],
				"temperature": 0,
				"avg_logprob": -0.12584256,
				"compression_ratio": 1.8502203,
				"no_speech_prob": 4.464848e-12
			},
			{
				"id": 361,
				"seek": 6807,
				"start": 1115.0034,
				"end": 1119.9834,
				"text": " you can see the youtube you can see the code um you can sign up for the next one and if all you",
				"tokens": [
					50785,
					291,
					393,
					536,
					264,
					12487,
					291,
					393,
					536,
					264,
					3089,
					1105,
					291,
					393,
					1465,
					493,
					337,
					264,
					958,
					472,
					293,
					498,
					439,
					291,
					51034
				],
				"temperature": 0,
				"avg_logprob": -0.12584256,
				"compression_ratio": 1.8502203,
				"no_speech_prob": 4.464848e-12
			},
			{
				"id": 362,
				"seek": 6807,
				"start": 1119.9834,
				"end": 1123.8434,
				"text": " really know about dg if all you really want to do is just watch all the content we have",
				"tokens": [
					51034,
					534,
					458,
					466,
					274,
					70,
					498,
					439,
					291,
					534,
					528,
					281,
					360,
					307,
					445,
					1159,
					439,
					264,
					2701,
					321,
					362,
					51227
				],
				"temperature": 0,
				"avg_logprob": -0.12584256,
				"compression_ratio": 1.8502203,
				"no_speech_prob": 4.464848e-12
			},
			{
				"id": 363,
				"seek": 6807,
				"start": 1123.8434,
				"end": 1128.4834,
				"text": " a playlist somewhere um that you can find and i don't hear my voice again",
				"tokens": [
					51227,
					257,
					16788,
					4079,
					1105,
					300,
					291,
					393,
					915,
					293,
					741,
					500,
					380,
					1568,
					452,
					3177,
					797,
					51459
				],
				"temperature": 0,
				"avg_logprob": -0.12584256,
				"compression_ratio": 1.8502203,
				"no_speech_prob": 4.464848e-12
			},
			{
				"id": 364,
				"seek": 6807,
				"start": 1128.4834,
				"end": 1133.8235,
				"text": " but i'll have everything on there and then i think we tried to make a",
				"tokens": [
					51459,
					457,
					741,
					603,
					362,
					1203,
					322,
					456,
					293,
					550,
					741,
					519,
					321,
					3031,
					281,
					652,
					257,
					51726
				],
				"temperature": 0,
				"avg_logprob": -0.12584256,
				"compression_ratio": 1.8502203,
				"no_speech_prob": 4.464848e-12
			},
			{
				"id": 365,
				"seek": 9529,
				"start": 1133.8235,
				"end": 1135.9034,
				"text": " interesting version of this",
				"tokens": [
					50365,
					1880,
					3037,
					295,
					341,
					50469
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 366,
				"seek": 9529,
				"start": 1135.9034,
				"end": 1138.2234,
				"text": " where we actually",
				"tokens": [
					50469,
					689,
					321,
					767,
					50585
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 367,
				"seek": 9529,
				"start": 1138.2234,
				"end": 1140.2634,
				"text": " do this with Dexter and you can actually scroll through here as well.",
				"tokens": [
					50585,
					360,
					341,
					365,
					1346,
					36671,
					293,
					291,
					393,
					767,
					11369,
					807,
					510,
					382,
					731,
					13,
					50687
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 368,
				"seek": 9529,
				"start": 1140.6234,
				"end": 1142.2634,
				"text": " Oh, man. Wait, this",
				"tokens": [
					50705,
					876,
					11,
					587,
					13,
					3802,
					11,
					341,
					50787
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 369,
				"seek": 9529,
				"start": 1142.2634,
				"end": 1144.2234,
				"text": " is sick. Wait, I want",
				"tokens": [
					50787,
					307,
					4998,
					13,
					3802,
					11,
					286,
					528,
					50885
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 370,
				"seek": 9529,
				"start": 1144.2234,
				"end": 1146.2634,
				"text": " this on humanlayer.dev too. I'll send you",
				"tokens": [
					50885,
					341,
					322,
					1952,
					8376,
					260,
					13,
					40343,
					886,
					13,
					286,
					603,
					2845,
					291,
					50987
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 371,
				"seek": 9529,
				"start": 1146.2634,
				"end": 1147.4034,
				"text": " the code. You can post it over.",
				"tokens": [
					50987,
					264,
					3089,
					13,
					509,
					393,
					2183,
					309,
					670,
					13,
					51044
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 372,
				"seek": 9529,
				"start": 1148.2434,
				"end": 1148.6034,
				"text": " Amazing.",
				"tokens": [
					51086,
					14165,
					13,
					51104
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 373,
				"seek": 9529,
				"start": 1150.6034,
				"end": 1152.1234,
				"text": " Do you much, and then point",
				"tokens": [
					51204,
					1144,
					291,
					709,
					11,
					293,
					550,
					935,
					51280
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 374,
				"seek": 9529,
				"start": 1152.1234,
				"end": 1154.1234,
				"text": " the resource, try this out locally. The thing about",
				"tokens": [
					51280,
					264,
					7684,
					11,
					853,
					341,
					484,
					16143,
					13,
					440,
					551,
					466,
					51380
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 375,
				"seek": 9529,
				"start": 1154.1234,
				"end": 1156.2035,
				"text": " we'll try and give you some sample code to go",
				"tokens": [
					51380,
					321,
					603,
					853,
					293,
					976,
					291,
					512,
					6889,
					3089,
					281,
					352,
					51484
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 376,
				"seek": 9529,
				"start": 1156.2035,
				"end": 1158.2434,
				"text": " try this out locally, but honestly, for most",
				"tokens": [
					51484,
					853,
					341,
					484,
					16143,
					11,
					457,
					6095,
					11,
					337,
					881,
					51586
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 377,
				"seek": 9529,
				"start": 1158.2434,
				"end": 1160.2035,
				"text": " of these problems, just take any of the problems that you have",
				"tokens": [
					51586,
					295,
					613,
					2740,
					11,
					445,
					747,
					604,
					295,
					264,
					2740,
					300,
					291,
					362,
					51684
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 378,
				"seek": 9529,
				"start": 1160.2035,
				"end": 1161.5234,
				"text": " that you've already been working on",
				"tokens": [
					51684,
					300,
					291,
					600,
					1217,
					668,
					1364,
					322,
					51750
				],
				"temperature": 0,
				"avg_logprob": -0.17181912,
				"compression_ratio": 1.7346939,
				"no_speech_prob": 2.47566e-12
			},
			{
				"id": 379,
				"seek": 12299,
				"start": 1161.5234,
				"end": 1164.8834,
				"text": " and just like go look at the inspect the output elements",
				"tokens": [
					50365,
					293,
					445,
					411,
					352,
					574,
					412,
					264,
					15018,
					264,
					5598,
					4959,
					50533
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 380,
				"seek": 12299,
				"start": 1164.8834,
				"end": 1166.0234,
				"text": " that are coming out of these problems.",
				"tokens": [
					50533,
					300,
					366,
					1348,
					484,
					295,
					613,
					2740,
					13,
					50590
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 381,
				"seek": 12299,
				"start": 1166.5635,
				"end": 1167.3434,
				"text": " So like for example,",
				"tokens": [
					50617,
					407,
					411,
					337,
					1365,
					11,
					50656
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 382,
				"seek": 12299,
				"start": 1168.5234,
				"end": 1173.0234,
				"text": " let's just write a really quick agent.",
				"tokens": [
					50715,
					718,
					311,
					445,
					2464,
					257,
					534,
					1702,
					9461,
					13,
					50940
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 383,
				"seek": 12299,
				"start": 1174.0635,
				"end": 1175.1034,
				"text": " Let's see if I have screen,",
				"tokens": [
					50992,
					961,
					311,
					536,
					498,
					286,
					362,
					2568,
					11,
					51044
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 384,
				"seek": 12299,
				"start": 1175.2035,
				"end": 1176.0635,
				"text": " if I don't have,",
				"tokens": [
					51049,
					498,
					286,
					500,
					380,
					362,
					11,
					51092
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 385,
				"seek": 12299,
				"start": 1177.2834,
				"end": 1180.1034,
				"text": " let's see if I don't have screen sharing.",
				"tokens": [
					51153,
					718,
					311,
					536,
					498,
					286,
					500,
					380,
					362,
					2568,
					5414,
					13,
					51294
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 386,
				"seek": 12299,
				"start": 1182.9435,
				"end": 1183.7834,
				"text": " Okay, cool.",
				"tokens": [
					51436,
					1033,
					11,
					1627,
					13,
					51478
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 387,
				"seek": 12299,
				"start": 1184.1835,
				"end": 1185.7834,
				"text": " I'm going to disable screen sharing really fast",
				"tokens": [
					51498,
					286,
					478,
					516,
					281,
					28362,
					2568,
					5414,
					534,
					2370,
					51578
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 388,
				"seek": 12299,
				"start": 1185.7834,
				"end": 1187.9634,
				"text": " while I copy the curl with my OpenAI key.",
				"tokens": [
					51578,
					1339,
					286,
					5055,
					264,
					22591,
					365,
					452,
					7238,
					48698,
					2141,
					13,
					51687
				],
				"temperature": 0,
				"avg_logprob": -0.28592366,
				"compression_ratio": 1.6074766,
				"no_speech_prob": 9.0679233e-13
			},
			{
				"id": 389,
				"seek": 14943,
				"start": 1187.9634,
				"end": 1191.9235,
				"text": " and then I will bring it back.",
				"tokens": [
					50365,
					293,
					550,
					286,
					486,
					1565,
					309,
					646,
					13,
					50563
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 390,
				"seek": 14943,
				"start": 1195.2034,
				"end": 1195.7634,
				"text": " Copy.",
				"tokens": [
					50727,
					25653,
					13,
					50755
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 391,
				"seek": 14943,
				"start": 1203.3834,
				"end": 1203.9034,
				"text": " Clear.",
				"tokens": [
					51136,
					14993,
					13,
					51162
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 392,
				"seek": 14943,
				"start": 1204.3834,
				"end": 1204.6234,
				"text": " Okay.",
				"tokens": [
					51186,
					1033,
					13,
					51198
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 393,
				"seek": 14943,
				"start": 1206.1234,
				"end": 1207.3235,
				"text": " I'm going to screen share again.",
				"tokens": [
					51273,
					286,
					478,
					516,
					281,
					2568,
					2073,
					797,
					13,
					51333
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 394,
				"seek": 14943,
				"start": 1209.1434,
				"end": 1213.0435,
				"text": " So if I run this API key, this request,",
				"tokens": [
					51424,
					407,
					498,
					286,
					1190,
					341,
					9362,
					2141,
					11,
					341,
					5308,
					11,
					51619
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 395,
				"seek": 14943,
				"start": 1213.4034,
				"end": 1215.8435,
				"text": " I can just see that it literally cached nothing.",
				"tokens": [
					51637,
					286,
					393,
					445,
					536,
					300,
					309,
					3736,
					269,
					15095,
					1825,
					13,
					51759
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 396,
				"seek": 14943,
				"start": 1216.1034,
				"end": 1217.2834,
				"text": " And the reason it didn't cache anything",
				"tokens": [
					51772,
					400,
					264,
					1778,
					309,
					994,
					380,
					19459,
					1340,
					51831
				],
				"temperature": 0,
				"avg_logprob": -0.32720393,
				"compression_ratio": 1.3525641,
				"no_speech_prob": 6.307145e-13
			},
			{
				"id": 397,
				"seek": 17875,
				"start": 1217.2834,
				"end": 1219.9834,
				"text": " is literally because I just have too small little context window.",
				"tokens": [
					50365,
					307,
					3736,
					570,
					286,
					445,
					362,
					886,
					1359,
					707,
					4319,
					4910,
					13,
					50500
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 398,
				"seek": 17875,
				"start": 1220.5034,
				"end": 1222.1635,
				"text": " If I just increase this context window",
				"tokens": [
					50526,
					759,
					286,
					445,
					3488,
					341,
					4319,
					4910,
					50609
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 399,
				"seek": 17875,
				"start": 1222.1635,
				"end": 1232.0234,
				"text": " by make this a real resume example that is dense.",
				"tokens": [
					50609,
					538,
					652,
					341,
					257,
					957,
					15358,
					1365,
					300,
					307,
					18011,
					13,
					51102
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 400,
				"seek": 17875,
				"start": 1236.7834,
				"end": 1238.1835,
				"text": " And I'll eventually do stuff.",
				"tokens": [
					51340,
					400,
					286,
					603,
					4728,
					360,
					1507,
					13,
					51410
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 401,
				"seek": 17875,
				"start": 1240.8834,
				"end": 1242.4834,
				"text": " Unlux all the stuff.",
				"tokens": [
					51545,
					1156,
					75,
					2449,
					439,
					264,
					1507,
					13,
					51625
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 402,
				"seek": 17875,
				"start": 1243.8634,
				"end": 1244.4235,
				"text": " Cool.",
				"tokens": [
					51694,
					8561,
					13,
					51722
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 403,
				"seek": 17875,
				"start": 1244.6234,
				"end": 1245.6034,
				"text": " Let's copy and paste this.",
				"tokens": [
					51732,
					961,
					311,
					5055,
					293,
					9163,
					341,
					13,
					51781
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 404,
				"seek": 17875,
				"start": 1246.2234,
				"end": 1247.2234,
				"text": " And let's go run this again.",
				"tokens": [
					51812,
					400,
					718,
					311,
					352,
					1190,
					341,
					797,
					13,
					51862
				],
				"temperature": 0,
				"avg_logprob": -0.37412927,
				"compression_ratio": 1.5433526,
				"no_speech_prob": 4.2520517e-13
			},
			{
				"id": 405,
				"seek": 20875,
				"start": 1247.2834,
				"end": 1255.0234,
				"text": " it still didn't cache anything i have to make it even bigger but the whole point is right over here",
				"tokens": [
					50365,
					309,
					920,
					994,
					380,
					19459,
					1340,
					741,
					362,
					281,
					652,
					309,
					754,
					3801,
					457,
					264,
					1379,
					935,
					307,
					558,
					670,
					510,
					50752
				],
				"temperature": 0,
				"avg_logprob": -0.1469441,
				"compression_ratio": 1.726415,
				"no_speech_prob": 1.3406217e-12
			},
			{
				"id": 406,
				"seek": 20875,
				"start": 1255.0234,
				"end": 1258.3235,
				"text": " you need to go ahead and actually understand why this is happening or not happening and part of the",
				"tokens": [
					50752,
					291,
					643,
					281,
					352,
					2286,
					293,
					767,
					1223,
					983,
					341,
					307,
					2737,
					420,
					406,
					2737,
					293,
					644,
					295,
					264,
					50917
				],
				"temperature": 0,
				"avg_logprob": -0.1469441,
				"compression_ratio": 1.726415,
				"no_speech_prob": 1.3406217e-12
			},
			{
				"id": 407,
				"seek": 20875,
				"start": 1258.3235,
				"end": 1262.1034,
				"text": " reason here is just i know this is too smaller too uh too small of a text",
				"tokens": [
					50917,
					1778,
					510,
					307,
					445,
					741,
					458,
					341,
					307,
					886,
					4356,
					886,
					2232,
					886,
					1359,
					295,
					257,
					2487,
					51106
				],
				"temperature": 0,
				"avg_logprob": -0.1469441,
				"compression_ratio": 1.726415,
				"no_speech_prob": 1.3406217e-12
			},
			{
				"id": 408,
				"seek": 20875,
				"start": 1262.1034,
				"end": 1270.2634,
				"text": " do you show the token count it's right here uh oh i see",
				"tokens": [
					51106,
					360,
					291,
					855,
					264,
					14862,
					1207,
					309,
					311,
					558,
					510,
					2232,
					1954,
					741,
					536,
					51514
				],
				"temperature": 0,
				"avg_logprob": -0.1469441,
				"compression_ratio": 1.726415,
				"no_speech_prob": 1.3406217e-12
			},
			{
				"id": 409,
				"seek": 20875,
				"start": 1270.2634,
				"end": 1273.2834,
				"text": " they won't even trigger caching here",
				"tokens": [
					51514,
					436,
					1582,
					380,
					754,
					7875,
					269,
					2834,
					510,
					51665
				],
				"temperature": 0,
				"avg_logprob": -0.1469441,
				"compression_ratio": 1.726415,
				"no_speech_prob": 1.3406217e-12
			},
			{
				"id": 410,
				"seek": 23475,
				"start": 1273.2834,
				"end": 1279.1835,
				"text": " triple the resume.",
				"tokens": [
					50365,
					15508,
					264,
					15358,
					13,
					50660
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 411,
				"seek": 23475,
				"start": 1279.5435,
				"end": 1281.6234,
				"text": " And just to be clear, you're just running the same request twice",
				"tokens": [
					50678,
					400,
					445,
					281,
					312,
					1850,
					11,
					291,
					434,
					445,
					2614,
					264,
					912,
					5308,
					6091,
					50782
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 412,
				"seek": 23475,
				"start": 1281.6234,
				"end": 1284.2834,
				"text": " and seeing if it auto caches it based on the shared prefix.",
				"tokens": [
					50782,
					293,
					2577,
					498,
					309,
					8399,
					269,
					13272,
					309,
					2361,
					322,
					264,
					5507,
					46969,
					13,
					50915
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 413,
				"seek": 23475,
				"start": 1284.7434,
				"end": 1285.1034,
				"text": " Exactly.",
				"tokens": [
					50938,
					7587,
					13,
					50956
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 414,
				"seek": 23475,
				"start": 1285.6234,
				"end": 1287.8035,
				"text": " And what I'm doing here is I'm just trying to have it triple the resume",
				"tokens": [
					50982,
					400,
					437,
					286,
					478,
					884,
					510,
					307,
					286,
					478,
					445,
					1382,
					281,
					362,
					309,
					15508,
					264,
					15358,
					51091
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 415,
				"seek": 23475,
				"start": 1287.8035,
				"end": 1289.2234,
				"text": " and that'll do some stuff probably.",
				"tokens": [
					51091,
					293,
					300,
					603,
					360,
					512,
					1507,
					1391,
					13,
					51162
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 416,
				"seek": 23475,
				"start": 1293.2834,
				"end": 1294.9235,
				"text": " And hopefully I'll make it long enough.",
				"tokens": [
					51365,
					400,
					4696,
					286,
					603,
					652,
					309,
					938,
					1547,
					13,
					51447
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 417,
				"seek": 23475,
				"start": 1294.9235,
				"end": 1298.3235,
				"text": " But the way that I can test this is just I can keep making test cases",
				"tokens": [
					51447,
					583,
					264,
					636,
					300,
					286,
					393,
					1500,
					341,
					307,
					445,
					286,
					393,
					1066,
					1455,
					1500,
					3331,
					51617
				],
				"temperature": 0,
				"avg_logprob": -0.16117734,
				"compression_ratio": 1.6591928,
				"no_speech_prob": 1.4667856e-12
			},
			{
				"id": 418,
				"seek": 25979,
				"start": 1298.3235,
				"end": 1303.9235,
				"text": " until I'm satisfied with the final output.",
				"tokens": [
					50365,
					1826,
					286,
					478,
					11239,
					365,
					264,
					2572,
					5598,
					13,
					50645
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 419,
				"seek": 25979,
				"start": 1304.8834,
				"end": 1306.3834,
				"text": " And I'm going to see if it's actually long enough.",
				"tokens": [
					50693,
					400,
					286,
					478,
					516,
					281,
					536,
					498,
					309,
					311,
					767,
					938,
					1547,
					13,
					50768
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 420,
				"seek": 25979,
				"start": 1306.7834,
				"end": 1306.8435,
				"text": " Clear.",
				"tokens": [
					50788,
					14993,
					13,
					50791
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 421,
				"seek": 25979,
				"start": 1311.0835,
				"end": 1312.2034,
				"text": " Eventually, I'll put something.",
				"tokens": [
					51003,
					17586,
					11,
					286,
					603,
					829,
					746,
					13,
					51059
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 422,
				"seek": 25979,
				"start": 1315.7434,
				"end": 1317.0034,
				"text": " I'm going to get a lot slower.",
				"tokens": [
					51236,
					286,
					478,
					516,
					281,
					483,
					257,
					688,
					14009,
					13,
					51299
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 423,
				"seek": 25979,
				"start": 1320.9235,
				"end": 1322.0835,
				"text": " I'm still not long enough.",
				"tokens": [
					51495,
					286,
					478,
					920,
					406,
					938,
					1547,
					13,
					51553
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 424,
				"seek": 25979,
				"start": 1322.3435,
				"end": 1324.6234,
				"text": " But you're getting the point of how I would go do this.",
				"tokens": [
					51566,
					583,
					291,
					434,
					1242,
					264,
					935,
					295,
					577,
					286,
					576,
					352,
					360,
					341,
					13,
					51680
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 425,
				"seek": 25979,
				"start": 1324.7434,
				"end": 1327.1234,
				"text": " I know that OpenAI is probably caching out about 1024 tokens",
				"tokens": [
					51686,
					286,
					458,
					300,
					7238,
					48698,
					307,
					1391,
					269,
					2834,
					484,
					466,
					1266,
					7911,
					22667,
					51805
				],
				"temperature": 0,
				"avg_logprob": -0.29019535,
				"compression_ratio": 1.5123153,
				"no_speech_prob": 4.8555664e-13
			},
			{
				"id": 426,
				"seek": 28859,
				"start": 1327.1234,
				"end": 1328.5034,
				"text": " based on what they said in their docs.",
				"tokens": [
					50365,
					2361,
					322,
					437,
					436,
					848,
					294,
					641,
					45623,
					13,
					50434
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 427,
				"seek": 28859,
				"start": 1328.9034,
				"end": 1330.5234,
				"text": " So in that case, I'm just going to make an example",
				"tokens": [
					50454,
					407,
					294,
					300,
					1389,
					11,
					286,
					478,
					445,
					516,
					281,
					652,
					364,
					1365,
					50535
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 428,
				"seek": 28859,
				"start": 1330.5234,
				"end": 1331.6234,
				"text": " that has 1024 tokens.",
				"tokens": [
					50535,
					300,
					575,
					1266,
					7911,
					22667,
					13,
					50590
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 429,
				"seek": 28859,
				"start": 1332.1635,
				"end": 1334.0435,
				"text": " And I ran into this personally myself a few times",
				"tokens": [
					50617,
					400,
					286,
					5872,
					666,
					341,
					5665,
					2059,
					257,
					1326,
					1413,
					50711
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 430,
				"seek": 28859,
				"start": 1334.0435,
				"end": 1335.7834,
				"text": " when I was unit testing some of the code that we write",
				"tokens": [
					50711,
					562,
					286,
					390,
					4985,
					4997,
					512,
					295,
					264,
					3089,
					300,
					321,
					2464,
					50798
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 431,
				"seek": 28859,
				"start": 1335.7834,
				"end": 1338.7834,
				"text": " because I was like, oh, why is caching not working?",
				"tokens": [
					50798,
					570,
					286,
					390,
					411,
					11,
					1954,
					11,
					983,
					307,
					269,
					2834,
					406,
					1364,
					30,
					50948
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 432,
				"seek": 28859,
				"start": 1338.8834,
				"end": 1341.2034,
				"text": " Well, it turned out I literally just wasn't implementing",
				"tokens": [
					50953,
					1042,
					11,
					309,
					3574,
					484,
					286,
					3736,
					445,
					2067,
					380,
					18114,
					51069
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 433,
				"seek": 28859,
				"start": 1341.2034,
				"end": 1343.1434,
				"text": " a long enough prompt and it would just break all the time.",
				"tokens": [
					51069,
					257,
					938,
					1547,
					12391,
					293,
					309,
					576,
					445,
					1821,
					439,
					264,
					565,
					13,
					51166
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 434,
				"seek": 28859,
				"start": 1344.0234,
				"end": 1345.6835,
				"text": " But once I actually implemented a big enough prompt,",
				"tokens": [
					51210,
					583,
					1564,
					286,
					767,
					12270,
					257,
					955,
					1547,
					12391,
					11,
					51293
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 435,
				"seek": 28859,
				"start": 1345.7434,
				"end": 1349.8035,
				"text": " I could consistently see hits on caching pretty well.",
				"tokens": [
					51296,
					286,
					727,
					14961,
					536,
					8664,
					322,
					269,
					2834,
					1238,
					731,
					13,
					51499
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 436,
				"seek": 28859,
				"start": 1353.2834,
				"end": 1354.4435,
				"text": " Other questions from anyone?",
				"tokens": [
					51673,
					5358,
					1651,
					490,
					2878,
					30,
					51731
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 437,
				"seek": 28859,
				"start": 1354.6635,
				"end": 1356.0635,
				"text": " Otherwise, I think we're 20 minutes over.",
				"tokens": [
					51742,
					10328,
					11,
					286,
					519,
					321,
					434,
					945,
					2077,
					670,
					13,
					51812
				],
				"temperature": 0,
				"avg_logprob": -0.12137099,
				"compression_ratio": 1.6578171,
				"no_speech_prob": 1.0728738e-12
			},
			{
				"id": 438,
				"seek": 31859,
				"start": 1357.1234,
				"end": 1361.7434,
				"text": " I answered Vijay's question.",
				"tokens": [
					50365,
					286,
					10103,
					41201,
					320,
					311,
					1168,
					13,
					50596
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 439,
				"seek": 31859,
				"start": 1362.1835,
				"end": 1363.8035,
				"text": " It's about cloud code proxying.",
				"tokens": [
					50618,
					467,
					311,
					466,
					4588,
					3089,
					447,
					87,
					1840,
					13,
					50699
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 440,
				"seek": 31859,
				"start": 1363.9235,
				"end": 1365.2034,
				"text": " I'm actually, I'm in Austin this week",
				"tokens": [
					50705,
					286,
					478,
					767,
					11,
					286,
					478,
					294,
					15356,
					341,
					1243,
					50769
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 441,
				"seek": 31859,
				"start": 1365.2034,
				"end": 1367.1434,
				"text": " with the Gauntlet AI squad.",
				"tokens": [
					50769,
					365,
					264,
					10384,
					2760,
					2631,
					7318,
					15310,
					13,
					50866
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 442,
				"seek": 31859,
				"start": 1367.3035,
				"end": 1368.4435,
				"text": " They have like a school here",
				"tokens": [
					50874,
					814,
					362,
					411,
					257,
					1395,
					510,
					50931
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 443,
				"seek": 31859,
				"start": 1368.4435,
				"end": 1369.9235,
				"text": " for learning AI engineers.",
				"tokens": [
					50931,
					337,
					2539,
					7318,
					11955,
					13,
					51005
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 444,
				"seek": 31859,
				"start": 1370.2034,
				"end": 1371.8435,
				"text": " And apparently one of the students here",
				"tokens": [
					51019,
					400,
					7970,
					472,
					295,
					264,
					1731,
					510,
					51101
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 445,
				"seek": 31859,
				"start": 1371.8435,
				"end": 1373.4235,
				"text": " built a thing called CC Proxy",
				"tokens": [
					51101,
					3094,
					257,
					551,
					1219,
					12630,
					1705,
					12876,
					51180
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 446,
				"seek": 31859,
				"start": 1373.4235,
				"end": 1375.4435,
				"text": " that lets you just like strip all of the traces",
				"tokens": [
					51180,
					300,
					6653,
					291,
					445,
					411,
					12828,
					439,
					295,
					264,
					26076,
					51281
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 447,
				"seek": 31859,
				"start": 1375.4435,
				"end": 1376.9034,
				"text": " out of your cloud code running locally.",
				"tokens": [
					51281,
					484,
					295,
					428,
					4588,
					3089,
					2614,
					16143,
					13,
					51354
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 448,
				"seek": 31859,
				"start": 1377.6434,
				"end": 1378.2634,
				"text": " That's cool.",
				"tokens": [
					51391,
					663,
					311,
					1627,
					13,
					51422
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 449,
				"seek": 31859,
				"start": 1378.9634,
				"end": 1379.4435,
				"text": " That's cool.",
				"tokens": [
					51457,
					663,
					311,
					1627,
					13,
					51481
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 450,
				"seek": 31859,
				"start": 1381.0635,
				"end": 1383.4034,
				"text": " I guess that's it for today's conversation in that case.",
				"tokens": [
					51562,
					286,
					2041,
					300,
					311,
					309,
					337,
					965,
					311,
					3761,
					294,
					300,
					1389,
					13,
					51679
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 451,
				"seek": 31859,
				"start": 1384.1034,
				"end": 1385.3435,
				"text": " Thank you guys for joining.",
				"tokens": [
					51714,
					1044,
					291,
					1074,
					337,
					5549,
					13,
					51776
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 452,
				"seek": 31859,
				"start": 1385.6434,
				"end": 1386.6234,
				"text": " Hopefully it was educational",
				"tokens": [
					51791,
					10429,
					309,
					390,
					10189,
					51840
				],
				"temperature": 0,
				"avg_logprob": -0.19381112,
				"compression_ratio": 1.6551725,
				"no_speech_prob": 1.7281103e-12
			},
			{
				"id": 453,
				"seek": 34809,
				"start": 1386.6234,
				"end": 1388.0435,
				"text": " and hopefully everyone learned a few things.",
				"tokens": [
					50365,
					293,
					4696,
					1518,
					3264,
					257,
					1326,
					721,
					13,
					50436
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 454,
				"seek": 34809,
				"start": 1388.8435,
				"end": 1390.5635,
				"text": " We'll have a new topic for next week",
				"tokens": [
					50476,
					492,
					603,
					362,
					257,
					777,
					4829,
					337,
					958,
					1243,
					50562
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 455,
				"seek": 34809,
				"start": 1390.5635,
				"end": 1392.7834,
				"text": " that I think will be hopefully just as educational.",
				"tokens": [
					50562,
					300,
					286,
					519,
					486,
					312,
					4696,
					445,
					382,
					10189,
					13,
					50673
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 456,
				"seek": 34809,
				"start": 1393.5635,
				"end": 1394.6434,
				"text": " It's going to be dope.",
				"tokens": [
					50712,
					467,
					311,
					516,
					281,
					312,
					23383,
					13,
					50766
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 457,
				"seek": 34809,
				"start": 1395.1234,
				"end": 1396.0635,
				"text": " Thank you all for coming.",
				"tokens": [
					50790,
					1044,
					291,
					439,
					337,
					1348,
					13,
					50837
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 458,
				"seek": 34809,
				"start": 1396.4034,
				"end": 1398.1234,
				"text": " Thanks, ViBot, for running the session today.",
				"tokens": [
					50854,
					2561,
					11,
					6626,
					33,
					310,
					11,
					337,
					2614,
					264,
					5481,
					965,
					13,
					50940
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 459,
				"seek": 34809,
				"start": 1398.4435,
				"end": 1399.5635,
				"text": " And thanks, everyone.",
				"tokens": [
					50956,
					400,
					3231,
					11,
					1518,
					13,
					51012
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 460,
				"seek": 34809,
				"start": 1399.9435,
				"end": 1400.7434,
				"text": " Thanks, everyone here.",
				"tokens": [
					51031,
					2561,
					11,
					1518,
					510,
					13,
					51071
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			},
			{
				"id": 461,
				"seek": 34809,
				"start": 1402.7234,
				"end": 1403.2034,
				"text": " I'll see you later.",
				"tokens": [
					51170,
					286,
					603,
					536,
					291,
					1780,
					13,
					51194
				],
				"temperature": 0,
				"avg_logprob": -0.26691526,
				"compression_ratio": 1.5668449,
				"no_speech_prob": 1.1510196e-12
			}
		],
		"x_groq": {
			"id": "req_01k7hvnnmyfmkrpznv1nabhj4k"
		}
	}
]