class VideoChapter {
  summay string @description(#"
    Summary of what happened in this chapter
  "#)
  chapter_title string @description(#"
    Very short title. 4 words maximum.
    Avoid stating the obvious - for example if the video is about cars, don't make the title "Wheels on cars" you can simply make it "Wheels" then.
  "#)
  bookmark int @description("The T value as an int, e.g. T=342 -> 342")
}

class VideoChaptersResponse {
  summary string @description("Summary of what happened in the video")
  chapters VideoChapter[] @description("Make sure to match the requested chapter count - don't make too few, and don't make too many!")
}


function FindChapters(context: string, instructions: string, video_part: string) -> VideoChaptersResponse {
  client GROQ_GPT_20B
  prompt #"
    {{ _.role('system') }}
    # Background
    You are a website form agent.

    # Video part format
    The video is marked by symbols T. For example T=4 to T=18.
    This way you can reference moments in the video by their T value.

    {{ context }}

    {{ _.role('user') }}
    {{ instructions }}

    {{ _.role('user') }}
    # I just wrote down what was said in a part of the video I watched
    {{ video_part }}

    {{ _.role('system') }}
    # Output format:
    {{ ctx.output_format(hoist_classes=true, always_hoist_enums=true) }}
  "#
}

test summarizationChapters {
  functions [FindChapters]
  args {
    context #"
      # About the video
      Title: "Context Engineering lessons from Manus - ðŸ¦„ #18"
      By: BoundaryML, startup that builds BAML, a made up DSL for prompt and context engineering.
    "#
    instructions #"
      # User query
      Suggest 3 chapters
    "#
    video_part #"
      Video-part duration: 732 seconds
      From T=400 to T=649:
      ```
      ...
      T=400 "I assume that they did it because of, oh, Eugene."
      T=401 "Yes, that is also true."
      T=402 "The model also has its own cache that computes stuff"
      T=403 "because of the architecture."
      T=404 "I'm assuming the caching the model providers are doing"
      T=405 "are slightly different."
      T=406 "I think that also helps quite a lot."
      T=407 "The actual inference time is also faster"
      T=408 "if you have repeated tokens."
      T=409 "Maybe that is what the paper was talking about,"
      T=410 "and I misinterpreted that."
      T=411 "But really quickly, I suspect the model providers"
      T=412 "don't do the caching storage on a cross-user basis"
      T=413 "because they need to have TTLs."
      T=414 "And those TTLs are really based on your personal usage,"
      T=415 "not on anyone else's."
      T=416 "Raycast is a fantastic way to test this out quickly and locally."
      T=417 "Can you introduce..."
      T=418 "You can introduce dynamic variables."
      T=419 "Is there something else you would use for a similar purpose?"
      T=420 "I don't know about something myself, Jens."
      T=421 "I was just thinking about like a quick snippet copy-paste tool."
      T=422 "I don't know if you store prompts in anything else"
      T=423 "or if you have a local thing that you guys use."
      T=424 "I have my own other setup myself, including that."
      T=425 "So just curious."
      T=426 "I don't have anything, Dex."
      T=427 "Yeah, no, I mean, I think part of it is like,"
      T=428 "this is a lot more about like building,"
      T=429 "like this is more applicable to like building software"
      T=430 "that interacts with models than for like how you prompt them."
      T=431 "Yeah, yeah, no, I get that."
      T=432 "But I guess as I iterate, I still test things locally"
      T=433 "before I would put them into what I'm actually going to use."
      T=434 "I don't know if that makes sense in the way I'm thinking of it,"
      T=435 "but if I'm iterating on a prompt essentially"
      T=436 "or a structure or workflow that I'm going to be implementing"
      T=437 "through a series of orchestrating agents,"
      T=438 "I will iterate on each part of that separately"
      T=439 "through using some sort of way to iterate on the snippet basically."
      T=440 "I mean, Vibeoff's got a lot of opinions"
      T=441 "on how to iterate on prompt snippets."
      T=442 "Yeah, so I guess that's where my mind's at right now. So sidetracked."
      T=443 "I would say like none of this, none of the stuff that we're talking about today really matters for prompt iteration speed."
      T=444 "That really doesn't matter. This only matters for like you have a prompt, it's working, but you want to make that thing work faster and better."
      T=445 "How do you go do that?"
      T=446 "Cool."
      T=447 "How do we know the task list is or isn't cached?"
      T=448 "Shouldn't this be managed as it changes?"
      T=449 "Yes, to some degree."
      T=450 "But really, there's only one way to know that it's cached,"
      T=451 "which is if the model provider tells you that."
      T=452 "If they don't actually give you that information,"
      T=453 "you can't possibly know."
      T=454 "So it really depends on the inference provider."
      T=455 "I mean, you can kind of proxy it because of latency,"
      T=456 "but not really definitively."
      T=457 "So this is why me and Vaibov work well together,"
      T=458 "is he's really good at just talking through my random questions and finishing his point."
      T=459 "So please keep doing that."
      T=460 "Question though, like, are you going to show us today how to use an LLM client to actually"
      T=461 "look at the responses and see the, like, caching, like, the caching, like, headers that come"
      T=462 "back?"
      T=463 "Like, I think that would be super, super valuable to, like, untangle, like, the weather, family"
      T=464 "or whatever, untangle the response and see, like, hey, look, let's change the end of it"
      T=465 "and see how many cache tokens we got"
      T=466 "versus changing the beginning"
      T=467 "and see that it blows the cache."
      T=468 "Yeah, that's impossible."
      T=469 "Eugene had a really good point, actually,"
      T=470 "and I realized I actually had totally missed this,"
      T=471 "which is there's actually two types of caching"
      T=472 "that are going on here,"
      T=473 "which is one caching done by the actual model providers"
      T=474 "in terms of helping with encoder decoder blocks."
      T=475 "And then the actual transformer architecture"
      T=476 "also allows you to do another type of cache in there."
      T=477 "I think I can pull this up."
      T=478 "Let's see if I can pull it up."
      T=479 "It's a very famous image."
      T=480 "But basically, let's see if I can find this."
      T=481 "Basically, a lot of the actual nodes in here,"
      T=482 "a lot of the math can also be pre-computed along the way,"
      T=483 "at any point in the way."
      T=484 "The part that I was talking about was actually this part,"
      T=485 "which is the encoder node of the transformer"
      T=486 "can actually be pre-computed based"
      T=487 "of the actual input tokens that are going in."
      T=488 "And this whole layer can almost be cached"
      T=489 "because it's not really dependent on the output probabilities."
      T=490 "And these are going to be able to take some advance of that."
      T=491 "The second half of this that Eugene is talking about"
      T=492 "is also the KB."
      T=493 "There's a KB cache in the actual model,"
      T=494 "like matrix multiplication areas."
      T=495 "And those can also dramatically improve the amount of computation throughput you can get"
      T=496 "out of the actual transform architecture."
      T=497 "That may be actually what the paper was talking about, Eugene."
      T=498 "I may have missed that."
      T=499 "Yeah, I think it is about the cache inside the model."
      T=500 "Yes."
      T=501 "So in that case, it's still the principle is still the same."
      T=502 "Everything's the same except nothing's going into Redis, I think probably is the key detail."
      T=503 "Yes."
      T=504 "Well, no, that one's cutting down on the actual math that it has to go to."
      T=505 "but the main difference"
      T=506 "the premise is still the same"
      T=507 "the number of continuity tokens you have"
      T=508 "dramatically changes the amount of"
      T=509 "caching that you're going to get"
      T=510 "anytime you go change the continuity"
      T=511 "of the tokens the more likely you are"
      T=512 "going to be to break the cache at any point"
      T=513 "and get a miss on the cache"
      T=514 "how do I"
      T=515 "how do I eval if my updated context"
      T=516 "handling will"
      T=517 "improve the quality"
      T=518 "I think"
      T=519 "we've had a couple talks on evals."
      T=520 "It's actually very annoying to do evals"
      T=521 "and it's very frustrating,"
      T=522 "but I think the first step to do evals"
      T=523 "is just to define quality to some degree"
      T=524 "and what it means."
      T=525 "Understand what is clearly good,"
      T=526 "what is clearly bad,"
      T=527 "what is mostly good, mostly bad."
      T=528 "And then just like buy the eval in the beginning."
      T=529 "And once you have a better understanding"
      T=530 "of that system,"
      T=531 "then you can go ahead"
      T=532 "and actually make these trade-offs."
      T=533 "The only reason that likely can do this work"
      T=534 "or any engineering team can do this work"
      T=535 "is because they first have to go ahead and say,"
      T=536 "this is good and this is bad."
      T=537 "that evals are useless and i'd say like most of the stuff we talk about about evals here is about"
      T=538 "quality of the outputs in terms of like accuracy host nations using the right context things like"
      T=539 "that um the quality improvements that we're getting here because like at the end of the day"
      T=540 "whether you cash it or not it's the same tokens in which means your chance of getting the right"
      T=541 "tokens out is probably about the same um so this is a really interesting kind of category of evals"
      T=542 "we probably haven't talked as much about,"
      T=543 "which is like, how do you evaluate the performance,"
      T=544 "not in terms of accuracy,"
      T=545 "but in terms of speed and cost?"
      T=546 "Yeah."
      T=547 "I think Vijay has an interesting question."
      T=548 "I guess Rajesh pointed out a different thing."
      T=549 "If the model weights are the same,"
      T=550 "everything is new,"
      T=551 "how do you actually get caching?"
      T=552 "It's really hard to describe"
      T=553 "how the math on a GPU gets cached,"
      T=554 "but you can do caching on there."
      T=555 "You should take my word for it."
      T=556 "There's a beautiful video made by,"
      T=557 "If any of you are interested, this is probably one of the best videos I've ever seen."
      T=558 "Hey, everyone."
      T=559 "I'm by Bob."
      T=560 "Sorry."
      T=561 "I'll give you..."
      T=562 "I'm sorry."
      T=563 "Every time I pull up the YouTube channel, I hear you introducing yourself."
      T=564 "I'll post the video on here."
      T=565 "There's like a 90-minute video that I watched that actually talked about how DeepSeek actually works under the hood and talks about the math behind it."
      T=566 "I think it's actually this one."
      T=567 "This video is..."
      T=568 "It's one of the best ones I've ever seen."
      T=569 "It will describe it."
      T=570 "It will probably teach you more about anything you could see."
      T=571 "I wish I could make that."
      T=572 "John, best video I've ever seen."
      T=573 "Pulls up video of self."
      T=574 "And I'll talk to you about how it's possible to get way better throughput than anyone else can on the same math that everyone else is doing."
      T=575 "Context caching in our app thesis, the old code is open source for anyone."
      T=576 "Okay, cool."
      T=577 "There's just an example."
      T=578 "If Duck has an example, then definitely share it, Duck, and we'd love to have people go be able to see it."
      T=579 "Yeah, drop it in the chat."
      T=580 "Vijay, we've got a question. We'll take that, and then we'll go to the next part of the system after that."
      T=581 "So if I understood this correctly, for example, let's say our model has an input window of 256k tokens,"
      T=582 "and we're trying to, let's say, take advantage of that entire context window."
      T=583 "So essentially what you're saying is whatever we input in that 256k context, as much as possible,"
      T=584 "that string should be unchanging or let's say static."
      T=585 "It should be the same in next iteration so that you can get the next open quicker."
      T=586 "And the end of that input context will probably be varying per call So I think the next step would be to understand how agents are iterating between different calls and see what is changing what is not changing"
      T=587 "in what we are sending the LLM"
      T=588 "and sort of restructure that entire input sequence."
      T=589 "Is that the next step?"
      T=590 "Like, am I thinking about this correctly?"
      T=591 "I think maybe, just maybe I don't understand"
      T=592 "all the words you said perfectly."
      T=593 "And it's probably because I woke up at 4."
      T=594 "Okay, let me simplify it."
      T=595 "So we are sending tokens to the LLM."
      T=596 "Yes."
      T=597 "The sequence of tokens, as much as it is unchanged,"
      T=598 "the faster we get a response."
      T=599 "Yes."
      T=600 "Therefore, our next step would obviously be to understand"
      T=601 "a couple of iterations of what we are sending the LLM,"
      T=602 "see what is changing, what is not changing,"
      T=603 "and try to restructure that."
      T=604 "Correct?"
      T=605 "Yes, exactly."
      T=606 "There are caveats, though,"
      T=607 "in terms of actual implementation details here that matter."
      T=608 "If you own the model yourself and you're doing inference, it's a very different game than if you're actually using the existing inference providers."
      T=609 "So, for example, Anthropic lets you actually control cache control with this cache control block."
      T=610 "And where you put it dramatically matters."
      T=611 "Because if you put it at the very beginning of a system, if you put it in your system message, it will dramatically change the throughput that you are personally able to get."
      T=612 "if you have a really long chat thread and you just put it at the beginning of a chat thread and you"
      T=613 "don't break apart your chat threads manually you're going to get worse hits there's just nothing you"
      T=614 "can do about that because your chat thread is basically you put you put a cache control block"
      T=615 "on the first user message and then you in the same chat control block you put another user message"
      T=616 "it's just a cache miss it's a guaranteed cache miss you can't actually do anything about that"
      T=617 "so every time you're not reconstructing the prompt in the same way as close to possible"
      T=618 "you are basically getting a miss gemini i think i think the way open ai does caching"
      T=619 "is opaque so what that means is i i think they give you almost no information on how caching"
      T=620 "works but i think they actually what they do but it's like less control but they kind of just try"
      T=621 "to do it for you this is a lot of them just like do a prefix and try to guess how to do it i didn't"
      T=622 "But now they added a prompt cache key."
      T=623 "So it's like prompt cache."
      T=624 "And the reason that they do this, I'm certain..."
      T=625 "Replace the user."
      T=626 "I don't know what this is."
      T=627 "But I'm pretty certain that the reason they do this"
      T=628 "is people just want it more control."
      T=629 "Because if you're able to have access to this"
      T=630 "and you can control this,"
      T=631 "you can just get better hits all the time."
      T=632 "But it is important to go read this stuff."
      T=633 "And the only way you can actually know"
      T=634 "if you're hitting this is just by looking at the usage."
      T=635 "there's no other way to really know um along unless you own the inference and most people"
      T=636 "don't own inference as far as i know um and most people probably shouldn't own inference as well"
      T=637 "gemini is probably the most flexible thing i have seen for how they cache and eugene is right on"
      T=638 "that which is if you actually go look into how to get caches like you do it you do some crap to go"
      T=639 "write this code but the trade-off of doing this crap is you do get to go get complete control over"
      T=640 "what it's doing and you get to go tell it this and that helps you as a developer but it also"
      T=641 "hurts you because you have to go build this stuff yourself you also have to manage the detail on"
      T=642 "everything automatically so there's always a trade-off in how much engineering effort you"
      T=643 "want to put in versus what you want to do manually what you want to do automatically and generally"
      T=644 "if you do something manually you will always get better throughput if you know what you're doing"
      T=645 "than someone that does it automatically."
      T=646 "But someone that does it automatically"
      T=647 "will always get better than someone"
      T=648 "that doesn't know what they're doing manually."
      T=649 "Because if you blow the cache out of time,"
      ...
      ```
    "#
  }
}
